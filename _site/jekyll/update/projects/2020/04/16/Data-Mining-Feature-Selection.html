<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Zizhun Guo</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Your awesome title" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description." />
<meta property="og:description" content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description." />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="Your awesome title" />


<script type="application/ld+json">
{"@type":"WebSite","url":"http://localhost:4000/","name":"Your awesome title","headline":"Your awesome title","description":"Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<!-- <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.css"> -->
<link rel="stylesheet" type="text/css" href="http://localhost:4001/css/bootstrap.css">
<link rel="stylesheet" type="text/css" href="http://localhost:4001/css/app.css">
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Your awesome title" /></head>
<link href="https://fonts.googleapis.com/css?family=Inconsolata|Noto+Serif+SC&display=swap" rel="stylesheet">

<body style="background: #e3e3d3;">
	<nav id="mainNavbar" class="navbar navbar-dark navbar-expand-md py-0 fixed-top">
    
    <a href="http://localhost:4001/index.html" class="navbar-brand">ZIZHUN GUO</a>
    
    <button class="navbar-toggler" data-toggle="collapse" data-target="#navLinks" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse justify-content-end" id="navLinks">
        <ul class="navbar-nav justify-content-end " >
            <li class="nav-item text-right">
                <a href="http://localhost:4001/about.html" class="nav-link">&#127925暴风雨</a>
            </li>
            <li class="nav-item text-right">
                <a href="http://localhost:4001/assets/Zizhun-Guo_Resume_MLE.pdf" class="nav-link">RESUME</a>
            </li>
        </ul>
    </div>
</nav>


	<section id="page-container" class="container my-5 py-5" style="background: #f6f6f6;">
	<div class="jumbotron text-center" style="background: #f6f6f6;">
		<h1 class="display-3">Data Mining: PCA + Pearson CC -> Feature Selection</h1>
		<p class="lead"><span>Author: Zizhun Guo & Martin Qian</span>
		</p>
		<p class="lead">		Written at <time datetime="">
			16 Apr 2020
		</time>
		</p>
</div>		

<div class="jumbotron" style="background: #f6f6f6; padding: 0rem 0rem;">
	<p><!-- MarkdownTOC -->

<ul>
  <li><a href="#1-feature-selection-using-cross-correlation">1. Feature Selection using Cross-Correlation</a>
      - <a href="#cc-in-three-significant-digits">CC in three significant digits:</a>
      - <a href="#projection-vector">Projection Vector:</a>
      - <a href="#decision-boundary">Decision Boundary:</a>
      - <a href="#feature-selection-using-cc">Feature Selection using CC:</a></li>
  <li><a href="#2-brute-force-search-for-2-best-features">2. Brute Force Search for 2 Best Features</a>
    <ul>
      <li><a href="#question-what-is-the-best-classification-accuracy-this-produces-what-is-the-second-best-classification-accuracy-this-produces">Question: What is the best classification accuracy this produces? What is the second best classification accuracy this produces?</a></li>
    </ul>
  </li>
  <li><a href="#3-principal-components-analysis">3. Principal Components Analysis</a>
    <ul>
      <li><a href="#pca-algorithm-in-sklearn">PCA algorithm in sklearn</a>
        <ul>
          <li><a href="#eigenvectors-of-pcalets-take-first-7-as-example">Eigenvectors of PCA(let’s take first 7 as example):</a></li>
          <li><a href="#singular-values-of-pca">singular values of PCA</a></li>
        </ul>
      </li>
      <li><a href="#what-do-the-coefficients-in-the-eigenvector-tell-you-about-which-features-are-important">What do the coefficients in the eigenvector tell you about which features are important?</a></li>
    </ul>
  </li>
  <li><a href="#4-projection-onto-pca">4. Projection onto PCA</a></li>
  <li><a href="#5-gradient-descent">5. Gradient Descent</a>
      - <a href="#result-of-gradient-descent">Result of gradient descent</a></li>
  <li><a href="#6-conclusions">6. Conclusions</a></li>
</ul>

<!-- /MarkdownTOC -->
<p>Team Project Link:
<a href="https://github.com/Qianjx/CSCI720-HW9">https://github.com/Qianjx/CSCI720-HW9</a></p>

<p>Data format used for this project:
<img src="https://i.imgur.com/13S5MyY.png" alt="data" /></p>

<p>Goals:</p>
<ol>
  <li>Re-balance the Data</li>
  <li>Feature Generation</li>
  <li>Feature Selection using CC</li>
  <li>Feature Selection Using PCA</li>
  <li>Create a Linear 2D Classifier</li>
  <li>Classify the Unknown data</li>
  <li>Conclusion</li>
</ol>

<h4 id="1-feature-selection-using-cross-correlation">1. Feature Selection using Cross-Correlation</h4>

<h6 id="cc-in-three-significant-digits">CC in three significant digits:</h6>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Author</span><span class="p">:</span> <span class="n">Zizhun</span> <span class="n">Guo</span>

<span class="k">def</span> <span class="nf">CC_analysis</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="s">'''
    Using correlation coefficient to analyze data
    '''</span>
    <span class="n">CC_mat</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s">'Class'</span><span class="p">]</span><span class="o">.</span><span class="n">corrwith</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'Class'</span><span class="p">])</span>
    <span class="n">CC_mat</span> <span class="o">=</span> <span class="n">CC_mat</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">CC_mat</span>
</code></pre></div></div>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># CC results</span>
<span class="n">Age</span>             <span class="o">-</span><span class="mf">0.283</span>
<span class="n">Ht</span>              <span class="o">-</span><span class="mf">0.010</span>
<span class="n">TailLn</span>          <span class="o">-</span><span class="mf">0.266</span>
<span class="n">HairLn</span>          <span class="o">-</span><span class="mf">0.095</span>
<span class="n">BangLn</span>           <span class="mf">0.203</span>
<span class="n">Reach</span>           <span class="o">-</span><span class="mf">0.092</span>
<span class="n">EarLobes</span>         <span class="mf">0.043</span>
<span class="n">TailLessHair</span>    <span class="o">-</span><span class="mf">0.222</span>
<span class="n">TailLessBang</span>    <span class="o">-</span><span class="mf">0.340</span>
<span class="n">ShagFactor</span>      <span class="o">-</span><span class="mf">0.515</span>
<span class="n">TailAndHair</span>     <span class="o">-</span><span class="mf">0.259</span>
<span class="n">TailAndBangs</span>    <span class="o">-</span><span class="mf">0.159</span>
<span class="n">HairAndBangs</span>     <span class="mf">0.049</span>
<span class="n">AllLengths</span>      <span class="o">-</span><span class="mf">0.157</span>
<span class="n">ApeFactor</span>       <span class="o">-</span><span class="mf">0.537</span>
<span class="n">HeightLessAge</span>    <span class="mf">0.193</span>
<span class="n">dtype</span><span class="p">:</span> <span class="n">float64</span>
</code></pre></div></div>
<p>Feature1: <strong>ApeFactor</strong>; CC = <strong>-0.537</strong>
Feature2: <strong>BangLn</strong>; CC = <strong>0.203</strong></p>

<h6 id="projection-vector">Projection Vector:</h6>
<p>So the projection vector should be the vector from the center of the attribute ApeFactor to the center of the BangLn</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Author</span><span class="p">:</span> <span class="n">Martin</span> <span class="n">Qian</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">CA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">CB</span><span class="p">[</span><span class="mi">0</span><span class="p">]],[</span><span class="n">CA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">CB</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>\
        <span class="s">'m-'</span> <span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">'projection vector'</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span> <span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
</code></pre></div></div>
<h6 id="decision-boundary">Decision Boundary:</h6>
<p>Dicision boundry is the vertical line of thr projection vector.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="p">(</span><span class="n">CA</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">CB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="n">CA</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">CB</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mf">0.</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="p">(</span><span class="n">CA</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">CB</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">/</span><span class="mf">2.</span><span class="p">)</span><span class="o">+</span><span class="p">(</span><span class="n">CA</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">CB</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mf">2.</span> <span class="p">,</span>\
         <span class="s">'k-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'decision boundry'</span><span class="p">)</span>
</code></pre></div></div>

<h6 id="feature-selection-using-cc">Feature Selection using CC:</h6>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Author</span><span class="p">:</span> <span class="n">Zizhun</span> <span class="n">Guo</span>

<span class="k">def</span> <span class="nf">feature_selection_using_CC</span><span class="p">(</span><span class="n">CC_mat</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="s">'''
    using CC to select two features that has the greatest abs and has opposite sign
    using LDA to test Accuracy
    '''</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"cross correlation matrix:</span><span class="se">\n</span><span class="s">"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">CC_mat</span><span class="p">))</span>
    <span class="c"># assign the feature by abs sort and choose the greates</span>
    <span class="n">feature1</span> <span class="o">=</span> <span class="n">CC_mat</span><span class="o">.</span><span class="nb">abs</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">CC_mat</span><span class="p">[</span><span class="n">feature1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span> 
        <span class="n">feature2</span> <span class="o">=</span> <span class="n">CC_mat</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span> 
        <span class="n">feature2</span> <span class="o">=</span> <span class="n">CC_mat</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">print</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">feature1</span><span class="p">)</span> <span class="o">+</span><span class="s">' '</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">feature2</span><span class="p">)</span> <span class="o">+</span> <span class="s">' Selected'</span><span class="p">)</span>
    <span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">feature1</span><span class="p">,</span> <span class="n">feature2</span><span class="p">)</span>
    
    <span class="c"># LDA classify the trainning data</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">(</span><span class="n">solver</span> <span class="o">=</span> <span class="s">'eigen'</span><span class="p">,</span> <span class="n">n_components</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:</span> <span class="p">,[</span><span class="n">feature1</span><span class="p">,</span> <span class="n">feature2</span><span class="p">]],</span> <span class="n">data</span><span class="p">[</span><span class="s">'Class'</span><span class="p">])</span>
    <span class="n">curr_score</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:</span> <span class="p">,[</span><span class="n">feature1</span><span class="p">,</span> <span class="n">feature2</span><span class="p">]],</span> <span class="n">data</span><span class="p">[</span><span class="s">'Class'</span><span class="p">])</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Accuracy for these features:"</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">curr_score</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</code></pre></div></div>
<p><img src="https://i.imgur.com/An6KPTT.png" alt="feature_selection" /></p>
<center> Image1: Feature Selection Scatter Plot </center>

<p><strong>Results (Accuracy for this classifier):</strong> <strong>0.753</strong></p>

<h4 id="2-brute-force-search-for-2-best-features">2. Brute Force Search for 2 Best Features</h4>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Author</span><span class="p">:</span> <span class="n">Martin</span> <span class="n">Qian</span>

<span class="k">def</span> <span class="nf">BFS_analysis</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="s">'''
    Using Brute Force Search to analyze data
    '''</span>
    <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">score_second</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">feature1</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">feature2</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">feature1</span> <span class="o">!=</span> <span class="n">feature2</span> <span class="ow">and</span> <span class="n">feature1</span> <span class="o">!=</span> <span class="s">'Class'</span> <span class="ow">and</span> <span class="n">feature2</span> <span class="o">!=</span> <span class="s">'Class'</span><span class="p">:</span>
                <span class="c"># LDA classifier</span>
                <span class="n">clf</span> <span class="o">=</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">(</span><span class="n">solver</span> <span class="o">=</span> <span class="s">'eigen'</span><span class="p">,</span> <span class="n">n_components</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:</span> <span class="p">,[</span><span class="n">feature1</span><span class="p">,</span> <span class="n">feature2</span><span class="p">]],</span> <span class="n">data</span><span class="p">[</span><span class="s">'Class'</span><span class="p">])</span>
                <span class="n">curr_score</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:</span> <span class="p">,[</span><span class="n">feature1</span><span class="p">,</span> <span class="n">feature2</span><span class="p">]],</span> <span class="n">data</span><span class="p">[</span><span class="s">'Class'</span><span class="p">])</span>
                <span class="k">if</span><span class="p">(</span> <span class="n">curr_score</span> <span class="o">&gt;</span> <span class="n">score</span><span class="p">):</span>
                    <span class="n">score_second</span> <span class="o">=</span> <span class="n">score</span>
                    <span class="n">score</span> <span class="o">=</span> <span class="n">curr_score</span>
                    <span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="n">feature1</span> <span class="p">,</span> <span class="n">feature2</span><span class="p">]</span>
                    <span class="n">best_clf</span> <span class="o">=</span> <span class="n">clf</span>
    <span class="k">return</span> <span class="n">features</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">best_clf</span>
</code></pre></div></div>
<p>So the highest score is achieved with the following features
Feature1: <strong>ShagFactor</strong>
Feature2: <strong>ApeFactor</strong></p>

<p><img src="https://i.imgur.com/P6YVlbz.png" alt="lda_scatter" /></p>
<center> Image2: Bruce For Search Best Classifier Scatter Plot </center>

<p><strong>Results (Accuracy):</strong> <strong>0.817</strong></p>

<h5 id="question-what-is-the-best-classification-accuracy-this-produces-what-is-the-second-best-classification-accuracy-this-produces">Question: What is the best classification accuracy this produces? What is the second best classification accuracy this produces?</h5>

<p><strong>Best: 0.817</strong>
<strong>Second Best: 0.778</strong></p>

<h4 id="3-principal-components-analysis">3. Principal Components Analysis</h4>

<h5 id="pca-algorithm-in-sklearn">PCA algorithm in sklearn</h5>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">7</span><span class="p">)</span>
    <span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s">'Class'</span><span class="p">])</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"eigen vectors of PCA</span><span class="se">\n</span><span class="s">"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"singular values of PCA</span><span class="se">\n</span><span class="s">"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">singular_values_</span><span class="p">))</span>
</code></pre></div></div>
<h6 id="eigenvectors-of-pcalets-take-first-7-as-example">Eigenvectors of PCA(let’s take first 7 as example):</h6>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[[</span><span class="o">-</span><span class="mf">0.023</span> <span class="o">-</span><span class="mf">0.58</span>  <span class="o">-</span><span class="mf">0.046</span> <span class="o">-</span><span class="mf">0.006</span> <span class="o">-</span><span class="mf">0.004</span> <span class="o">-</span><span class="mf">0.582</span> <span class="o">-</span><span class="mf">0.003</span> <span class="o">-</span><span class="mf">0.041</span> <span class="o">-</span><span class="mf">0.042</span> <span class="o">-</span><span class="mf">0.001</span>
  <span class="o">-</span><span class="mf">0.052</span> <span class="o">-</span><span class="mf">0.051</span> <span class="o">-</span><span class="mf">0.01</span>  <span class="o">-</span><span class="mf">0.056</span> <span class="o">-</span><span class="mf">0.002</span> <span class="o">-</span><span class="mf">0.557</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.723</span>  <span class="mf">0.19</span>   <span class="mf">0.128</span>  <span class="mf">0.019</span> <span class="o">-</span><span class="mf">0.005</span>  <span class="mf">0.227</span>  <span class="mf">0.003</span>  <span class="mf">0.109</span>  <span class="mf">0.133</span>  <span class="mf">0.023</span>
   <span class="mf">0.147</span>  <span class="mf">0.123</span>  <span class="mf">0.014</span>  <span class="mf">0.142</span>  <span class="mf">0.037</span> <span class="o">-</span><span class="mf">0.533</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.249</span> <span class="o">-</span><span class="mf">0.146</span>  <span class="mf">0.335</span>  <span class="mf">0.093</span>  <span class="mf">0.077</span> <span class="o">-</span><span class="mf">0.133</span>  <span class="mf">0.002</span>  <span class="mf">0.241</span>  <span class="mf">0.258</span>  <span class="mf">0.017</span>
   <span class="mf">0.428</span>  <span class="mf">0.411</span>  <span class="mf">0.17</span>   <span class="mf">0.505</span>  <span class="mf">0.012</span>  <span class="mf">0.103</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.066</span>  <span class="mf">0.041</span> <span class="o">-</span><span class="mf">0.21</span>   <span class="mf">0.269</span>  <span class="mf">0.25</span>   <span class="mf">0.012</span> <span class="o">-</span><span class="mf">0.02</span>  <span class="o">-</span><span class="mf">0.479</span> <span class="o">-</span><span class="mf">0.46</span>   <span class="mf">0.019</span>
   <span class="mf">0.06</span>   <span class="mf">0.041</span>  <span class="mf">0.52</span>   <span class="mf">0.31</span>  <span class="o">-</span><span class="mf">0.03</span>  <span class="o">-</span><span class="mf">0.025</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.183</span> <span class="o">-</span><span class="mf">0.309</span> <span class="o">-</span><span class="mf">0.02</span>   <span class="mf">0.102</span> <span class="o">-</span><span class="mf">0.093</span>  <span class="mf">0.441</span> <span class="o">-</span><span class="mf">0.011</span> <span class="o">-</span><span class="mf">0.123</span>  <span class="mf">0.073</span>  <span class="mf">0.196</span>
   <span class="mf">0.082</span> <span class="o">-</span><span class="mf">0.114</span>  <span class="mf">0.009</span> <span class="o">-</span><span class="mf">0.012</span>  <span class="mf">0.75</span>  <span class="o">-</span><span class="mf">0.125</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.037</span>  <span class="mf">0.095</span> <span class="o">-</span><span class="mf">0.001</span>  <span class="mf">0.288</span> <span class="o">-</span><span class="mf">0.312</span> <span class="o">-</span><span class="mf">0.15</span>   <span class="mf">0.006</span> <span class="o">-</span><span class="mf">0.289</span>  <span class="mf">0.311</span>  <span class="mf">0.6</span>
   <span class="mf">0.287</span> <span class="o">-</span><span class="mf">0.314</span> <span class="o">-</span><span class="mf">0.024</span> <span class="o">-</span><span class="mf">0.026</span> <span class="o">-</span><span class="mf">0.245</span>  <span class="mf">0.058</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.003</span> <span class="o">-</span><span class="mf">0.005</span> <span class="o">-</span><span class="mf">0.006</span>  <span class="mf">0.005</span>  <span class="mf">0.006</span>  <span class="mf">0.003</span>  <span class="mf">0.</span>    <span class="o">-</span><span class="mf">0.01</span>  <span class="o">-</span><span class="mf">0.011</span> <span class="o">-</span><span class="mf">0.001</span>
  <span class="o">-</span><span class="mf">0.001</span>  <span class="mf">0.</span>     <span class="mf">0.01</span>   <span class="mf">0.005</span>  <span class="mf">0.009</span> <span class="o">-</span><span class="mf">0.003</span><span class="p">]]</span>
</code></pre></div></div>
<h6 id="singular-values-of-pca">singular values of PCA</h6>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="mf">931.941</span> <span class="mf">573.253</span> <span class="mf">457.488</span> <span class="mf">260.495</span> <span class="mf">109.357</span>  <span class="mf">62.967</span>  <span class="mf">10.034</span><span class="p">]</span>
</code></pre></div></div>
<h5 id="what-do-the-coefficients-in-the-eigenvector-tell-you-about-which-features-are-important">What do the coefficients in the eigenvector tell you about which features are important?</h5>

<p>First of all, by the definition of the Principal Components, the eigen vector with the greater singular values works as a better vector to project and have a better effect on seprating the data.
Secondly, the eigen vector stands for the coefficient multipled by the features of raw data, so apparently, the more its absolute value is, the more importance the corresponding feature has. 
However, we know that PCA is used to do dimension reduction. At the same time, we only have 6 features at first and the rest are the linear combinations of these 6 features. As the results, these new feature should have no impact on the final eigenvectors. (Because the rank of the data is 6, it would break mathematic rules if it has more than 6 eigen vectors. So the result also supprted my opinion as eigen vectors that are more than 6 are nearly all 0s)</p>

<h4 id="4-projection-onto-pca">4. Projection onto PCA</h4>
<p>Since the first two singular values are the greatest among all, project the data onto the first two Principal Components.</p>

<p><img src="https://i.imgur.com/mTcVZky.png" alt="pca" /></p>
<center> Image3: PCA Scatter Plot using first two eigen vectors</center>

<p>On the other hand, I also get the projection scatter plot with only original dataset as following. And these two graphs are quite similar(if we ratote one of them by 180 degrees we can see distribution similarity).</p>

<p><img src="https://i.imgur.com/xl8Z41K.png" alt="pca_1" /></p>
<center> Image4: PCA Scatter Plot using original dataset</center>

<h4 id="5-gradient-descent">5. Gradient Descent</h4>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">theta</span><span class="p">,</span> <span class="n">rho</span> <span class="o">=</span>  <span class="n">Gradient_Descent__Fit_Through_a_Line_v100</span><span class="p">(</span>\
        <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="n">data</span><span class="p">[</span><span class="n">features</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span><span class="n">data</span><span class="p">[</span><span class="s">'Class'</span><span class="p">]]</span> <span class="p">,</span> <span class="mf">62.7</span><span class="p">,</span> <span class="mf">11.2</span> <span class="p">,</span><span class="mf">4.5</span><span class="p">)</span>
</code></pre></div></div>
<p>By calling the method from HW8 and modifying their parameters( 62.7 is initial theta, 11.2 is initial rho and 4.5 is initial alpha, they can be calculated mannually with two centers of selected features), we could use gradient descent for this question. We set the loss function as the misclassification rate. The initial features we selected are ApeFactor and ShagFactor, we believe they are more reasonable since firstly their correlaton coefficients’ absolute are the greatest, though both are negative; secondly they are selected as the features from the brute force algo part.</p>

<p><img src="https://i.imgur.com/94010eq.png" alt="gd" /></p>
<center> Image5: Boundry given by GD</center>

<h6 id="result-of-gradient-descent">Result of gradient descent</h6>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>starting point:0.458 * x + 0.889 * y = 11.2(theta = 62.7)
final result: 0.781 * x + 0.6241 * y + -6.09 = 0   
Accuracy is 0.817
</code></pre></div></div>
<p>The results are interesting, since the accuracy the model produced is very closed to the one given by the Linear Discriminant Analysis. So we believe this is the limitation linear classifier that can achieve on this dataset.</p>

<h4 id="6-conclusions">6. Conclusions</h4>
<p>Done by all works above, the final classifier that was chosen by us is the decision boudary conducted by the <strong>gradient descent</strong> algorithm, since it provides the highest accuracy in the prediction task on the unclassified dataset.</p>
</p>

</div>


</section>
	
	<a href="#page-container" style="position:fixed;right:0;bottom:0">Back to Top</a>

	<!-- Optional JavaScript -->
	<!-- jQuery first, then Popper.js, then Bootstrap JS -->
	<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
	crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
	crossorigin="anonymous"></script>
	<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
	crossorigin="anonymous"></script>

	<script>
		$(function () {
			$(document).scroll(function () {
				var $nav = $("#mainNavbar");
				$nav.toggleClass("scrolled", $(this).scrollTop() > $nav.height());
			});
		});
	</script>
</body>
</html>