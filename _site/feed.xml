<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4001/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4001/" rel="alternate" type="text/html" /><updated>2021-06-09T00:15:31-04:00</updated><id>http://localhost:4001/feed.xml</id><title type="html">Your awesome title</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">Alibaba Taobao User Behaviors Analysis: Dataset 1 billion records</title><link href="http://localhost:4001/jekyll/update/projects/2021/05/21/Taobao_Behavior_Analysis_Intro.html" rel="alternate" type="text/html" title="Alibaba Taobao User Behaviors Analysis: Dataset 1 billion records" /><published>2021-05-21T07:34:57-04:00</published><updated>2021-05-21T07:34:57-04:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/05/21/Taobao_Behavior_Analysis_Intro</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/05/21/Taobao_Behavior_Analysis_Intro.html">&lt;p&gt;&lt;strong&gt;Alibaba Taobao.com&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Taobao (Chinese: 淘宝网) is a Chinese online shopping platform. It is headquartered in Hangzhou and owned by Alibaba. It is ranked as the eighth most-visited website. Taobao.com was registered on April 21, 2003 by Alibaba Cloud Computing (Beijing) Co., Ltd.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Taobao Marketplace facilitates consumer-to-consumer (C2C) retail by providing a platform for small businesses and individual entrepreneurs to open online stores that mainly cater to consumers in Chinese-speaking regions (Mainland China, Hong Kong, Macau and Taiwan) and abroad,[4] which is made payable by online accounts. Its stores usually offer an express delivery service.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Taobao&quot;&gt;https://en.wikipedia.org/wiki/Taobao&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Intro/AlibabaLogo.jpg&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Intro/AlibabaLogo.jpg&quot; alt=&quot;drawing&quot; style=&quot;width: 40%;&quot; /&gt;
   &lt;/a&gt;
   &lt;br /&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Intro/Taobao_Logo.svg&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Intro/Taobao_Logo.svg&quot; alt=&quot;drawing&quot; style=&quot;width: 40%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Alibaba Group LOGO &lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;dataset-user-behavior-data-from-taobao-for-recommendation&quot;&gt;Dataset: User Behavior Data from Taobao for Recommendation&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The dataset is collected from &lt;a href=&quot;https://tianchi.aliyun.com/dataset/dataDetail?dataId=649&amp;amp;userId=1&quot;&gt;&lt;strong&gt;Tianchi&lt;/strong&gt;&lt;/a&gt; - Data Science Workshop from Aliyun(阿里云)- literally means &lt;a href=&quot;https://us.alibabacloud.com/&quot;&gt;&lt;strong&gt;Alibaba Cloud&lt;/strong&gt;&lt;/a&gt;, the cloud computing service ranked &lt;strong&gt;third-largest&lt;/strong&gt; infrastucture as a service provider, right behind Amazon Web Services, Microsoft Azure.&lt;/p&gt;

&lt;p&gt;User Behavior is a dataset of user behaviors from Taobao, for recommendation problem with implicit feedback. The dataset is offered by Alibaba.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;File&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Feature&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;UserBehavior.csv&lt;/td&gt;
      &lt;td&gt;All user behavior data&lt;/td&gt;
      &lt;td&gt;User ID, item ID, category ID, behavior type, timestamp&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;UserBehavior.csv&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We random select about 1 million users who have behaviors including click, purchase, adding item to shopping cart and item favoring during November 25 to December 03, 2017. The dataset is organized in a very similar form to MovieLens-20M, i.e., each line represents a specific user-item interaction, which consists of user ID, item ID, item’s category ID, behavior type and timestamp, separated by commas. The detailed descriptions of each field are as follows:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Field&lt;/th&gt;
      &lt;th&gt;Explanation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;User ID&lt;/td&gt;
      &lt;td&gt;An integer, the serialized ID that represents a user&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Item ID&lt;/td&gt;
      &lt;td&gt;An integer, the serialized ID that represents an item&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Category ID&lt;/td&gt;
      &lt;td&gt;An integer, the serialized ID that represents the category which the corresponding item belongs to&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Behavior type&lt;/td&gt;
      &lt;td&gt;A string, enum-type from (‘pv’, ‘buy’, ‘cart’, ‘fav’)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Timestamp&lt;/td&gt;
      &lt;td&gt;An integer, the timestamp of the behavior&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Note that the dataset contains 4 different types of behaviors, they are&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Behavior&lt;/th&gt;
      &lt;th&gt;Explanation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;pv&lt;/td&gt;
      &lt;td&gt;Page view of an item’s detail page, equivalent to an item click&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;fav&lt;/td&gt;
      &lt;td&gt;Purchase an item&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;cart&lt;/td&gt;
      &lt;td&gt;Add an item to shopping cart&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;buy&lt;/td&gt;
      &lt;td&gt;Favor an item&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Dimensions of the dataset are&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Dimension&lt;/th&gt;
      &lt;th&gt;Number&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;# of users&lt;/td&gt;
      &lt;td&gt;987,994&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;# of items&lt;/td&gt;
      &lt;td&gt;4,162,024&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;# of categories&lt;/td&gt;
      &lt;td&gt;9,439&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;# of interactions&lt;/td&gt;
      &lt;td&gt;100,150,807&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;
&lt;p&gt;Copyright @ 2021 Zizhun Guo. All Rights Reserved.&lt;/p&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html">Alibaba Taobao.com</summary></entry><entry><title type="html">YOLO Project Poster Released!!! [with Audio]</title><link href="http://localhost:4001/jekyll/update/projects/2021/04/16/CP-Poster.html" rel="alternate" type="text/html" title="YOLO Project Poster Released!!! [with Audio]" /><published>2021-04-16T08:18:18-04:00</published><updated>2021-04-16T08:18:18-04:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/04/16/CP-Poster</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/04/16/CP-Poster.html">&lt;audio controls=&quot;&quot; autoplay=&quot;&quot; muted=&quot;&quot;&gt;
  &lt;!-- &lt;source src=&quot;http://localhost:4001/assets/2021-04-16-CP-Poster/record.mp3&quot; type=&quot;audio/ogg&quot;&gt; --&gt;
  &lt;source src=&quot;http://localhost:4001/assets/2021-04-16-CP-Poster/recordnew.mp3&quot; type=&quot;audio/mpeg&quot; /&gt;
  Your browser does not support the audio element.
&lt;/audio&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-04-16-CP-Poster/Poster.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-04-16-CP-Poster/Poster.png&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Poster for capstone project&lt;/figcaption&gt;
&lt;/div&gt;

&lt;hr /&gt;
&lt;p&gt;Copyright @ 2021 Zizhun Guo. All Rights Reserved.&lt;/p&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html">Your browser does not support the audio element.</summary></entry><entry><title type="html">YOLO Explore Object Detection PERFORMANCES In Relation To INPUT IMAGE DETERIORATION</title><link href="http://localhost:4001/jekyll/update/projects/2021/04/15/YOLO-Experiment-copy.html" rel="alternate" type="text/html" title="YOLO Explore Object Detection PERFORMANCES In Relation To INPUT IMAGE DETERIORATION " /><published>2021-04-15T18:34:18-04:00</published><updated>2021-04-15T18:34:18-04:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/04/15/YOLO-Experiment%20copy</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/04/15/YOLO-Experiment-copy.html">&lt;p&gt;&lt;strong&gt;Experiment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Baseline Model and Tuning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We consider having the model trained based on the original dataset without processing it with distorted quality and treat it as the baseline model. A fine-tune process is also necessary to conduct in order to get the optimal performance for object detection. This allows us to fit the same dataset processed in distorted methods in further step and make it possible for us to compare how the dataset in the variable of distortion types would impact the performance of the baseline model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning Rate Annealing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;During the training process, the adjustment of the learning rate is best have. The learning rate represents the speed at which information accumulates in the neural network over time. In an ideal situation, we would start with a large learning rate and gradually reduce the speed until the loss value no longer diverges.
The technique we use is learning rate annealing, which starts with a relatively high learning rate and then slowly reduces the learning rate during training. The idea behind this method is to move from the initial parameter to a “good” range of parameter values. We use the &lt;strong&gt;ReduceLROnPlateau&lt;/strong&gt; callback method of the TensorFlow API to implement this step. This callback monitors a quantity and if no improvement is seen for a ‘patience’ number of epochs, the learning rate is reduced.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Early Stopping&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In order to get better generalization performance to prevent the model from overfitting, we use the early stopping method. It is implemented by the TensorFlow EarlyStopping callback function. It stops training when a monitored metric has stopped improving based on the ‘patience’ - number of epochs with no improvement after which training will be stopped. We set it 3 for giving the 3 epochs training threshold times.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NMS Parameters Tuning: Score and IoU&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Once the YOLO model has finished the training process, it is necessary to fine-tune the IoU threshold and Score threshold to improve the overall performance in mAP metric. We choose the strategy by individually incrementing the values of each parameter in the stride of 0.1 and evaluate its model performance. During the individual tuning of each parameter, we select the current parameter value that can make mAP reach the highest value as the baseline setting. Therefore, for the final tuning result, we expect to get IoU and Score setting as in the format of values of pairs.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/01-finetune-score.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/01-finetune-score.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Score Threshold Tuning Results&lt;/figcaption&gt;
&lt;/div&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/02-finetune-score.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/02-finetune-score.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Score Threshold Tuning Line Graph&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;It can be seen from Figure 12 that the overall line graph follows a monotonous downward trend which also applies to both categories of the Traffic sign and Stop sign.&lt;/p&gt;

&lt;p&gt;The Step sign category has the lowest degree of decline in AP value, and the decline rate is slower, so the curve looks smoother. From table 1, the results show Stop sign performs consistently steady until the score threshold reaches the values that close to the end.&lt;/p&gt;

&lt;p&gt;Traffic sign behaves in an opposite way, the AP value is decreasing from the beginning, as the score threshold increases linearly, the AP value decreases more rapidly.&lt;/p&gt;

&lt;p&gt;When the score value is 0.1, the mAP value of the YOLO model is the highest and performs best. Therefore 0.1 is selected for the score threshold value for the baseline YOLO model.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/03-finetune-iou.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/03-finetune-iou.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;IoU Threshold Tuning Results&lt;/figcaption&gt;
&lt;/div&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/04-finetune-iou.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/04-finetune-iou.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;IoU Threshold Tuning&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;It can be seen from Figure 13 all three lines display a two-phased behavior. In the first phase, they both demonstrate a smooth upwards trend in values. The increasing rate is too small to be noticed in the figure, but the performance improves from the results table.&lt;/p&gt;

&lt;p&gt;In the second phase, the overall mAP and AP of all categories drop rapidly. The starting point for having such a situation is when IoU threshold equals around 0.6.&lt;/p&gt;

&lt;p&gt;When the IoU value is 0.3, the mAP value of the YOLO model is the highest and performs best. Therefore, the IoU threshold setting for the baseline YOLO model is 0.3.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fitting Dataset in Distortion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As it is mentioned in part 2, the dataset that employed for the experiment contains 3200 images, in which 10 percent of them are 320 images that are selected as the test set. The images in the test set will first be processed by different distortion methods, and then the test images of different types and levels of distortion will be input into the neural network for prediction, and the results will be used to measure the performance of the object detector using the final statistics of mAP.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/05-distortion-types.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/05-distortion-types.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Distortion Image Comparison Results&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;Noted that all 320 test images serve only for testing the performance of the YOLO model. The model is trained and validated by the other 90 percent of the dataset.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/06-map_results.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/06-map_results.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Distortion Image Comparison Results&lt;/figcaption&gt;
&lt;/div&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/07-map_comparison.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/07-map_comparison.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Distortion Image Comparison Bar Graph&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;From the experiment results, it is interesting to observe that the object detector is remarkably affected by the images of different distorted types in various behaviors.&lt;/p&gt;

&lt;p&gt;There are 3 distortion types that affect the detector in a positive way by increasing the mAP values. These three distortion types are Noise100, Grayscale, and Contrast. However, the increase of values is not significant therefore the impact levels are considered to be limited and weakly.&lt;/p&gt;

&lt;p&gt;The other three distortion types of images – Noise200, 2percentPackLost, 5percentPackLost – suffer extreme decrease. The 2percentPackLost and 5percentPackLost are even close to half and one-third of the performance with original images.&lt;/p&gt;

&lt;p&gt;Horizontally comparing detector’s performances with Stop signs(SS) and Traffic signs(TS), one using SS for all distortion types and levels beats what use TS. Surprisingly to observe that the 4 distortion types of images in Noise100, Noise200, Grayscale, and Contrast boosts the performances using SS. In other words, these 4 distortion types improved the detector’s ability to predict. However, it works in the opposite way when employs TS and exhibits a similar decrease in performance of overall mAP.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Copyright @ 2021 Zizhun Guo. All Rights Reserved.&lt;/p&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html">Experiment</summary></entry><entry><title type="html">YOLO Mechanism Part II: Training, Evaluation Metrics</title><link href="http://localhost:4001/jekyll/update/projects/2021/04/11/YOLO-Mechanism-part2-copy.html" rel="alternate" type="text/html" title="YOLO Mechanism Part II: Training, Evaluation Metrics" /><published>2021-04-11T06:13:14-04:00</published><updated>2021-04-11T06:13:14-04:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/04/11/YOLO-Mechanism-part2%20copy</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/04/11/YOLO-Mechanism-part2-copy.html">&lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Use Pretrained Model&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The pre-trained model is a deep learning architecture that has been trained to perform specific tasks on large amounts of data (for example, identifying classification problems in pictures). This kind of training is not easy to perform, and usually requires a lot of resources, beyond the resources available to many people who can use deep learning models, such as not having a large number of such GPUs.
It should be noted that the weight of the pre-training model needs to conform to the model structure used in the current task. In other words, the pre-training model must also be trained with the model structure of the current architecture. Therefore, the only difference is whether the machine equipment can support large-scale dataset training.&lt;/p&gt;

&lt;p&gt;The purpose of this is to increase training efficiency. In fact, many pre-training models already have relatively good prediction efficiency, but it should be noted that the data set during training of the pre-training model should have a target value similar to the current prediction task. In our case, there are two target categories in the Coco dataset that are similar to the two categories of the currently used Open Image dataset, namely Street Sign (COCO) corresponding to Traffic sign (Open Image) and Stop sign (COCO) and Stop sign ( Open Image).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training Work Flow&lt;/strong&gt;&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism-part2/01-training.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism-part2/01-training.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Training workflow&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;We train in two steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Warm-up stage: Load the weights of the pre-trained model. For the first 50 epochs, the first 60 layers of pre-trained model hidden layers are frozen. These 60 layers of hidden layers are the backbone feature extraction network. The purpose of this is to speed up the training speed, because only the weights of the YOLO head part are updated, which protects the network weights from being influenced at the initial stage of training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unfreeze stage: Unfreeze the front 60 layers of backbone feature extraction network starting from the 51st epoch, and let the entire neural network participate in training. The training time will be greatly increased, but all weights will be updated in which the purpose is to make the backbone network fully adapt to the dataset of the current task.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Evaluation Metrics&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Generally speaking, for neural network models, it is hoped that the network model has a fast recognition speed, a small memory usage, and high detection accuracy. In terms of the current target detection problem, the general commonly used evaluation indicators are: mean average precision (map) and floating point operations per second (FLOPS). For the reason that our task only detects objects in the format of image files instead of video streaming, it is no longer needed to evaluate the detecting speed of the YOLO detector in our case, therefore mAP is the only metric we use to evaluate the performance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Precision and Recall&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Below follows the definition of precison and recall :&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism-part2/02-metric.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism-part2/02-metric.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Precision Recall&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;tp&lt;/strong&gt; represents true positive, &lt;strong&gt;fp&lt;/strong&gt; represents false positive, &lt;strong&gt;fn&lt;/strong&gt; represents false negative. The precision measures of all predictions made the percentage of correct positive predictions. The recall measures of all positive cases in reality the percentage of the correct positives cases are predicted. The trade-off between the two metrics depends on the task goal. In our case, the YOLO object detector is designed for traffic signs and stop signs detection which is related to traffic scenarios that are safety-sensitive, therefore to detects as many positive cases as possible from all positive cases is more important to increase the detection percentage among its own prediction, hence recall should be more primary to consider than precision. However, we only use precision and recall to calculate AP by drawing the PR-Curve.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Average Precision (AP) and PR-Curve&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The definition of &lt;strong&gt;AP&lt;/strong&gt; is defined as the area under the precision-recall curver (PR-curve). For multiple pairs of precision and recalls values, we get them through changing the confidence score threshold. The object that has a lower score than the threshold would be detected as a false positive.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism-part2/03-prcurve.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism-part2/03-prcurve.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;PR-Curve Example for Stop Sign&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;Given a PR-curve example shows above, the precision goes down once the recall goes up and vice versa. As a simple explanation to this pattern is, as more cases being detected positive cases, the precision increases due to at the same time, the overall detection cases increase as well but in smaller percentages. This would cause the recall to become smaller. Hence, the curve looks monotonically decreasing.&lt;/p&gt;

&lt;p&gt;Noted that the later number simply represents the IoU threshold values. For example, AP50 represents the IoU threshold that equals 50%.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mean Average Precision (mAP)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The metric mAP the averaged AP across all categories. In our cases, we have two categories Traffic sign and Stop signs, therefore the mAP in our experiment it simply means the summation of Traffic sign mAP and Stop sign mAP that divided by 2. The mAP is calculated after running the NMS algorithm. The mAP is the final evaluation metric of the detection model. After operations are completed, the final detection result is used to calculate the mAP value.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Copyright @ 2021 Zizhun Guo. All Rights Reserved.&lt;/p&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html">Training</summary></entry><entry><title type="html">YOLO Mechanism Part I: Input, Output, Encoding, Anchor Box, IoU, Decode, Confidence, NMS, Ground Truth, Loss</title><link href="http://localhost:4001/jekyll/update/projects/2021/03/31/YOLO-Mechanism-part1.html" rel="alternate" type="text/html" title="YOLO Mechanism Part I: Input, Output, Encoding, Anchor Box, IoU, Decode, Confidence, NMS, Ground Truth, Loss" /><published>2021-03-31T06:13:14-04:00</published><updated>2021-03-31T06:13:14-04:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/03/31/YOLO-Mechanism-part1</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/03/31/YOLO-Mechanism-part1.html">&lt;!-- ├── Project
   ├── classification
   │   ├── adversarial
   │   ├── contrast020
   │   ├── grayscale
   │   ├── noise100
   │   ├── noise200
   │   └── original
   └── App.py
       ├── contrast020
       ├── ensemble_adversarials
       ├── grayscale
       ├── noise100
       ├── noise200
       └── original --&gt;

&lt;p&gt;&lt;strong&gt;Mechanism&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Object Detection is a regression task that neural networks are required to output not only the predicting score and class for an object but also the bounding boxes. Figure 3 shows the basic mechanism of Yolo networks as the one-staged object detector.&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/01-mechanism.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/01-mechanism.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;YOLO Mechanism&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Label y&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As the figure[mechanism] shows, the output label is not the results output from the YOLO head layer, since the decode component would convert the bounding boxes’ encoding offsets into the real coordinates of the input image, Yolo Head works for filtering the real bound boxes and select ones that are expected to remain, therefore using the intermediate result – encodings as the labels ignores the additional operation and keeps the offsets as the normalized target values, which improved the efficiency to calculate the loss for the training process.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The shape of Input and Output&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The input image is sized in (m, 416, 416, 3), whereas m represents the number of batches set by the practitioners, (416, 416) represents the pixel numbers for the weight and height of the input images, and the 3 represents the channels numbers as RGB channels.&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/02-input-output.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/02-input-output.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Original Input Image vs Images in cells&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;The output encoding is sized in (m, 26, 26, 21) or (m, 13, 13, 21), whereas m is the same as the input image, due to the object detector is FPN structured, therefore two scaled feature maps are extracted but the network, however, both (26, 26) and (13, 13) represents the same meaning, which is the cells numbers of the original images. Yolo algorithm requires the image to be divided into cells into variable scales so that to enable extracting objects in different distances since the objects at a far distance would be small-sized in the image which takes fewer numbers of pixels than the objects in closer distance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Encodings&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The encodings are fetched by the encoding process (see Encode in figure 3) while assigning the ground true samples during the training session or they are the intermediate results representing the extracted features process (see Feature Extractor in figure 3) which are output by the network backbone. The tensor size for the embeddings contains the same number of ranks of the input image but the actual features are thereby translated and embedded into the special object detection format.&lt;/p&gt;

&lt;p&gt;The last dimension number of the output encoding represents the list of bounding boxes along with the recognized classes. In our case, we have two classes: Traffic Signs and Stop Signs. As the specific flatten format of the encodings can be reorganized in a more meaningful way, from figure 5, the last element of the output encoding is 21, which also can be represented in (3, 7). The first element of 3 indicates that there are three anchor boxes, which are set to vary the scales of detecting results, whereas the number of 7 represents p_c, t_x, t_y, t_w, t_h, c_1, c_2.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;p_c  : the confidence if it exists an object in current cell.&lt;/li&gt;
  &lt;li&gt;t_x  : the x coordinate of the offset to anchor box grid cell.&lt;/li&gt;
  &lt;li&gt;t_y  : the y coordinate of the offset to anchor box grid cell..&lt;/li&gt;
  &lt;li&gt;t_w  : the relative weight ratio for the anchor box.&lt;/li&gt;
  &lt;li&gt;t_h  : the relative height ratio for the anchor box.&lt;/li&gt;
  &lt;li&gt;c_1  : the probability for the object being as a Traffic Sign.&lt;/li&gt;
  &lt;li&gt;c_2  : the probability for the object being as a Stop Sign.&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/03-encodings.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/03-encodings.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Encoding in (13, 13, 3, 7)&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Anchor Box&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The concept of anchor box was originally introduced by Faster RCNN. The anchor box (also known as the bounding box prior in the paper, and the anchor box is used later) is the statistics (using k-means) from all the ground truth boxes in the training set and the most frequently appearing box shapes and sizes in the training set. These statistical prior (or human) experiences can be added to the model in advance so that when the model is learning, the model converges quickly.&lt;/p&gt;

&lt;p&gt;Another understanding about using anchor boxes is that the traditional object detection head used as the regressor can only detect one object. The performance for multiple object detection would be affected and interference due to the variant shape of the bounding boxes. To solve this issue, multiple regressors can be used which are all limited to specific detecting regions. To achieve such a mechanism, for each grid cell, there could set multiple anchor boxes in different shapes specifically in charge for detecting objects around the positions.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/04-anchor-boxes.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/04-anchor-boxes.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;3 Anchor Boxes for single route of feature maps&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;From 4.2 FPN and 5.3 Encodings parts, we have two routes of feature map output for each input image sample. Therefore, we have two features that are in tensors’ shape in (13, 13, 3, 7) and (26, 26, 3, 7). The third number 3 implies there are three anchor boxes that are used for specifying the regressors.&lt;/p&gt;

&lt;p&gt;For the route feature maps in the shape of (26, 26, 3, 7), we select (23,27), (37,58), (81,82) sized bounding boxes as the anchor boxes. For the route feature maps in the shape of (13, 13, 3, 7), we select (81,82), (135,169), (344,319) as the anchor boxes.&lt;/p&gt;

&lt;p&gt;As can be seen by the size values, for images divided into (26, 26) cells, each cell is smaller than the ones processed into (13, 13), hence the anchor boxes for (26, 26) cells are smaller in the number of pixels, whereas the (13, 13) cells have greater sized anchor boxes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Intersection over Union for bounding boxes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The bounding box is represented in a format of 4 numbers. In the different processes of the YOLO neural networks, the tensors of the bounding boxes are variant processed. For example, the bounding box values as the intermediate results that are produced after the encoding process are represented in the format of (t_x, t_y, t_w, t_h). This tensor cannot be directly used to reference the exact bounding box’s coordinates so that to use it as the prediction results. Therefore, such intermediate should be decoded into the real coordinates that have the midpoints, weights, and heights in size of the same scales of the real image. The actual bounding box is set to be in the format of (b_x, b_y, b_w, b_h) whereas b_x represents the bounding box mid point x-coordinate, b_y  represents the bounding box mid point y-coordinate, b_w   represents the weight of the bounding box and b_h  represents the height of the bounding box.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/05-IoU.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/05-IoU.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Bounding Box example&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;Intersection over Union (IoU) is an evaluation metric that can be used in the prediction task of bounding boxes in values of ranges. The metric applies to all shapes of objects. In YOLO neural networks, it serves the same purpose by calculating the ratio between the intersection area over the union area to measure the accuracy of bounding boxes regressor.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Decode&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As the anchor boxes have different sizes of the boxes, YOLO only predicts the bounding boxes in the format of (t_x, t_y, t_w, t_h) that bonds to different anchor boxes where each value represent the offsets and relative values, due to the model instability when in early training iteration[]. However, this form of coordinates does not affect the final location prediction. The decoding process would translate the temporary coordinates into the bounding boxes coordinates relevant to the real image size.&lt;/p&gt;

&lt;p&gt;b_x= σ(t_x )+ c_x&lt;/p&gt;

&lt;p&gt;b_y= σ(t_y )+ c_y&lt;/p&gt;

&lt;p&gt;b_w=p_w  e^(t_w )&lt;/p&gt;

&lt;p&gt;b_h=p_h  e^(t_h )&lt;/p&gt;

&lt;p&gt;b_x and b_y represent the bounding box center coordinates,  b_w and b_h represent the bounding box weight and height. t_x and t_y represent the level of the center point shifts reletively to the c_x and c_y which represent that the cell is offset from the top left corner of the image. The b_w and b_h represent the weight and height of the bounding box prior (also known as the anchor box).&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/06-decode.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/06-decode.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Bounding Box Decode&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;The Decode process transforms the feature maps in the shape of (t_x, t_y, t_w, t_h) into the shape of (b_x, b_y, b_w, b_h), which makes it possible to not only take the advantage of anchor box mechanism but also by constraining the values of bounding boxes so that to make the network more stable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Confidence Scores&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;YOLO defines the Confidence Scores as the product of the confidence and the Intersection of Union (IOU) of prediction and ground truth bounding boxes’ paris.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/07-confidence.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/07-confidence.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
&lt;/div&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;The Pr(〖Class〗_i&lt;/td&gt;
      &lt;td&gt;Object) from the equation in our case is c_1  and c_2. Therefore, the final confidence score is determined how much the overlapping ratio between the prediction and ground true bounding boxes and the object-specific probability.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Non-Max Suppression&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Non-maximum suppression (NMS) is an algorithm for removing non-maximum values. The simple understanding is to select all the partial substitutions by defining the part. In the process of object detection, a large number of candidate bounding boxes will be generated at the same object position. These candidate boxes may overlap with each other. In this situation, the NMS would first generates a detection box with the object detection confidence score, the detection boxes with the highest scores are replaced, and other detection boxes that have obvious overlap with the replaced detection boxes are suppressed.&lt;/p&gt;

&lt;p&gt;The progress of the Non-maximum suppression algorithm : (1) Sort bounding box list by confidence scores; (2) Select the bounding box with the highest confidence score and add it to the final output list, and delete it from the bounding box list; (3) Calculate the area of all bounding boxes; (4) Calculate the IoU of the bounding box with the highest confidence score and other additional boxes; (5) Delete bounding boxes with IoU greater than the threshold; (6) Repeat the above process until the bounding box list is empty.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/08-NMS.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/08-NMS.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;NMSe&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;The image on the left is the result of the candidate boxes of YOLO detection. Each bounding box has a confidence score. If NMS is not used, multiple candidate boxes will appear such as duplicate bounding boxes on the same object and bounding boxes on other objects which should not be seen as the target objects. The image on the right-hand side is the result of using non-maximum suppression, which fits the expectation to have two bounding boxes attached on two traffic signs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ground Truth Boxes Assignment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Before the training session, one necessary step is to define the positive and negative cases for target variables for each image sample. It is essential to assign the ground true bounding box in the appropriate anchor box. Since each object, has only one corresponding bounding box for labeling but exists multiple anchor box regressors. The strategy is to calculate the IoU between ground true bounding box with each anchor box and select the anchor box with the greatest IoU as the target.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Loss&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The loss function of YOLO v4 is mainly divided into three parts: bounding box regression loss, confidence loss and classification loss.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/09-loss.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/09-loss.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;The S and B represent the grid cells number and the bounding box priors (anchor boxes) number. The value of I_ij^obj parameter decides whether count the loss of the bounding boxes. If I_ij^obj equals 1, it means that the predicting bounding box matches the current anchor box, whereas if it is 0, the loss does not take into account.&lt;/p&gt;

&lt;p&gt;The G_ij plays a similar role as the I_ij^obj, it defines whether the confidence scores of the prediction result should be considered. For C_i^j =1, the predicted bounding box has the greatest IoU with ground truth box, hence C_i^j =0 for the cells of other anchor box types. One particular case is, the current anchor box which does not assign to detect a specific object class produces the IoU that is greater than the IoU threshold (YOLO paper is 0.5), G_ij would be 0 to neglect the loss calculation for it.&lt;/p&gt;

&lt;p&gt;In the original YOLO paper[], the loss function employs sum-squared error for it is easy to calculate. In our project, we use the binary cross-entropy as the cost for each grid cell and bounding box prior.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Copyright @ 2021 Zizhun Guo. All Rights Reserved.&lt;/p&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html"></summary></entry><entry><title type="html">YOLO Background &amp;amp; YOLO v4 tiny Archintecture</title><link href="http://localhost:4001/jekyll/update/projects/2021/03/12/YOLO-Background.html" rel="alternate" type="text/html" title="YOLO Background &amp; YOLO v4 tiny Archintecture" /><published>2021-03-12T03:54:12-05:00</published><updated>2021-03-12T03:54:12-05:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/03/12/YOLO-Background</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/03/12/YOLO-Background.html">&lt;!-- ├── Project
   ├── classification
   │   ├── adversarial
   │   ├── contrast020
   │   ├── grayscale
   │   ├── noise100
   │   ├── noise200
   │   └── original
   └── App.py
       ├── contrast020
       ├── ensemble_adversarials
       ├── grayscale
       ├── noise100
       ├── noise200
       └── original --&gt;

&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Object detection is the main direction of computer vision and digital image processing. It has important practical significance to reduce the consumption of human capital through computer vision in many fields such as robot navigation, intelligent video surveillance, industrial detection, and aerospace. Therefore, object detection has become a research hotspot in theory and application. It is an important branch of image processing and computer vision and is also a core part of intelligent monitoring systems. At the same time, object detection is also a basic algorithm in the field of generic recognition. It plays a vital role in subsequent tasks such as face recognition, gait recognition, crowd counting, and instance segmentation. The detection method mainly introduces two target detection algorithm ideas based on deep learning, a one-stage object detection algorithm, and a two-stage target detection algorithm.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-12-YOLO-Background/01-example.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-12-YOLO-Background/01-example.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Example: Object Detection Output &lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;The task of object detection can be divided into two key subtasks: object classification and object localization. The object classification task is responsible for judging whether there are objects of the interest category in the input image or the selected image area (Proposals) and output the possibility of a series of scored labels indicating that the objects of the interest category appear in the input image or the selected image area (Proposals). The object localization task is responsible for determining the position and range of the object of interest in the input image or the selected image area (Proposals), the bounding box of the output object, or the center of the object, or the closed boundary of the object, etc., usually a square bounding box, namely Bounding Box is used to represent the location information of an object.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Existing Approaches&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The current mainstream object detection algorithms are mainly based on deep learning models, which can be roughly divided into two categories: (1) One-Stage target detection algorithms: this type of detection algorithm does not require the Region Proposal stage, and can directly generate the category probability of the object and the position coordinate value, the more typical algorithms are YOLO, SSD and CornerNet; (2) Two-Stage target detection algorithm: this kind of detection algorithm divides the detection problem into two stages, the first stage generates the candidate region (Region Proposals), which contains the approximate location information of the object, and then classifies and refines the location of the candidate area in the second stage. Typical representatives of this type of algorithm are R-CNN, Fast R-CNN, Faster R-CNN, etc.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-12-YOLO-Background/02-onestaged-vs-twostaged.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-12-YOLO-Background/02-onestaged-vs-twostaged.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;One-Stage Detector vs Two-Stage Detector &lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;The One-Stage object detection algorithm can directly generate the category probability and position coordinate value of the object in one stage. Compared with the two-stage object detection algorithm, the Region Proposal stage is not required, and the overall process is simpler. As shown in the figure above, the input image is output through the backbone and neck of the networks, and the corresponding detection frame can be generated by decoding (post-processing) in dense prediction, whereas the two-staged object detection has one more step sparse prediction to process.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Model Architecture&lt;/strong&gt; 
For our study, we choose to use a One-Stage object detection model to conduct the experiements.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CSPDarknet53-tiny&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Darknet53 was first proposed by YOLOv4. Its main goal is to use as a Feature Extractor to embed images into vectors to perform post-prediction regression calculations after decoding. Therefore, as the backbone feature extractor, its role is to encode the image into smaller sized feature maps which contain the unique information about the input image.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CSP&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The CSP in CSPdarknet represents CSPNet architecture, which stands for Cross Stage Partial Network [1], its main purpose is to enable the architecture to achieve richer gradient combination information while reducing the amount of calculation. This goal can be achieved by dividing the feature map of the base layer into two parts and then merging them through the proposed cross-stage hierarchical structure. See CSPBlock structure in Figure 1, as the module adopting CSP architecture as part of the CSPDarknet53-tiny, it is composed of partial dense layer and partial convolutional layer. It is similar to the proposed CSPDenseNet which keeps the benefits of DenseNet’s feature for reusing characteristics, but meanwhile prevents an additional amount of duplicate gradient information by truncating the gradient flow [1]. In CSPDarknet53-tiny, the CSPBlock module divides the feature map into two parts, and combines the two parts by cross stage residual edge. This makes the gradient flow can propagate in two different network paths to increase the correlation difference of gradient information [2].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;FPN&lt;/strong&gt;
Through the FPN algorithm, the high-layer features of low-resolution and high-semantic information and the low-layer features of high-resolution and low-semantic information are connected from the top to the bottom side, so that to make the semantic information from features at all scales rich. Prediction on feature layers of different scales makes the effect of generating proposals better than the YOLOv2 Darknet19 backbone network and other traditional feature extraction algorithms such as VGG16 that only predict at the top level. As Figure 1 shows, using FPN can effectively reduce detection time [2]，it employs two-scale predictions (shape size in 26x26 and 13x13), conducting convolution layer and up-sampling layer at last feature layer in shapes of 26x26 and 13x13 so that to perform feature fusion.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-12-YOLO-Background/03-CSPDarkent53-tiny.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-12-YOLO-Background/03-CSPDarkent53-tiny.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Example: Architecture of CSPDarknet53-tiny for COCO Dataset&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;Through the above modules, the input image is extracted from the backbone feature network through the backbone feature network, and the two branch layer features are extracted through the CSP structure. After feature fusion is performed through the FPN algorithm, the two encoded image features have been obtained. The next part is to perform decoding process on these feature maps and run YOLO algorithm to make predictions. These processes are implemented by the YOLO Head in the last part of the result.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;YOLO Head&lt;/strong&gt;
The shape of the feature maps output from the above feature detectors is either (26, 26, 255) or (13, 13, 255). Since YOLO, it has a grid system that divides the image into smaller cells, therefore the first two coordinates represent the number of cells in total. The last coordinate represents the encoded information about the images. The details of how encoding works would be illustrated in the next part.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Copyright @ 2021 Zizhun Guo. All Rights Reserved.&lt;/p&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html"></summary></entry><entry><title type="html">YOLOv4 Deployment on AWS EC2 for Traffic/Stop Signs (TensorFlow + Flask + nginx + gunicorn)</title><link href="http://localhost:4001/jekyll/update/projects/2021/03/01/YOLOv4-Deployment-AWSEC2.html" rel="alternate" type="text/html" title="YOLOv4 Deployment on AWS EC2 for Traffic/Stop Signs (TensorFlow + Flask + nginx + gunicorn)" /><published>2021-03-01T15:13:14-05:00</published><updated>2021-03-01T15:13:14-05:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/03/01/YOLOv4-Deployment-AWSEC2</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/03/01/YOLOv4-Deployment-AWSEC2.html">&lt;!-- ├── Project
   ├── classification
   │   ├── adversarial
   │   ├── contrast020
   │   ├── grayscale
   │   ├── noise100
   │   ├── noise200
   │   └── original
   └── App.py
       ├── contrast020
       ├── ensemble_adversarials
       ├── grayscale
       ├── noise100
       ├── noise200
       └── original --&gt;

&lt;p&gt;&lt;strong&gt;Preparation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;YOLOv3 python program&lt;/li&gt;
  &lt;li&gt;Flask APP&lt;/li&gt;
  &lt;li&gt;AWS EC2 instance&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Make sure YOLOv3 works fine: input an image and output the corresponding image plotting the predicted category score and bounding boxes.&lt;/li&gt;
  &lt;li&gt;Create a Flask App, design a web page templete that matches the routes.&lt;/li&gt;
  &lt;li&gt;Create an ubuntu EC2 Instance with default setting. Follow the rules by &lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/connection-prereqs.html#connection-prereqs-get-info-about-instance&quot;&gt;General prerequisites for connecting to your instance&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;Open Security Group and open inbound SSH connection on local IP address.&lt;/li&gt;
      &lt;li&gt;Use PuTTy create SSH session for CLI mode remotely control the instance. Use winSCP create SFTP session for file uploading to the instance. Detail in &lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html&quot;&gt;Connect to your Linux instance from Windows using PuTTY&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Configure the instance by upgrade/update/install environment for being able to running the YOLO application:
    &lt;ul&gt;
      &lt;li&gt;Python3.8&lt;/li&gt;
      &lt;li&gt;TensorFlow 2.3.1&lt;/li&gt;
      &lt;li&gt;Pillow&lt;/li&gt;
      &lt;li&gt;opencv-python&lt;/li&gt;
      &lt;li&gt;matplotlib&lt;/li&gt;
      &lt;li&gt;Flask&lt;/li&gt;
      &lt;li&gt;Gunicorn&lt;/li&gt;
      &lt;li&gt;nginx&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Deploy it online (Details in (Deploy flask app with Nginx using Gunicorn)[https://medium.com/faun/deploy-flask-app-with-nginx-using-gunicorn-7fda4f50066a]):
    &lt;ul&gt;
      &lt;li&gt;Upload the project to the instance&lt;/li&gt;
      &lt;li&gt;Create the WSGI Entry Point wsgi.py&lt;/li&gt;
      &lt;li&gt;Serve the project by Gunicorn&lt;/li&gt;
      &lt;li&gt;Configuring Nginx&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt apt-get install python3.8
python3.8 -m pip install --upgrade pip
python3.8 -m pip install tensorflow==2.3.1
python3.8 -m pip install Pillow
python3.8 -m pip install opencv-python
python3.8 -m pip install matplotlib
python3.8 -m pip install Flask
python3.8 -m pip install Gunicorn
python3.8 -m pip install tnginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Application URL&lt;/strong&gt;
&lt;a href=&quot;http://3.16.135.19/home&quot;&gt;http://3.16.135.19/home&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Application screenshot&lt;/strong&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/Yolo_deployment.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/Yolo_deployment.png&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;YOLO project web page&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Input example&lt;/strong&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/test.jpg&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/test.jpg&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Test Image&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/img_output.jpg&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/img_output.jpg&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Predicted Image&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Results Screenshot&lt;/strong&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/output_screenshot.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/output_screenshot.png&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Results Screenshot&lt;/figcaption&gt;
&lt;/div&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/1_zGC7qRcsw4G9I9u9KjMqaQ.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/1_zGC7qRcsw4G9I9u9KjMqaQ.png&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Deployment Architecture&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Problems:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Unicorn Server crash frequently when uploading greater size image.&lt;/li&gt;
  &lt;li&gt;The result of predicted image may stay unchanged though using different input image&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Potential Solutions:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Deploy a supervisor program to manage server automatically by restarting the server.&lt;/li&gt;
  &lt;li&gt;The browser keep the cache for the previous results since the name of the output image is the same. Mechanism for storing the output image on server could be changed to solve it.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://faun.pub/deploy-flask-app-with-nginx-using-gunicorn-7fda4f50066a&quot;&gt;Deploy flask app with Nginx using Gunicorn:&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/32254439&quot;&gt;python web 部署：nginx + gunicorn + supervisor + flask 部署笔记:&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtu.be/tW6jtOOGVJI&quot;&gt;Deployment video:&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html"></summary></entry><entry><title type="html">Object Detection YOLO Mindmap</title><link href="http://localhost:4001/jekyll/update/projects/2021/02/25/YOLO-Mindmap-copy.html" rel="alternate" type="text/html" title="Object Detection YOLO Mindmap" /><published>2021-02-25T15:13:14-05:00</published><updated>2021-02-25T15:13:14-05:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/02/25/YOLO-Mindmap%20copy</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/02/25/YOLO-Mindmap-copy.html">&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-02-25-YOLO-Mindmap/YOLO.jpg&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-02-25-YOLO-Mindmap/YOLO.jpg&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;YOLO project resource overview&lt;/figcaption&gt;
&lt;/div&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html">YOLO project resource overview</summary></entry><entry><title type="html">Certificate/Courses: DeepLearning.AI Deep Learning Specialization Mindmap/Notes/Certificate</title><link href="http://localhost:4001/jekyll/update/projects/2021/01/07/Deep-Learning-Specialization.html" rel="alternate" type="text/html" title="Certificate/Courses: DeepLearning.AI Deep Learning Specialization Mindmap/Notes/Certificate" /><published>2021-01-07T15:13:14-05:00</published><updated>2021-01-07T15:13:14-05:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/01/07/Deep-Learning-Specialization</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/01/07/Deep-Learning-Specialization.html">&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-01-07-Deep-Learning-Specialization/DL_coursera.jpg&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-01-07-Deep-Learning-Specialization/DL_coursera.jpg&quot; alt=&quot;drawing&quot; style=&quot;width: 70%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Deep Learning Specialization Notes&lt;/figcaption&gt;
&lt;/div&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;https://www.coursera.org/account/accomplishments/specialization/XZQ953AVUJUT&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-01-07-Deep-Learning-Specialization/certificate.png&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;&lt;a href=&quot;https://www.coursera.org/account/accomplishments/specialization/XZQ953AVUJUT&quot;&gt;Certificate Link: https://www.coursera.org/account/accomplishments/specialization/XZQ953AVUJUT&lt;/a&gt;&lt;/figcaption&gt;
&lt;/div&gt;

&lt;h3 id=&quot;resources&quot;&gt;Resources&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/fengdu78/deeplearning_ai_books&quot;&gt;Course Notes CN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/mbadry1/DeepLearning.ai-Summary&quot;&gt;Course Notes EN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://baozoulin.gitbook.io/neural-networks-and-deep-learning/di-er-men-ke-gai-shan-shen-ceng-shen-jing-wang-luo-chao-can-shu-tiao-shi-zheng-ze-hua-yi-ji-you-hua/improving-deep-neural-networks/practical-aspects-of-deep-learning/14-zheng-ze-hua-ff08-regularization&quot;&gt;Course Summary CN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html">Deep Learning Specialization Notes</summary></entry><entry><title type="html">Certificate/Courses: TensorFlow Developer Certificate</title><link href="http://localhost:4001/jekyll/update/projects/2021/01/07/TensorFlow-Developer-Certificate-copy.html" rel="alternate" type="text/html" title="Certificate/Courses: TensorFlow Developer Certificate" /><published>2021-01-07T15:13:14-05:00</published><updated>2021-01-07T15:13:14-05:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/01/07/TensorFlow-Developer-Certificate%20copy</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/01/07/TensorFlow-Developer-Certificate-copy.html">&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-01-31-TensorFlow-Developer-Certificate/TensorFlow_Certificate.jpg&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-01-31-TensorFlow-Developer-Certificate/TensorFlow_Certificate.jpg&quot; alt=&quot;drawing&quot; style=&quot;width: 70%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;TensorFlow Developer Certificate Notes in Details&lt;/figcaption&gt;
&lt;/div&gt;

&lt;h3 id=&quot;reference-courses-for-preparation&quot;&gt;Reference Courses for preparation:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.coursera.org/professional-certificates/tensorflow-in-practice?utm_source=gg&amp;amp;utm_medium=sem&amp;amp;utm_campaign=33-DeepLearningAI-TensorFlow-US&amp;amp;utm_content=33-DeepLearningAI-TensorFlow-US&amp;amp;campaignid=12447638588&amp;amp;adgroupid=120038996082&amp;amp;device=c&amp;amp;keyword=tensorflow%20developer&amp;amp;matchtype=p&amp;amp;network=g&amp;amp;devicemodel=&amp;amp;adpostion=&amp;amp;creativeid=501889850765&amp;amp;hide_mobile_promo&amp;amp;gclid=Cj0KCQiA1pyCBhCtARIsAHaY_5cDU3bKFC0azYWxzLnTFIZbYzlxpwXN0-W4ci2PlqnRIxTcVf247iIaAmPYEALw_wcB&quot;&gt;Coursera(semi-Official)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/lmoroney/dlaicourse&quot;&gt;Coursera(semi-Official) - Github&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://classroom.udacity.com/courses/ud187&quot;&gt;Udacity(Official)- Intro to TensorFlow for Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;https://www.credential.net/2f7e7dc7-237d-4fbc-b8af-0d40c82155e8#gs.v5yedg&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-01-31-TensorFlow-Developer-Certificate/certificate.png&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;&lt;a href=&quot;https://www.credential.net/2f7e7dc7-237d-4fbc-b8af-0d40c82155e8#gs.v5yedg&quot;&gt;Certificate Link: https://www.credential.net/2f7e7dc7-237d-4fbc-b8af-0d40c82155e8#gs.v5yedg&lt;/a&gt;&lt;/figcaption&gt;
&lt;/div&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html">TensorFlow Developer Certificate Notes in Details</summary></entry></feed>