<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4001/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4001/" rel="alternate" type="text/html" /><updated>2021-04-04T23:37:51-04:00</updated><id>http://localhost:4001/feed.xml</id><title type="html">Your awesome title</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">YOLO Mechanism: Input, Output, Encoding, Anchor Box, IoU, Decode, Confidence, NMS, Ground Truth, Loss</title><link href="http://localhost:4001/jekyll/update/projects/2021/03/31/YOLO-Mechanism.html" rel="alternate" type="text/html" title="YOLO Mechanism: Input, Output, Encoding, Anchor Box, IoU, Decode, Confidence, NMS, Ground Truth, Loss" /><published>2021-03-31T06:13:14-04:00</published><updated>2021-03-31T06:13:14-04:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/03/31/YOLO-Mechanism</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/03/31/YOLO-Mechanism.html">&lt;!-- ├── Project
   ├── classification
   │   ├── adversarial
   │   ├── contrast020
   │   ├── grayscale
   │   ├── noise100
   │   ├── noise200
   │   └── original
   └── App.py
       ├── contrast020
       ├── ensemble_adversarials
       ├── grayscale
       ├── noise100
       ├── noise200
       └── original --&gt;

&lt;p&gt;&lt;strong&gt;Mechanism&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Object Detection is a regression task that neural networks are required to output not only the predicting score and class for an object but also the bounding boxes. Figure 3 shows the basic mechanism of Yolo networks as the one-staged object detector.&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/01-mechanism.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/01-mechanism.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;YOLO Mechanism&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Label y&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As the figure[mechanism] shows, the output label is not the results output from the YOLO head layer, since the decode component would convert the bounding boxes’ encoding offsets into the real coordinates of the input image, Yolo Head works for filtering the real bound boxes and select ones that are expected to remain, therefore using the intermediate result – encodings as the labels ignores the additional operation and keeps the offsets as the normalized target values, which improved the efficiency to calculate the loss for the training process.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The shape of Input and Output&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The input image is sized in (m, 416, 416, 3), whereas m represents the number of batches set by the practitioners, (416, 416) represents the pixel numbers for the weight and height of the input images, and the 3 represents the channels numbers as RGB channels.&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/02-input-output.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/02-input-output.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Original Input Image vs Images in cells&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;The output encoding is sized in (m, 26, 26, 21) or (m, 13, 13, 21), whereas m is the same as the input image, due to the object detector is FPN structured, therefore two scaled feature maps are extracted but the network, however, both (26, 26) and (13, 13) represents the same meaning, which is the cells numbers of the original images. Yolo algorithm requires the image to be divided into cells into variable scales so that to enable extracting objects in different distances since the objects at a far distance would be small-sized in the image which takes fewer numbers of pixels than the objects in closer distance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Encodings&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The encodings are fetched by the encoding process (see Encode in figure 3) while assigning the ground true samples during the training session or they are the intermediate results representing the extracted features process (see Feature Extractor in figure 3) which are output by the network backbone. The tensor size for the embeddings contains the same number of ranks of the input image but the actual features are thereby translated and embedded into the special object detection format.&lt;/p&gt;

&lt;p&gt;The last dimension number of the output encoding represents the list of bounding boxes along with the recognized classes. In our case, we have two classes: Traffic Signs and Stop Signs. As the specific flatten format of the encodings can be reorganized in a more meaningful way, from figure 5, the last element of the output encoding is 21, which also can be represented in (3, 7). The first element of 3 indicates that there are three anchor boxes, which are set to vary the scales of detecting results, whereas the number of 7 represents p_c, t_x, t_y, t_w, t_h, c_1, c_2.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;p_c  : the confidence if it exists an object in current cell.&lt;/li&gt;
  &lt;li&gt;t_x  : the x coordinate of the offset to anchor box grid cell.&lt;/li&gt;
  &lt;li&gt;t_y  : the y coordinate of the offset to anchor box grid cell..&lt;/li&gt;
  &lt;li&gt;t_w  : the relative weight ratio for the anchor box.&lt;/li&gt;
  &lt;li&gt;t_h  : the relative height ratio for the anchor box.&lt;/li&gt;
  &lt;li&gt;c_1  : the probability for the object being as a Traffic Sign.&lt;/li&gt;
  &lt;li&gt;c_2  : the probability for the object being as a Stop Sign.&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/03-encodings.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/03-encodings.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Encoding in (13, 13, 3, 7)&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Anchor Box&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The concept of anchor box was originally introduced by Faster RCNN. The anchor box (also known as the bounding box prior in the paper, and the anchor box is used later) is the statistics (using k-means) from all the ground truth boxes in the training set and the most frequently appearing box shapes and sizes in the training set. These statistical prior (or human) experiences can be added to the model in advance so that when the model is learning, the model converges quickly.&lt;/p&gt;

&lt;p&gt;Another understanding about using anchor boxes is that the traditional object detection head used as the regressor can only detect one object. The performance for multiple object detection would be affected and interference due to the variant shape of the bounding boxes. To solve this issue, multiple regressors can be used which are all limited to specific detecting regions. To achieve such a mechanism, for each grid cell, there could set multiple anchor boxes in different shapes specifically in charge for detecting objects around the positions.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/04-anchor-boxes.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/04-anchor-boxes.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;3 Anchor Boxes for single route of feature maps&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;From 4.2 FPN and 5.3 Encodings parts, we have two routes of feature map output for each input image sample. Therefore, we have two features that are in tensors’ shape in (13, 13, 3, 7) and (26, 26, 3, 7). The third number 3 implies there are three anchor boxes that are used for specifying the regressors.&lt;/p&gt;

&lt;p&gt;For the route feature maps in the shape of (26, 26, 3, 7), we select (23,27), (37,58), (81,82) sized bounding boxes as the anchor boxes. For the route feature maps in the shape of (13, 13, 3, 7), we select (81,82), (135,169), (344,319) as the anchor boxes.&lt;/p&gt;

&lt;p&gt;As can be seen by the size values, for images divided into (26, 26) cells, each cell is smaller than the ones processed into (13, 13), hence the anchor boxes for (26, 26) cells are smaller in the number of pixels, whereas the (13, 13) cells have greater sized anchor boxes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Intersection over Union for bounding boxes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The bounding box is represented in a format of 4 numbers. In the different processes of the YOLO neural networks, the tensors of the bounding boxes are variant processed. For example, the bounding box values as the intermediate results that are produced after the encoding process are represented in the format of (t_x, t_y, t_w, t_h). This tensor cannot be directly used to reference the exact bounding box’s coordinates so that to use it as the prediction results. Therefore, such intermediate should be decoded into the real coordinates that have the midpoints, weights, and heights in size of the same scales of the real image. The actual bounding box is set to be in the format of (b_x, b_y, b_w, b_h) whereas b_x represents the bounding box mid point x-coordinate, b_y  represents the bounding box mid point y-coordinate, b_w   represents the weight of the bounding box and b_h  represents the height of the bounding box.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/05-IoU.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/05-IoU.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Bounding Box example&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;Intersection over Union (IoU) is an evaluation metric that can be used in the prediction task of bounding boxes in values of ranges. The metric applies to all shapes of objects. In YOLO neural networks, it serves the same purpose by calculating the ratio between the intersection area over the union area to measure the accuracy of bounding boxes regressor.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Decode&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As the anchor boxes have different sizes of the boxes, YOLO only predicts the bounding boxes in the format of (t_x, t_y, t_w, t_h) that bonds to different anchor boxes where each value represent the offsets and relative values, due to the model instability when in early training iteration[]. However, this form of coordinates does not affect the final location prediction. The decoding process would translate the temporary coordinates into the bounding boxes coordinates relevant to the real image size.&lt;/p&gt;

&lt;p&gt;b_x= σ(t_x )+ c_x&lt;/p&gt;

&lt;p&gt;b_y= σ(t_y )+ c_y&lt;/p&gt;

&lt;p&gt;b_w=p_w  e^(t_w )&lt;/p&gt;

&lt;p&gt;b_h=p_h  e^(t_h )&lt;/p&gt;

&lt;p&gt;b_x and b_y represent the bounding box center coordinates,  b_w and b_h represent the bounding box weight and height. t_x and t_y represent the level of the center point shifts reletively to the c_x and c_y which represent that the cell is offset from the top left corner of the image. The b_w and b_h represent the weight and height of the bounding box prior (also known as the anchor box).&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/06-decode.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/06-decode.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Bounding Box Decode&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;The Decode process transforms the feature maps in the shape of (t_x, t_y, t_w, t_h) into the shape of (b_x, b_y, b_w, b_h), which makes it possible to not only take the advantage of anchor box mechanism but also by constraining the values of bounding boxes so that to make the network more stable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Confidence Scores&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;YOLO defines the Confidence Scores as the product of the confidence and the Intersection of Union (IOU) of prediction and ground truth bounding boxes’ paris.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/07-confidence.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/07-confidence.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
&lt;/div&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;The Pr(〖Class〗_i&lt;/td&gt;
      &lt;td&gt;Object) from the equation in our case is c_1  and c_2. Therefore, the final confidence score is determined how much the overlapping ratio between the prediction and ground true bounding boxes and the object-specific probability.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Non-Max Suppression&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Non-maximum suppression (NMS) is an algorithm for removing non-maximum values. The simple understanding is to select all the partial substitutions by defining the part. In the process of object detection, a large number of candidate bounding boxes will be generated at the same object position. These candidate boxes may overlap with each other. In this situation, the NMS would first generates a detection box with the object detection confidence score, the detection boxes with the highest scores are replaced, and other detection boxes that have obvious overlap with the replaced detection boxes are suppressed.&lt;/p&gt;

&lt;p&gt;The progress of the Non-maximum suppression algorithm : (1) Sort bounding box list by confidence scores; (2) Select the bounding box with the highest confidence score and add it to the final output list, and delete it from the bounding box list; (3) Calculate the area of all bounding boxes; (4) Calculate the IoU of the bounding box with the highest confidence score and other additional boxes; (5) Delete bounding boxes with IoU greater than the threshold; (6) Repeat the above process until the bounding box list is empty.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/08-NMS.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/08-NMS.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;NMSe&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;The image on the left is the result of the candidate boxes of YOLO detection. Each bounding box has a confidence score. If NMS is not used, multiple candidate boxes will appear such as duplicate bounding boxes on the same object and bounding boxes on other objects which should not be seen as the target objects. The image on the right-hand side is the result of using non-maximum suppression, which fits the expectation to have two bounding boxes attached on two traffic signs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ground Truth Boxes Assignment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Before the training session, one necessary step is to define the positive and negative cases for target variables for each image sample. It is essential to assign the ground true bounding box in the appropriate anchor box. Since each object, has only one corresponding bounding box for labeling but exists multiple anchor box regressors. The strategy is to calculate the IoU between ground true bounding box with each anchor box and select the anchor box with the greatest IoU as the target.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Loss&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The loss function of YOLO v4 is mainly divided into three parts: bounding box regression loss, confidence loss and classification loss.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/09-loss.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/09-loss.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;The S and B represent the grid cells number and the bounding box priors (anchor boxes) number. The value of I_ij^obj parameter decides whether count the loss of the bounding boxes. If I_ij^obj equals 1, it means that the predicting bounding box matches the current anchor box, whereas if it is 0, the loss does not take into account.&lt;/p&gt;

&lt;p&gt;The G_ij plays a similar role as the I_ij^obj, it defines whether the confidence scores of the prediction result should be considered. For C_i^j =1, the predicted bounding box has the greatest IoU with ground truth box, hence C_i^j =0 for the cells of other anchor box types. One particular case is, the current anchor box which does not assign to detect a specific object class produces the IoU that is greater than the IoU threshold (YOLO paper is 0.5), G_ij would be 0 to neglect the loss calculation for it.&lt;/p&gt;

&lt;p&gt;In the original YOLO paper[], the loss function employs sum-squared error for it is easy to calculate. In our project, we use the binary cross-entropy as the cost for each grid cell and bounding box prior.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Copyright @ 2021 Zizhun Guo. All Rights Reserved.&lt;/p&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html"></summary></entry><entry><title type="html">YOLO Background &amp;amp; YOLO v4 tiny Archintecture</title><link href="http://localhost:4001/jekyll/update/projects/2021/03/12/YOLO-Background.html" rel="alternate" type="text/html" title="YOLO Background &amp; YOLO v4 tiny Archintecture" /><published>2021-03-12T03:54:12-05:00</published><updated>2021-03-12T03:54:12-05:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/03/12/YOLO-Background</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/03/12/YOLO-Background.html">&lt;!-- ├── Project
   ├── classification
   │   ├── adversarial
   │   ├── contrast020
   │   ├── grayscale
   │   ├── noise100
   │   ├── noise200
   │   └── original
   └── App.py
       ├── contrast020
       ├── ensemble_adversarials
       ├── grayscale
       ├── noise100
       ├── noise200
       └── original --&gt;

&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Object detection is the main direction of computer vision and digital image processing. It has important practical significance to reduce the consumption of human capital through computer vision in many fields such as robot navigation, intelligent video surveillance, industrial detection, and aerospace. Therefore, object detection has become a research hotspot in theory and application. It is an important branch of image processing and computer vision and is also a core part of intelligent monitoring systems. At the same time, object detection is also a basic algorithm in the field of generic recognition. It plays a vital role in subsequent tasks such as face recognition, gait recognition, crowd counting, and instance segmentation. The detection method mainly introduces two target detection algorithm ideas based on deep learning, a one-stage object detection algorithm, and a two-stage target detection algorithm.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-12-YOLO-Background/01-example.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-12-YOLO-Background/01-example.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Example: Object Detection Output &lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;The task of object detection can be divided into two key subtasks: object classification and object localization. The object classification task is responsible for judging whether there are objects of the interest category in the input image or the selected image area (Proposals) and output the possibility of a series of scored labels indicating that the objects of the interest category appear in the input image or the selected image area (Proposals). The object localization task is responsible for determining the position and range of the object of interest in the input image or the selected image area (Proposals), the bounding box of the output object, or the center of the object, or the closed boundary of the object, etc., usually a square bounding box, namely Bounding Box is used to represent the location information of an object.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Existing Approaches&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The current mainstream object detection algorithms are mainly based on deep learning models, which can be roughly divided into two categories: (1) One-Stage target detection algorithms: this type of detection algorithm does not require the Region Proposal stage, and can directly generate the category probability of the object and the position coordinate value, the more typical algorithms are YOLO, SSD and CornerNet; (2) Two-Stage target detection algorithm: this kind of detection algorithm divides the detection problem into two stages, the first stage generates the candidate region (Region Proposals), which contains the approximate location information of the object, and then classifies and refines the location of the candidate area in the second stage. Typical representatives of this type of algorithm are R-CNN, Fast R-CNN, Faster R-CNN, etc.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-12-YOLO-Background/02-onestaged-vs-twostaged.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-12-YOLO-Background/02-onestaged-vs-twostaged.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;One-Stage Detector vs Two-Stage Detector &lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;The One-Stage object detection algorithm can directly generate the category probability and position coordinate value of the object in one stage. Compared with the two-stage object detection algorithm, the Region Proposal stage is not required, and the overall process is simpler. As shown in the figure above, the input image is output through the backbone and neck of the networks, and the corresponding detection frame can be generated by decoding (post-processing) in dense prediction, whereas the two-staged object detection has one more step sparse prediction to process.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Model Architecture&lt;/strong&gt; 
For our study, we choose to use a One-Stage object detection model to conduct the experiements.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CSPDarknet53-tiny&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Darknet53 was first proposed by YOLOv4. Its main goal is to use as a Feature Extractor to embed images into vectors to perform post-prediction regression calculations after decoding. Therefore, as the backbone feature extractor, its role is to encode the image into smaller sized feature maps which contain the unique information about the input image.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CSP&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The CSP in CSPdarknet represents CSPNet architecture, which stands for Cross Stage Partial Network [1], its main purpose is to enable the architecture to achieve richer gradient combination information while reducing the amount of calculation. This goal can be achieved by dividing the feature map of the base layer into two parts and then merging them through the proposed cross-stage hierarchical structure. See CSPBlock structure in Figure 1, as the module adopting CSP architecture as part of the CSPDarknet53-tiny, it is composed of partial dense layer and partial convolutional layer. It is similar to the proposed CSPDenseNet which keeps the benefits of DenseNet’s feature for reusing characteristics, but meanwhile prevents an additional amount of duplicate gradient information by truncating the gradient flow [1]. In CSPDarknet53-tiny, the CSPBlock module divides the feature map into two parts, and combines the two parts by cross stage residual edge. This makes the gradient flow can propagate in two different network paths to increase the correlation difference of gradient information [2].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;FPN&lt;/strong&gt;
Through the FPN algorithm, the high-layer features of low-resolution and high-semantic information and the low-layer features of high-resolution and low-semantic information are connected from the top to the bottom side, so that to make the semantic information from features at all scales rich. Prediction on feature layers of different scales makes the effect of generating proposals better than the YOLOv2 Darknet19 backbone network and other traditional feature extraction algorithms such as VGG16 that only predict at the top level. As Figure 1 shows, using FPN can effectively reduce detection time [2]，it employs two-scale predictions (shape size in 26x26 and 13x13), conducting convolution layer and up-sampling layer at last feature layer in shapes of 26x26 and 13x13 so that to perform feature fusion.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-12-YOLO-Background/03-CSPDarkent53-tiny.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-12-YOLO-Background/03-CSPDarkent53-tiny.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Example: Architecture of CSPDarknet53-tiny for COCO Dataset&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;Through the above modules, the input image is extracted from the backbone feature network through the backbone feature network, and the two branch layer features are extracted through the CSP structure. After feature fusion is performed through the FPN algorithm, the two encoded image features have been obtained. The next part is to perform decoding process on these feature maps and run YOLO algorithm to make predictions. These processes are implemented by the YOLO Head in the last part of the result.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;YOLO Head&lt;/strong&gt;
The shape of the feature maps output from the above feature detectors is either (26, 26, 255) or (13, 13, 255). Since YOLO, it has a grid system that divides the image into smaller cells, therefore the first two coordinates represent the number of cells in total. The last coordinate represents the encoded information about the images. The details of how encoding works would be illustrated in the next part.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Copyright @ 2021 Zizhun Guo. All Rights Reserved.&lt;/p&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html"></summary></entry><entry><title type="html">YOLOv3 Deployment on AWS EC2 for Traffic/Stop Signs (TensorFlow + Flask + nginx + gunicorn)</title><link href="http://localhost:4001/jekyll/update/projects/2021/03/01/YOLOv3-Deployment-AWSEC2.html" rel="alternate" type="text/html" title="YOLOv3 Deployment on AWS EC2 for Traffic/Stop Signs (TensorFlow + Flask + nginx + gunicorn)" /><published>2021-03-01T15:13:14-05:00</published><updated>2021-03-01T15:13:14-05:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/03/01/YOLOv3-Deployment-AWSEC2</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/03/01/YOLOv3-Deployment-AWSEC2.html">&lt;!-- ├── Project
   ├── classification
   │   ├── adversarial
   │   ├── contrast020
   │   ├── grayscale
   │   ├── noise100
   │   ├── noise200
   │   └── original
   └── App.py
       ├── contrast020
       ├── ensemble_adversarials
       ├── grayscale
       ├── noise100
       ├── noise200
       └── original --&gt;

&lt;p&gt;&lt;strong&gt;Preparation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;YOLOv3 python program&lt;/li&gt;
  &lt;li&gt;Flask APP&lt;/li&gt;
  &lt;li&gt;AWS EC2 instance&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Make sure YOLOv3 works fine: input an image and output the corresponding image plotting the predicted category score and bounding boxes.&lt;/li&gt;
  &lt;li&gt;Create a Flask App, design a web page templete that matches the routes.&lt;/li&gt;
  &lt;li&gt;Create an ubuntu EC2 Instance with default setting. Follow the rules by &lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/connection-prereqs.html#connection-prereqs-get-info-about-instance&quot;&gt;General prerequisites for connecting to your instance&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;Open Security Group and open inbound SSH connection on local IP address.&lt;/li&gt;
      &lt;li&gt;Use PuTTy create SSH session for CLI mode remotely control the instance. Use winSCP create SFTP session for file uploading to the instance. Detail in &lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html&quot;&gt;Connect to your Linux instance from Windows using PuTTY&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Configure the instance by upgrade/update/install environment for being able to running the YOLO application:
    &lt;ul&gt;
      &lt;li&gt;Python3.8&lt;/li&gt;
      &lt;li&gt;TensorFlow 2.3.1&lt;/li&gt;
      &lt;li&gt;Pillow&lt;/li&gt;
      &lt;li&gt;opencv-python&lt;/li&gt;
      &lt;li&gt;matplotlib&lt;/li&gt;
      &lt;li&gt;Flask&lt;/li&gt;
      &lt;li&gt;Gunicorn&lt;/li&gt;
      &lt;li&gt;nginx&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Deploy it online (Details in (Deploy flask app with Nginx using Gunicorn)[https://medium.com/faun/deploy-flask-app-with-nginx-using-gunicorn-7fda4f50066a]):
    &lt;ul&gt;
      &lt;li&gt;Upload the project to the instance&lt;/li&gt;
      &lt;li&gt;Create the WSGI Entry Point wsgi.py&lt;/li&gt;
      &lt;li&gt;Serve the project by Gunicorn&lt;/li&gt;
      &lt;li&gt;Configuring Nginx&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt apt-get install python3.8
python3.8 -m pip install --upgrade pip
python3.8 -m pip install tensorflow==2.3.1
python3.8 -m pip install Pillow
python3.8 -m pip install opencv-python
python3.8 -m pip install matplotlib
python3.8 -m pip install Flask
python3.8 -m pip install Gunicorn
python3.8 -m pip install tnginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Application URL&lt;/strong&gt;
&lt;a href=&quot;http://3.16.135.19/home&quot;&gt;http://3.16.135.19/home&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Application screenshot&lt;/strong&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/Yolo_deployment.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/Yolo_deployment.png&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;YOLO project web page&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Input example&lt;/strong&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/test.jpg&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/test.jpg&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Test Image&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/img_output.jpg&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/img_output.jpg&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Predicted Image&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Results Screenshot&lt;/strong&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/output_screenshot.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/output_screenshot.png&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Results Screenshot&lt;/figcaption&gt;
&lt;/div&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/1_zGC7qRcsw4G9I9u9KjMqaQ.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/1_zGC7qRcsw4G9I9u9KjMqaQ.png&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Deployment Architecture&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Problems:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Unicorn Server crash frequently when uploading greater size image.&lt;/li&gt;
  &lt;li&gt;The result of predicted image may stay unchanged though using different input image&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Potential Solutions:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Deploy a supervisor program to manage server automatically by restarting the server.&lt;/li&gt;
  &lt;li&gt;The browser keep the cache for the previous results since the name of the output image is the same. Mechanism for storing the output image on server could be changed to solve it.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=tW6jtOOGVJI&quot;&gt;Deploy flask app with Nginx using Gunicorn:&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/32254439&quot;&gt;python web 部署：nginx + gunicorn + supervisor + flask 部署笔记:&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtu.be/tW6jtOOGVJI&quot;&gt;Deployment video:&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html"></summary></entry><entry><title type="html">Object Detection YOLO Mindmap</title><link href="http://localhost:4001/jekyll/update/projects/2021/02/25/YOLO-Mindmap-copy.html" rel="alternate" type="text/html" title="Object Detection YOLO Mindmap" /><published>2021-02-25T15:13:14-05:00</published><updated>2021-02-25T15:13:14-05:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/02/25/YOLO-Mindmap%20copy</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/02/25/YOLO-Mindmap-copy.html">&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-02-25-YOLO-Mindmap/YOLO.jpg&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-02-25-YOLO-Mindmap/YOLO.jpg&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;YOLO project resource overview&lt;/figcaption&gt;
&lt;/div&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html">YOLO project resource overview</summary></entry><entry><title type="html">Certificate/Courses: TensorFlow Developer Certificate</title><link href="http://localhost:4001/jekyll/update/projects/2021/01/07/TensorFlow-Developer-Certificate-copy.html" rel="alternate" type="text/html" title="Certificate/Courses: TensorFlow Developer Certificate" /><published>2021-01-07T15:13:14-05:00</published><updated>2021-01-07T15:13:14-05:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/01/07/TensorFlow-Developer-Certificate%20copy</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/01/07/TensorFlow-Developer-Certificate-copy.html">&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-01-31-TensorFlow-Developer-Certificate/TensorFlow_Certificate.jpg&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-01-31-TensorFlow-Developer-Certificate/TensorFlow_Certificate.jpg&quot; alt=&quot;drawing&quot; style=&quot;width: 70%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;TensorFlow Developer Certificate Notes in Details&lt;/figcaption&gt;
&lt;/div&gt;

&lt;h3 id=&quot;reference-courses-for-preparation&quot;&gt;Reference Courses for preparation:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.coursera.org/professional-certificates/tensorflow-in-practice?utm_source=gg&amp;amp;utm_medium=sem&amp;amp;utm_campaign=33-DeepLearningAI-TensorFlow-US&amp;amp;utm_content=33-DeepLearningAI-TensorFlow-US&amp;amp;campaignid=12447638588&amp;amp;adgroupid=120038996082&amp;amp;device=c&amp;amp;keyword=tensorflow%20developer&amp;amp;matchtype=p&amp;amp;network=g&amp;amp;devicemodel=&amp;amp;adpostion=&amp;amp;creativeid=501889850765&amp;amp;hide_mobile_promo&amp;amp;gclid=Cj0KCQiA1pyCBhCtARIsAHaY_5cDU3bKFC0azYWxzLnTFIZbYzlxpwXN0-W4ci2PlqnRIxTcVf247iIaAmPYEALw_wcB&quot;&gt;Coursera(semi-Official)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/lmoroney/dlaicourse&quot;&gt;Coursera(semi-Official) - Github&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://classroom.udacity.com/courses/ud187&quot;&gt;Udacity(Official)- Intro to TensorFlow for Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;https://www.credential.net/2f7e7dc7-237d-4fbc-b8af-0d40c82155e8#gs.v5yedg&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-01-31-TensorFlow-Developer-Certificate/certificate.png&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;&lt;a href=&quot;https://www.credential.net/2f7e7dc7-237d-4fbc-b8af-0d40c82155e8#gs.v5yedg&quot;&gt;Certificate Link: https://www.credential.net/2f7e7dc7-237d-4fbc-b8af-0d40c82155e8#gs.v5yedg&lt;/a&gt;&lt;/figcaption&gt;
&lt;/div&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html">TensorFlow Developer Certificate Notes in Details</summary></entry><entry><title type="html">Certificate/Courses: DeepLearning.AI Deep Learning Specialization Mindmap/Notes/Certificate</title><link href="http://localhost:4001/jekyll/update/projects/2021/01/07/Deep-Learning-Specialization.html" rel="alternate" type="text/html" title="Certificate/Courses: DeepLearning.AI Deep Learning Specialization Mindmap/Notes/Certificate" /><published>2021-01-07T15:13:14-05:00</published><updated>2021-01-07T15:13:14-05:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/01/07/Deep-Learning-Specialization</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/01/07/Deep-Learning-Specialization.html">&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-01-07-Deep-Learning-Specialization/DL_coursera.jpg&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-01-07-Deep-Learning-Specialization/DL_coursera.jpg&quot; alt=&quot;drawing&quot; style=&quot;width: 70%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Deep Learning Specialization Notes&lt;/figcaption&gt;
&lt;/div&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;https://www.coursera.org/account/accomplishments/specialization/XZQ953AVUJUT&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-01-07-Deep-Learning-Specialization/certificate.png&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;&lt;a href=&quot;https://www.coursera.org/account/accomplishments/specialization/XZQ953AVUJUT&quot;&gt;Certificate Link: https://www.coursera.org/account/accomplishments/specialization/XZQ953AVUJUT&lt;/a&gt;&lt;/figcaption&gt;
&lt;/div&gt;

&lt;h3 id=&quot;resources&quot;&gt;Resources&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/fengdu78/deeplearning_ai_books&quot;&gt;Course Notes CN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/mbadry1/DeepLearning.ai-Summary&quot;&gt;Course Notes EN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://baozoulin.gitbook.io/neural-networks-and-deep-learning/di-er-men-ke-gai-shan-shen-ceng-shen-jing-wang-luo-chao-can-shu-tiao-shi-zheng-ze-hua-yi-ji-you-hua/improving-deep-neural-networks/practical-aspects-of-deep-learning/14-zheng-ze-hua-ff08-regularization&quot;&gt;Course Summary CN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html">Deep Learning Specialization Notes</summary></entry><entry><title type="html">(8/8)NN Models - Text Mining for Multiclass Classification based on Yelp User’s Reviews</title><link href="http://localhost:4001/jekyll/update/projects/2020/12/15/Text-Mining-NN_Models.html" rel="alternate" type="text/html" title="(8/8)NN Models - Text Mining for Multiclass Classification based on Yelp User’s Reviews" /><published>2020-12-15T13:50:07-05:00</published><updated>2020-12-15T13:50:07-05:00</updated><id>http://localhost:4001/jekyll/update/projects/2020/12/15/Text-Mining-NN_Models</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2020/12/15/Text-Mining-NN_Models.html">&lt;h3 id=&quot;neural-network-models&quot;&gt;Neural Network Models&lt;/h3&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;python-files&quot;&gt;Python Files&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;create_CNN_model.py&lt;/strong&gt;: This files contains the python code to create the CNN model used for the experiments.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;create_RNN_model.py&lt;/strong&gt;: This files contains the python code to create the RNN model used for the experiments.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;tuning_CNN_model.py&lt;/strong&gt;: This files contains the python code for tuning the CNN model used for the experiments.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1-tensorflow-keras-embedding-layer&quot;&gt;1. Tensorflow Keras Embedding Layer&lt;/h3&gt;

&lt;p&gt;The embedding layer is a special layer that is normally placed at the beginning of a neural network. Similar to W2V, it is an individual layer trained to work as a lookup table for the one-hot encoding input documents. The weights matrix represents the word embeddings. The documents which are made of the list of sequence would be converted into a stack of word vectors. We employ the average pooling layer to produce the mean word embeddings to represent a single document for normal W2V cases, but in our CNN model or RNN models, we would use more strategies to use the embedding layer’s output. Besides, the weights matrix of the embedding layer would be trained along with the other parameters of the neural network using the back-propagation method.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4001/assets/2020-12-15-Text-Mining/images/NN_models/Embedding_layer_model.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1: Textual Model Structure using TensorFlow Keras Embedding Layer&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;2-cnn&quot;&gt;2. CNN&lt;/h3&gt;
&lt;p&gt;As one of famous deep learning models, CNN has been widely used in computer vision tasks. Since the year of 2014, the CNN has been discovered to be proved successfully workable not only the computer vision jobs but also the text classification.&lt;/p&gt;

&lt;h5 id=&quot;21-model-architecture&quot;&gt;2.1. Model Architecture&lt;/h5&gt;

&lt;p&gt;In our experiment, we implement a CNN model with its architecture designed based on the Yoon Kim’s architecture. The Embedding Layer works for training the word embeddings as it was been discussed previously. The Convolutional Layer has hundreds of feature maps as filters to convolute the document representation. The Global Max Layer is charged for keeping the most unique value to represent an independent document among all. The Dropout layer works as a regularization role. At last, the Softmax layer computes the probability distribution for three categories. We then classify the sample into the category which has the greatest value. In our experiments, we use the TensorFlow Keras deep learning library to implement this CovNet.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4001/assets/2020-12-15-Text-Mining/images/NN_models/CNN_architecture.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2: CNN model architecture&lt;/strong&gt;&lt;/p&gt;

&lt;h5 id=&quot;22-hyperparameters-tuning&quot;&gt;2.2. Hyperparameters Tuning&lt;/h5&gt;
&lt;p&gt;In order to find the CNN model that produces the highest prediction accuracy, we have tuned the model regarding three hyperparameters that are hugely affected the model’s performance: Sequence Size, Filter Region Size and Feature Maps numbers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tuning Sequence Size:&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Sequence Size&lt;/th&gt;
      &lt;th&gt;Val_Acc&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;0.722&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;0.7905&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;0.835167&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;0.874&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;0.906667&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;200&lt;/td&gt;
      &lt;td&gt;0.926667&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;400&lt;/td&gt;
      &lt;td&gt;0.938167&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;800&lt;/td&gt;
      &lt;td&gt;0.944&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1000&lt;/td&gt;
      &lt;td&gt;0.9475&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4001/assets/2020-12-15-Text-Mining/images/NN_models/acc_sequence_sizes_forReport.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3: CNN model train/validate Accuracy vs Embedding Sequence Sizes&lt;/strong&gt;
&lt;strong&gt;Tuning Filter Region Size:&lt;/strong&gt;
| Sequence Size   | Val_Acc |
| ———– | ———– |
|1	|0.946667|
|3	|0.947833|
|5	|0.945833|
|7	|0.944333|
|10	|0.941833|
|15	|0.942|
|20	|0.938833|
|25	|0.937333|
|30	|0.9385|&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4001/assets/2020-12-15-Text-Mining/images/NN_models/acc_filter_region_sizes_forReport.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4: CNN model train/validate Accuracy vs Embedding Filter Region Sizes&lt;/strong&gt;
&lt;strong&gt;Tuning Embedding Trainable Methods:&lt;/strong&gt;
| Trainable  | Val_Acc |
| ———– | ———– |
|non-static	|0.948667|
|static	|0.9475|&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tuning Filter Feature Maps Numbers:&lt;/strong&gt;
|Feature Maps Numbers|Val_Acc|
| ———– | ———– |
|10|	0.925167|
|20|	0.936|
|30|	0.937667|
|50|	0.943167|
|100|	0.947167|
|200|	0.948667|
|400|	0.948833|
|800|	0.951667|
|1000|	0.951833|
&lt;img src=&quot;http://localhost:4001/assets/2020-12-15-Text-Mining/images/NN_models/acc_filter_featuremaps_numbers_forReport.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 5: CNN model train/validate Accuracy vs Featuremaps Number&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Configuration&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Input word vectors:	GloVe 300d&lt;/li&gt;
  &lt;li&gt;Static or non-static:	static&lt;/li&gt;
  &lt;li&gt;Filter region size (window):	3&lt;/li&gt;
  &lt;li&gt;Stride:	1&lt;/li&gt;
  &lt;li&gt;Feature maps:	1000&lt;/li&gt;
  &lt;li&gt;Activation function:	ReLU&lt;/li&gt;
  &lt;li&gt;Pooling: 	GlobalMaxPooling&lt;/li&gt;
  &lt;li&gt;Dropout rate:	0.5&lt;/li&gt;
  &lt;li&gt;Optimizer:	Adam&lt;/li&gt;
  &lt;li&gt;Batch size:	128&lt;/li&gt;
  &lt;li&gt;Epochs:	20&lt;/li&gt;
  &lt;li&gt;Cross Validation:	10-folds&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3-rnn-model-architecture&quot;&gt;3. RNN Model Architecture&lt;/h3&gt;
&lt;h5 id=&quot;31-encoder-architecture&quot;&gt;3.1. Encoder Architecture&lt;/h5&gt;
&lt;p&gt;Recurrent Neural Network (RNN) has long been used for learning the sequence of documents employing a directed node graph along temporal steps. We adopt the Encoder part of the RNN-Encoder-Decoder model as our RNN architecture since the architecture fits the requirement of the text classification task that documents can be input along with the temporal steps and in the end, the memory unit will produce a document representation. As seen from the figure 5, the encoder takes in a variable-length sequence 〖(X〗_1,X_2,…,X_T)and converts it into a fixed-length vector representation C. 
&lt;img src=&quot;http://localhost:4001/assets/2020-12-15-Text-Mining/images/NN_models/RNN-Encoder.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 6: RNN Encoder Model&lt;/strong&gt;&lt;/p&gt;

&lt;h5 id=&quot;32-gru-and-lstm&quot;&gt;3.2. GRU and LSTM&lt;/h5&gt;
&lt;p&gt;LSTM has a longer history than GRU. It was firstly introduced in 1997 and was modified throughout the time. New features like forget gate and peephole would add during the evolution. The GRU is a newer generation of RNN work similar to LSTM that enables memorizing the short-term and long-term dependencies by implementing a reset gate and an update gate during training. There is no memory cell as it is called in LSTM but has a hidden state works the same function. The figure 6 shows the inner structure of two memory units.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4001/assets/2020-12-15-Text-Mining/images/NN_models/GRU_LSTM.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 7: Unit Comparison between naïve RNN, LSTM and GRU&lt;/strong&gt;&lt;/p&gt;

&lt;h5 id=&quot;33-model-architecture&quot;&gt;3.3. Model Architecture&lt;/h5&gt;
&lt;p&gt;In our experiment, we implement a RNN decoder model with its architecture designed based on the Cho’s architecture. It works the same way as it was described in the CNN model’s section. The document-representation sends the batches of samples in the Input Layer at the beginning. After the embedding process by the Embedding Layer, the embeddings are continuously transferred to GRU layer. Through a recurrent time-based learning process using the Backpropagation method, the weights of the model would be updated. The final layer is the Softmax Layer which works to classify the samples into three categories.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4001/assets/2020-12-15-Text-Mining/images/NN_models/RNN_GRU.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 8: RNN-GRU model architecture&lt;/strong&gt;
As for memory unit, the Encoder model can use either Gated Recurrent Unit (GRU) or Long-short term Memory (LSTM) to memorize the historical dependencies as in the form of the hidden state.&lt;/p&gt;

&lt;h3 id=&quot;appendix&quot;&gt;Appendix&lt;/h3&gt;

&lt;h5 id=&quot;a1-other-layers-details&quot;&gt;A1. Other layers details&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;Global Max Pooling Layer&lt;/strong&gt;
The output tensors from convolution layer yields concatenate results of each 128 filters’ hence the original matrix turns into the vector. For each sequence, we only keep the maximum value which maintains the review local feature.
&lt;strong&gt;Concatenate Layer and Flatten Layer&lt;/strong&gt;
We concatenate all results of three types of filters and flatten readily for the neurons to take.
&lt;strong&gt;Regularization: Dropout Layer&lt;/strong&gt;
We assign a Dropout Layer as regularization masks to help prevent overfitting issue.
&lt;strong&gt;Fully Connected Layer&lt;/strong&gt;
We only assign a dense layer as the output layer using activation function of Softmax.&lt;/p&gt;

&lt;h5 id=&quot;a2-tensorflow-keras-summary&quot;&gt;A2. TensorFlow Keras Summary&lt;/h5&gt;
&lt;p&gt;•	Utils.plot_model: Plot Model diagram. It requires pydot and graphviz, which should be installed to python lib and set executable directory into system environment path.
•	Optimizers: Provides different classes of optimize rule like ‘Adam’, ‘Adadelta’, etc.
•	Layers: All types of NN layers like ‘Flatten’, ‘Concatenate’, etc
•	Experimental.preprocessing.TextVectorization&lt;/p&gt;

&lt;h5 id=&quot;a3-gpu-acceleration&quot;&gt;A3. GPU Acceleration&lt;/h5&gt;
&lt;p&gt;NVIDIA GPU with CUDA architectures supports acceleration for TensorFlow, therefore by using GPU to process DL tasks is good. Currently, my machine has a video card of GeForce GTX 1060 6GB, which has the compute Capability of 6.1. They way to implement it is to install CUDA and NVCC on the machine. The TF will automatically search out the installation and use it while training. After getting deeper knowledge about TF, specific operation controlling GPU/CPU as multiple purpose can be achieve by coding without Keras package instead the self-configure TF package.&lt;/p&gt;

&lt;h3 id=&quot;resources&quot;&gt;Resources&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Implementing a CNN for Text Classification in TensorFlow: http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/&lt;/li&gt;
  &lt;li&gt;Understanding Convolutional Neural Networks for NLP: http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/&lt;/li&gt;
  &lt;li&gt;Global Max Pooling vs Max Pooling: https://stackoverflow.com/questions/43728235/what-is-the-difference-between-keras-maxpooling1d-and-globalmaxpooling1d-functi/43730861
Multi Class Text Classification with Keras and LSTM: https://medium.com/@djajafer/multi-class-text-classification-with-keras-and-lstm-4c5525bef592
Report on Text Classification using CNN, RNN &amp;amp; HAN: https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f
Text classification with an RNN: https://www.tensorflow.org/tutorials/text/text_classification_rnn&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html">Neural Network Models</summary></entry><entry><title type="html">(7/8)ML Models - Text Mining for Multiclass Classification based on Yelp User’s Reviews</title><link href="http://localhost:4001/jekyll/update/projects/2020/12/15/Text-Mining-ML-Models.html" rel="alternate" type="text/html" title="(7/8)ML Models - Text Mining for Multiclass Classification based on Yelp User’s Reviews" /><published>2020-12-15T13:50:06-05:00</published><updated>2020-12-15T13:50:06-05:00</updated><id>http://localhost:4001/jekyll/update/projects/2020/12/15/Text-Mining-ML-Models</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2020/12/15/Text-Mining-ML-Models.html">&lt;h3 id=&quot;ml-models&quot;&gt;ML Models&lt;/h3&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;python-files&quot;&gt;Python Files&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;tuning_ML_models.py&lt;/strong&gt;: This files contains the python code to tune the ML models.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1-perceptron-model&quot;&gt;1. Perceptron Model&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;11-purpose&quot;&gt;1.1. Purpose&lt;/h4&gt;
    &lt;ul&gt;
      &lt;li&gt;Tune the model by iterating values of two hyperparameters to compare results and analyze how it works with the aids of mathematical formulas&lt;/li&gt;
      &lt;li&gt;Understand L1/L2 norm and how it affects the loss function&lt;/li&gt;
      &lt;li&gt;Revise gradient descent and learn perceptron with back-propagation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;12-hyperparameters&quot;&gt;1.2. Hyperparameters&lt;/h4&gt;
    &lt;ul&gt;
      &lt;li&gt;Stepping Criteria: tol&lt;/li&gt;
      &lt;li&gt;Regularization Constant: alpha&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;13-principle&quot;&gt;1.3. Principle:&lt;/h4&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Stopping Criteria ‘tol’&lt;/strong&gt;
  The perceptron learning model adopts SGD to fit itself. The way to compute the gradients is through back-propagation. It implements the chain rule to update the weights of the perceptron model, as it is the same for many other neural network structures. Stopping criteria as one essential hyper-parameter of the model control when to stop for the training process. Together with max_iteration times (which defines the ending condition), it somehow affects the model performance.
        &lt;blockquote&gt;
          &lt;h5 id=&quot;loss--errory-y-1&quot;&gt;$Loss = Error(y, y’)$ (1)&lt;/h5&gt;
          &lt;h5 id=&quot;loss--previousloss---tol--2&quot;&gt;$Loss &amp;gt; previousLoss - tol$  (2)&lt;/h5&gt;
          &lt;h5 id=&quot;y---y--y---y---tol-3&quot;&gt;$y’ - y &amp;gt; y’’ - y - tol$ (3)&lt;/h5&gt;
          &lt;p&gt;(1) and (2) are the definition of Loss function and how it is implemented for the stopping criteria. From (1) and (2), we get (3) that once the difference between two loss gets less than ‘tol’, in other words, the changing rate of loss becomes acceptable, the training process would stop as the model is considered convergent.&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Alpha&lt;/strong&gt;
  Alpha is a regularization rate that determines how much the l1/l2 norm(in our case) affects the loss function. If it is high, the greater proportion the regularization takes, so loss functions tend to prone to smaller weights, therefore as it gets convergent, the model is less overfitted, since the smaller weights decide less change once the predict on samples.
        &lt;blockquote&gt;
          &lt;p&gt;$L_1 = L + \alpha |W|$ (4)
$L_2 = L + \alpha W^2$ (5)
$W’ = W - \alpha \Delta (6)$&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;(4) and (5) are the formula that describe the Loss function aided by the regularization. (6) shows that each iteration of the gradient decent, the weights decreases more. Therefore as the model converges, the weights are smaller than what the regularization is not included.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;14-results-and-analysis&quot;&gt;1.4. Results and Analysis&lt;/h4&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;‘tol’&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;When the ‘tol’ is in the range between $1/10^1$ to $1/10^2$, the f1 is irregularly chaotic, this may own to the high ‘tol’ value that terminates the iteration process sooner than it is supposed to be converged.&lt;/li&gt;
          &lt;li&gt;In contrast, once ‘tol’ gets smaller enough (in the range between $1/10^3$ to $1/10^4$), the f1 socre is more stable. 
  &lt;img src=&quot;http://localhost:4001/assets/2020-12-15-Text-Mining/images/ML_models/perceptron_tol.png&quot; alt=&quot;Paris&quot; class=&quot;center&quot; /&gt;&lt;/li&gt;
        &lt;/ul&gt;
        &lt;div align=&quot;center&quot;&gt;Figure 1: F1 score of Perceptron model vs Stopping Criteria&lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;‘alpha’&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Overall trend: f1 scores increases in sigmoid-like shape, of which the alpha value is high (left-hand side) and small on the right-hand side.&lt;/li&gt;
          &lt;li&gt;When the alpha value is high, the regularization affects more in loss function, the optimal weights produced from training serves lower performance due to the penalty from the regularization.&lt;/li&gt;
          &lt;li&gt;When the alpha value is low, the model tends to be overfitted since the weights of the perceptron model get greater.&lt;/li&gt;
          &lt;li&gt;Comparing L1 norm and L2 norm, once the perceptron model arrives convergence (alpha equals around $1/10^3$), the L2 model is slightly less performed than L1 model. This can be seen as a result of the compromise that with the same alpha value assigned, L2 norm prone to produce lower weights(weight decay) hence sacrifice some precision and recall.
&lt;img src=&quot;http://localhost:4001/assets/2020-12-15-Text-Mining/images/ML_models/perceptron_alpha.png&quot; alt=&quot;Paris&quot; class=&quot;center&quot; /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&quot;center&quot;&gt;Figure 1: F1 score of Perceptron model vs Stopping Criteria&lt;/div&gt;

&lt;h3 id=&quot;2-other-ml-models-lrsvm&quot;&gt;2. Other ML models: LR/SVM&lt;/h3&gt;
&lt;p&gt;Please check out textual models &lt;a href=&quot;../textuals_models/README.md&quot;&gt;README.md&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;resources&quot;&gt;Resources&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261#1d17&quot;&gt;Intuitions on L1 and L2 Regularisation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/weight-decay-l2-regularization-90a9e17713cd&quot;&gt;Weight Decay == L2 Regularization?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036&quot;&gt;Regularization in Deep Learning — L1, L2, and Dropout&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kdnuggets.com/2019/12/5-techniques-prevent-overfitting-neural-networks.html#:~:text=One%20of%20the%20most%20common,data%20that%20is%20too%20noisy.&amp;amp;text=The%20goal%20of%20a%20machine,data%20from%20the%20problem%20domain.&quot;&gt;5 Techniques to Prevent Overfitting in Neural Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/37953585/what-is-the-difference-between-sgd-and-back-propagation&quot;&gt;What is the difference between SGD and back-propagation?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html">ML Models Python Files tuning_ML_models.py: This files contains the python code to tune the ML models.</summary></entry><entry><title type="html">(6/8)Experiment - Text Mining for Multiclass Classification based on Yelp User’s Reviews</title><link href="http://localhost:4001/jekyll/update/projects/2020/12/15/Text-Mining-Experiment.html" rel="alternate" type="text/html" title="(6/8)Experiment - Text Mining for Multiclass Classification based on Yelp User’s Reviews" /><published>2020-12-15T13:50:05-05:00</published><updated>2020-12-15T13:50:05-05:00</updated><id>http://localhost:4001/jekyll/update/projects/2020/12/15/Text-Mining-Experiment</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2020/12/15/Text-Mining-Experiment.html">&lt;h3 id=&quot;experiment&quot;&gt;Experiment&lt;/h3&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;python-files&quot;&gt;Python Files&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;run_ML.py&lt;/strong&gt;: This files contains the python code to run traditional ML models.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;run_RNN.py&lt;/strong&gt;: This files contains the python code to run the RNN model.
    &lt;h3 id=&quot;library&quot;&gt;Library&lt;/h3&gt;
  &lt;/li&gt;
  &lt;li&gt;TensorFlow Keras
    &lt;h3 id=&quot;rnn-model&quot;&gt;RNN model&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;rnn-gru-architecture&quot;&gt;RNN-GRU architecture&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;../images/experiment/rnn_gru_model.png&quot; alt=&quot;&quot; /&gt;
Figure 1: RNN-GRU&lt;/p&gt;

&lt;h5 id=&quot;rnn-bidirectional-lstm-architecture&quot;&gt;RNN-Bidirectional LSTM architecture&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;../images/experiment/rnn_model_BiLSTM.png&quot; alt=&quot;&quot; /&gt;
Figure 2: RNN-BiLSTM&lt;/p&gt;

&lt;h5 id=&quot;some-important-details&quot;&gt;Some important details:&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;Single Layered RNN-GRU and Bidirectional RNN-LSTM is are the two models that would eventually converge to produce a decent accuracy. However, the other two models – Multi-layered RNN-GRU and Single layered LSTM cannot converge.&lt;/li&gt;
  &lt;li&gt;The training/testing observation showed that if the loss for each iteration becomes below 1.0, the model would be quickly converged in the next 2-3 epochs.&lt;/li&gt;
  &lt;li&gt;The single layered RNN-GRU is the encoder model of RNN-Encoder Decoder.&lt;/li&gt;
  &lt;li&gt;Word Embedding dimensions: 300&lt;/li&gt;
  &lt;li&gt;Hidden States dimensions: 64, 128 (bidirectional RNN-LSTM)&lt;/li&gt;
  &lt;li&gt;Max sequence length: 1000 (padded with 0)&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;results&quot;&gt;Results&lt;/h5&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Models&lt;/th&gt;
      &lt;th&gt;Training&lt;/th&gt;
      &lt;th&gt;Testing&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Single Layered RNN-GRU (encoder)&lt;/td&gt;
      &lt;td&gt;0.9628888905048371&lt;/td&gt;
      &lt;td&gt;0.9414999961853028&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Bidirectional RNN-LSTM&lt;/td&gt;
      &lt;td&gt;0.9951481461524964&lt;/td&gt;
      &lt;td&gt;0.9171666741371155&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h5 id=&quot;some-resources&quot;&gt;Some resources:&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;How to implement Seq2Seq LSTM Model in Keras: https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639&lt;/li&gt;
  &lt;li&gt;Difference Between Return Sequences and Return States for LSTMs in Keras: https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/&lt;/li&gt;
  &lt;li&gt;Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras: https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/&lt;/li&gt;
  &lt;li&gt;A ten-minute introduction to sequence-to-sequence learning in Keras: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html&lt;/li&gt;
  &lt;li&gt;Understanding Tensorflow’s tensors shape: static and dynamic: https://pgaleone.eu/tensorflow/2018/07/28/understanding-tensorflow-tensors-shape-static-dynamic/&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html">Experiment</summary></entry><entry><title type="html">(5/8)Textual Models - Text Mining for Multiclass Classification based on Yelp User’s Reviews</title><link href="http://localhost:4001/jekyll/update/projects/2020/12/15/Text-Mining-Texual-Models.html" rel="alternate" type="text/html" title="(5/8)Textual Models - Text Mining for Multiclass Classification based on Yelp User’s Reviews" /><published>2020-12-15T13:50:04-05:00</published><updated>2020-12-15T13:50:04-05:00</updated><id>http://localhost:4001/jekyll/update/projects/2020/12/15/Text-Mining-Texual-Models</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2020/12/15/Text-Mining-Texual-Models.html">&lt;h3 id=&quot;textual-features&quot;&gt;Textual Features&lt;/h3&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;1-tuning-parameters-for-w2v-model-and-logistic-regression-model&quot;&gt;1. Tuning Parameters for W2V model and Logistic Regression model&lt;/h3&gt;

&lt;p&gt;Since the tasks are classification related hence the carefully selected classifiers play important roles. Based on the embedded words vectors from the W2V model, where the VxN weights matrix contains all vocabulary vectors where it linearly spans in a fixed(smaller than vocabulary total length) number of dimensions space. Here it introduces two different classifiers for the tuning works.&lt;/p&gt;

&lt;h5 id=&quot;11-f1-score-comparison-under-different-w2v-models-and-sizes-lr-solvers-and-penalties&quot;&gt;1.1. F1 Score comparison under different w2v models and sizes, LR solvers and penalties&lt;/h5&gt;

&lt;p&gt;&lt;strong&gt;w2v models&lt;/strong&gt;: Continuous-bag-of-words (CBOW) and Skip-grams (SG)
&lt;strong&gt;w2v sizes&lt;/strong&gt;: The number of dimensions (N) of the N-dimensional space that gensim Word2Vec maps the words onto.
&lt;strong&gt;LR solvers&lt;/strong&gt;: The algorithms to optimize the loss function.
&lt;strong&gt;LR penalty&lt;/strong&gt;: To specify the norm used in the penalization (regularization).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;W2V Params options:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The tuning process try fitting the model in SG and CBOW, under each version model, it fits the models infeature numbers in 12 options. 2 * 12 = 24 W2V rounds fitting.
```py {.line-numbers}&lt;/p&gt;
&lt;h1 id=&quot;w2v-params-options&quot;&gt;W2V params options&lt;/h1&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for sg in [0, 1]:
    for size in [10,20,30,40,50,100,150,200,250,300,400,500]:
        word2vec_model = Word2Vec(cutWords_list, sg = sg, size=size, iter=30, min_count=5,
                                    workers=multiprocessing.cpu_count()) ``` The tuning process would try l1 norm and liblinear first, and switch to l2 norm with 4 other penalties: liblinear, newton-cg, lbfgs and sag. (1 + 4) = 5 model training and testing for each W2V round.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;LR params options:

{'penalty': ['l1'], 'solver': ['liblinear']},
{'penalty': ['l2'], 'solver': ['liblinear','newton-cg', 'lbfgs', 'sag']}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The newton-cg, lbfgs and sag optimization algorithms require loss function being able to calculate first and second continuous derivative, hence L1-norm (manhattan distance) should not employ them. However, liblinear works fine in both L1-norm and L2-norm. The LR works fine for multiclass classification, but only in One-vs-Rest(OvR) for liblinear, which works worse than many-va-many(MvM) strategy in theories.
In total, the overall task tuning times are 24 * 5 = 120 times.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4001/assets/2020-12-15-Text-Mining/images/textual_models/LR_sizes_f1_new.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;W2V model analysis:&lt;/strong&gt;
SG overall performs better than CBOW from the beginning to end, where it has around 3 percent differences of F1 score, which is considered to be high contrast since the average F1 increase in variant W2V sizes are relatively small. Based on the images, the W2V models in the first 6 sizes (particularly 10, 20, 30, 40, 50, 100), the increase rate of F1 is high, the trend shows the promising upwards direction almost get vertical, after size becoming 100, the upward trends mitigate and gets more flattened. This is because the model fits convergence in which more features would not hugely benefit the classification and rather to intensify the machine resources. 
The recommend params are SG and size between 200 to 400 whereas the W2V models has 3356 vocabulary and 6000 samples numbers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LR model analysis:&lt;/strong&gt;
As previously mentioned, the liblinear algorithm works fine for multiclass classification but only in OvR strategy, therefore when performing multiclass LR, the F1 is smaller than other algorithms. The curve proves it that either by using SG or CBOW, the lines in colors blue, orange, brown, and pink always are less performative than other lines of algorithms. The results given by the other three algorithms show the lines are overlapped with each other. This is because the F1 score is too close (in 0.01% - 0.001% granularity) therefore the Matplotlib could not show the difference (It can be solved by plotting the logarithmic version of F1 comparison). This means at least for this experiment cases, the algorithms newton-cg, lbfgs and sag work well.&lt;/p&gt;

&lt;h5 id=&quot;12-f1-score-comparison-under-different-w2v-iteration-time-lr-solvers-and-penalties&quot;&gt;1.2. F1 Score comparison under different w2v Iteration time, LR solvers, and penalties&lt;/h5&gt;
&lt;p&gt;The iteration times decide how much the W2V is well trained in rounds just by comprehend words’ meanings. The tuning process takes different values of iteration parameters to check how this affects the final performance.
```py {.line-numbers}&lt;/p&gt;
&lt;h1 id=&quot;w2v-params-options-1&quot;&gt;W2V params options&lt;/h1&gt;

&lt;p&gt;for sg in [0, 1]:
    for iters in [1,2,3,4,5,10,15,20,30,40,50]:
        word2vec_model = Word2Vec(cutWords_list, sg = sg, size=300,iter=iters, min_count=5, workers=multiprocessing.cpu_count())&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;**Attention**: The feature size is set to be 300, which comes from the previous test that 300 is considered to be a good candidate to produce high F1 in the end.

![](http://localhost:4001/assets/2020-12-15-Text-Mining/images/textual_models/LR_iter_f1_new.png)
**W2V model Iteration time analysis:**
Same as previous experiments illustrated, SG has better performance than CBOW in the case dealing with a small number of samples, hence the images tell in the overall perspective, SG always performs better. The curves are sigmoid-like that in the range of 1 to 10 iteration times, the trend is exponential whereas each more iteration times gives a huge performance increase. It however converges to once the iteration times gets to 20 above, which makes sense that the embedding model is tuned to be the most fitted to the current samples.

---
### 2. Tuning Parameters for SVC model

##### 2.1. F1 Score comparison under different w2v models and sizes, LR solvers and penalties
**RBF** kernel function is used for tuning the SVC classifier. It has two essential parameters that determine the final performance. The SVC costs usually much more time than other classifiers when training/testing, but will be quick for prediction on validation datasets.

**SVC C**: The regularization parameter. The strength of the regularization is inversely proportional to C. This is a relaxation coefficient to balance the relationship between the complex of support vectors and the mis-classification rate. 
**SVC gamma**: This is a hyper-parameter that is commonly set to be 1/# features. This parameter defines how much a single sample can affect the hyper-plane.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;SVC params options&lt;/p&gt;

&lt;p&gt;param_grid = [
{‘estimator__kernel’: [‘rbf’], “estimator__C”:[0.1, 1.0, 10], “estimator__gamma”:[1.0, 0.1, 0.01, 0.03]}
]&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The tuning process will run 3 * 4 = 12 times for each param pair try.
![](http://localhost:4001/assets/2020-12-15-Text-Mining/images/textual_models/SVC_f1_C.png)
The images show that as the values of C gets bigger, the F1 increases. The trends apply to all combinations of gamma values. This is because as C gets larger, the loss function becomes more tolerant to those points in further space thus treat them as support vectors. However, it makes the hyper-planes become more complex which may cause the model to suffers overfitting issues. As C gets small, fewer samples would become the support vectors thus the model gets simpler.

![](http://localhost:4001/assets/2020-12-15-Text-Mining/images/textual_models/SVC_f1_gamma.png)
The trends for F1 scores under different gamma values look similar to what it is showed from VS C's. When gamma is small, the single sample has less impact on the hyper-plane therefore be less possible to be selected as support vectors. By the way, it is interesting to see the gamma differences particularly impacts more on the small C's model. This phenomenon tells that as C gets large, the influence of gamma becomes least.


---
##### 2.2. Comparison between LR and SVC
W2V &amp;amp; LR:
- size = 400
- iter = 30
- sg = SG
- penalty = 'l2'
- solver = 'sag'

F1: **0.82721686**

W2V &amp;amp; SVC:
- size = 400
- iter = 30
- sg = SG
- C = 1.0
- gamma = 1.0

F1: **0.83151219**

It is hard to say Using SVC is better than LR, since the difference of F1 scores is not significant, only 0.43% around. In the perspective of fitting time for their two, SVC spent 4 times of time than what is costed by LR. Each classifier employs multiclass classification strategies to deal with the experiment case.



### 3. Tuning Parameters for GloVe model and Logistic Regression model

Since the tasks are classification related hence the carefully selected classifiers play important roles. Based on the embedded words vectors from the GloVe model, where the VxN weights matrix contains all vocabulary vectors where it linearly spans in a fixed(smaller than vocabulary total length) number of dimensions space. Here it introduces two different classifiers for the tuning works.

##### 3.1. F1 Score comparison under different w2v/GloVe models and sizes, LR solvers and penalties

**w2v models**: Continuous-bag-of-words (CBOW) and Skip-grams (SG)
**w2v sizes**: The number of dimensions (N) of the N-dimensional space that gensim Word2Vec maps the words onto.
**LR solvers**: The algorithms to optimize the loss function.
**LR penalty**: To specify the norm used in the penalization (regularization).

**W2V Params options:**

The tuning process try fitting the model in SG and CBOW, under each version model, it fits the models infeature numbers in 12 options. 2 * 12 = 24 W2V rounds fitting.
```py {.line-numbers}
# W2V params options
    for sg in [0, 1]:
        for size in [10,20,30,40,50,100,150,200,250,300,400]:
            word2vec_model = Word2Vec(cutWords_list, sg = sg, size=size, iter=30, min_count=5,
                                        workers=multiprocessing.cpu_count())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The tuning process would try l1 norm and liblinear first, and switch to l2 norm with 4 other penalties:
liblinear, newton-cg, lbfgs and sag. (1 + 4) = 5 model training and testing for each W2V round.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;LR params options:

{'penalty': ['l1'], 'solver': ['liblinear']},
{'penalty': ['l2'], 'solver': ['liblinear','newton-cg', 'lbfgs', 'sag']}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The newton-cg, lbfgs and sag optimization algorithms require loss function being able to calculate first and second continuous derivative, hence L1-norm (manhattan distance) should not employ them. However, liblinear works fine in both L1-norm and L2-norm. The LR works fine for multiclass classification, but only in One-vs-Rest(OvR) for liblinear, which works worse than many-va-many(MvM) strategy in theories.
In total, the overall task tuning times are 24 * 5 = 120 times.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4001/assets/2020-12-15-Text-Mining/images/textual_models/LR_sizes_f1_new.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GloVe model size analysis:&lt;/strong&gt;
The size of the word vectors shows a general similar upwards trend comparing with SG and CBOW. However, the f1 scores soar at the very beginning when the word vectors size is below 100, the increasing rate is higher than SG and CBOW, but since the starting f1 score is lower than both, the GloVe model ends up being in the middle performance before CBOW eventually exceeds it. The overall performance shows that SG is still out-performed than CBOW and GloVe, whereas the differences between CBOW and GloVe are not significant.&lt;/p&gt;

&lt;h5 id=&quot;32-f1-score-comparison-under-different-w2vglove-iteration-time&quot;&gt;3.2. F1 Score comparison under different w2v/GloVe Iteration time&lt;/h5&gt;
&lt;p&gt;The iteration times decide how much the W2V/GloVe is well trained in rounds just by comprehend words’ meanings. The tuning process takes different values of iteration parameters to check how this affects the final performance.
```py {.line-numbers}&lt;/p&gt;
&lt;h1 id=&quot;w2v-params-options-2&quot;&gt;W2V params options&lt;/h1&gt;

&lt;p&gt;for sg in [0, 1]:
    for iters in [1,2,3,4,5,10,15,20,30,40,50]:
        word2vec_model = Word2Vec(cutWords_list, sg = sg, size=300,iter=iters, min_count=5, workers=multiprocessing.cpu_count())&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;**Attention**: The feature size is set to be 300, which comes from the previous test that 300 is considered to be a good candidate to produce high F1 in the end.

![](http://localhost:4001/assets/2020-12-15-Text-Mining/images/textual_models/LR_iters_f1_new.png)
**GloVe model Iteration time analysis:**
Same as previous experiments illustrated, SG has better performance than CBOW and GloVe in all cases. GloVe also plays in the middle role between SG and CBOW, but in this case (comparing iteration times), GloVe has almost the same score increasing rate. Interesting thing is, when the iteration is low, the GloVe performs close to SG, whereas when iteration time gets larger (greater or equal to 20), the GloVe is beaten by SG and close to the CBOW. In the end, CBOW and GloVe have emerged into similar performances.

##### 3.3. F1 Score comparison under different w2v/GloVe Window size
```py
    windows = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]
        for window in windows:
            ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4001/assets/2020-12-15-Text-Mining/images/textual_models/LR_windows_f1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GloVe model Window size analysis:&lt;/strong&gt;
The window size also decides all three models’ performances. As can be seen from the figure, the very model that has most influenced by the window size is GloVe, since GloVe is highly dependent on the co-occurrence matrix statistically extracted from the corpora. If the size of the window is small, the loss function would suffer the heavier than SG and CBOW. The best window size in our case is around 17, which means there would be 17 up ahead and down below (in total 34) neighboring words would be counted when producing the co-occurrence matrix.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;4-comparison-w2vglove-results-under-different-classifiers&quot;&gt;4. Comparison W2V/GloVe Results under different classifiers&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4001/assets/2020-12-15-Text-Mining/images/textual_models/F1_vs_models_clrs.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We employed five classifiers based on different classification methodologies and best-tuned parameters to conduct the experiments on the exact same features and target values. The results showed the MNB produced the least results and LR out-performed among all. The highest score that gets from the records is 0.82806571 from SVM-SG, and the lowest score came from the same classifiers which are 0.69019009 from SVM-CBOW. This shows the high contrast of performances using SVM model. The highest average f1 score came from LR, whereas LR-GloVe, LR-SG, and LR-CBOW are all exceeded 0.80. The other two classifiers performed close - the scores range from 0.70 to 0.75. When it comes to the comparison among the textual models, SG wined over all others which proved it to be the best-performed choice of textual model. CBOW should be the second place but in some cases (SVM or LR), GloVe is better. In terms of the classifiers’ ability of model tolerance, RF produces the most averaged results and LR is similar.&lt;/p&gt;

&lt;h3 id=&quot;appendix&quot;&gt;Appendix&lt;/h3&gt;

&lt;h5 id=&quot;a1-tuning-technique-scikit-learn-gridsearch&quot;&gt;A1. Tuning technique: Scikit-learn GridSearch&lt;/h5&gt;
&lt;p&gt;GaridSearchCV is a convenient tool for the usage of tuning the parameters under different models. The way to use it shows below:
```py {.line-numbers}
from sklearn.model_selection import GridSearchCV&lt;/p&gt;

&lt;p&gt;param_grid = [
{‘n_estimators’: [3, 10, 30], ‘max_features’: [2, 4, 6, 8]},
{‘bootstrap’: [False], ‘n_estimators’: [3, 10], ‘max_features’: [2, 3, 4]},
]&lt;/p&gt;

&lt;p&gt;forest_reg = RandomForestRegressor()
grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
                          scoring=’neg_mean_squared_error’)&lt;/p&gt;

&lt;p&gt;grid_search.fit(housing_prepared, housing_labels)&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The parameters grid contains a different set of parameter options. Every dictionary represents one possible set of parameters, it is different than the item pairs inside of it since each inside pairs can be using to perform one type of tuning process while the outer dictionary can be used to set for separate goals. (The parameters for different classifiers are designed differently sometimes)

For different inner classifers like (where classifiers are wrapped by an outer package):
```py
    param_grid = [
        {'estimator__kernel': ['rbf'], &quot;estimator__C&quot;:[10], &quot;estimator__gamma&quot;:[1.0]}
        ]
    clf = OneVsOneClassifier(SVC())
    grid_search = GridSearchCV(clf, param_grid, cv=10,
                          scoring=scoring, return_train_score = True, refit=False, iid = True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The parameter names have to be in form of inner members. 
The GridSearchCV allows performing K-fold Cross Validations.&lt;/p&gt;

&lt;h5 id=&quot;a2-w2v&quot;&gt;A2. W2V&lt;/h5&gt;

&lt;p&gt;W2V is a word embedding technique for natural language processing(NLP). It uses a shallow neural network with one hidden layer to learn a large corpus of text. After tokenized the documents, it transformed the one-hot encoding words into the embedded words vectors through the hidden layer weight matrix. The output vector for this neural network ends up being the probability distribution for all words vocabulary.&lt;/p&gt;

&lt;p&gt;This technique takes into account the words pair through the window sampling, whereas the neighboring words would be taken as input and output for the neural network hence to tuning the weights matrix through model training. There are two types of neural network structures: CBOW and Skip-gram. Both models assume the neighboring words are closely relevant. CBOW is motivated that the words are decided by its neighboring words. The Skip-gram is different in that each word determines the neighboring ones.&lt;/p&gt;

&lt;p&gt;In this report, given the corpus of Yelp reviews are commonly what customers make on the restaurant services, hence the words are natual committed articles in which the words are composed in specific orders in human habits that contain semantics through nearby words pairs. It would be good to implement the W2V model in this case. The package for implementing the model is by Genism word2vecoter.&lt;/p&gt;

&lt;h6 id=&quot;a21-genism-w2v-implementation&quot;&gt;A2.1 Genism W2V Implementation&lt;/h6&gt;

&lt;p&gt;To implement the model, the first step is to convert the corpus(a list of documents where each document is a complete sentence in a string type) into a list of lists, where each inner list contains the tokenized vocabularies extracted from each document:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[
    ['love', 'deagan'],
    ['walk', 'right', 'assum', 'crowd'],
    ...
    ['check', 'event', 'attend']
]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This list is called cutwords list. The Genism W2V model would take in this list and convert and train it into the word vectors.&lt;/p&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;word2vec_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Word2Vec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cutWords_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;workers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multiprocessing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h6 id=&quot;a22-prints-the-most-similar-words&quot;&gt;A2.2 Prints the most similar words&lt;/h6&gt;
&lt;p&gt;The W2V model can be used to list most similar words with given word. The ouputs are sorted by the Coscine Similarity:
```py {.line-numbers}
print(word2vec_model.wv.most_similar(‘good’))&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;[(‘decent’, 0.7268179655075073),
(‘moder’, 0.7154113054275513),
(‘ordinari’, 0.7079516053199768),
(‘darn’, 0.707633376121521),
(‘alright’, 0.7061334848403931),
(‘drawback’, 0.7018848657608032),
(‘great’, 0.6995223164558411),
(‘prob’, 0.6944646835327148), 
(‘prici’, 0.6898257732391357), 
(‘rival’, 0.6890593767166138)]&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;As reulsts showed, given input word 'good', the most output words are adjectives as 'good' is. ('decent', 'moderate', 'ordinary', 'alright', 'great', 'pricy')

###### A2.3 Prints the similarity between two words
The model can also be used to produced the similarity between two words.
```py {.line-numbers}
print(word2vec_model.similarity('waitress', 'waiter'))
print(word2vec_model.similarity('fork', 'sour'))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0.76473266
0.60534066
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The word ‘Waitress’ is much closely related to ‘waiter’ than the words pair between ‘fork’ and ‘sour’.&lt;/p&gt;

&lt;hr /&gt;
&lt;h5 id=&quot;a3-feature-engineering-x-and-ordinal-label-encoding-y&quot;&gt;A3 Feature Engineering (X) and Ordinal Label encoding (y)&lt;/h5&gt;
&lt;p&gt;Just by getting the trained W2V model does not satisfy the needs. The Train dataset has to be converted from the original strings of sentences into the word vectors.&lt;/p&gt;
&lt;h6 id=&quot;a31-represent-each-document-as-a-word-vector&quot;&gt;A3.1 Represent each document as a word vector&lt;/h6&gt;
&lt;p&gt;For every document, the word vector for each word can be obtained from the model weights matrix. The document can be represented by the mean of all word vectors.
```py {.line-numbers}
def get_contentVector(cutWords, word2vec_model):
    ‘’’
        Getting the mean word vector for each document
        @cutWords: document cut words list
    ‘’’
    vector_list = [word2vec_model.wv[k] for k in cutWords if k in word2vec_model]
    contentVector = np.array(vector_list).mean(axis=0)
    return contentVector&lt;/p&gt;

&lt;p&gt;cutWords_list = [x.split(“ “) for x in X_original]
X = [get_contentVector(cutWords, word2vec_model) for cutWords in cutWords_list]&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;###### A3.2 Multiclass label encoding
Since it has 3 classes(American(New), Sushi Bars and Fast food), it needs to convert the categorical target values into ordinal values. The way to implement it is by using Scikit-learn LabelEncoder.
```py {.line-numbers}
y = labelEncoder.fit_transform(df['category'])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html">Textual Features</summary></entry></feed>