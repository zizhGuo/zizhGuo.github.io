<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4001/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4001/" rel="alternate" type="text/html" /><updated>2021-06-10T23:25:01-04:00</updated><id>http://localhost:4001/feed.xml</id><title type="html">Your awesome title</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">Alibaba Taobao User Behaviors Analysis IV: AARRR Framework (Revenue) - Sale rank, RFM model annotation</title><link href="http://localhost:4001/jekyll/update/projects/2021/05/27/Taobao_Behavior_Analysis_Model_3.html" rel="alternate" type="text/html" title="Alibaba Taobao User Behaviors Analysis IV: AARRR Framework (Revenue) - Sale rank, RFM model annotation" /><published>2021-05-27T19:10:27-04:00</published><updated>2021-05-27T19:10:27-04:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/05/27/Taobao_Behavior_Analysis_Model_3</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/05/27/Taobao_Behavior_Analysis_Model_3.html">&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;1-recap-of-the-user-behavior-table&quot;&gt;1. Recap of the user behavior table&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+-------+-------+-----------+--------+----------+-------------------+----------+-----+---+----+---------+
|user_id|item_id|category_id|behavior|timestamps|           datetime|      date|month|day|hour|dayofweek|
+-------+-------+-----------+--------+----------+-------------------+----------+-----+---+----+---------+
|      1|2268318|    2520377|      pv|1511544070|2017-11-24 12:21:10|2017-11-24|   11| 24|  12|        6|
|      1|2333346|    2520771|      pv|1511561733|2017-11-24 17:15:33|2017-11-24|   11| 24|  17|        6|
|      1|2576651|     149192|      pv|1511572885|2017-11-24 20:21:25|2017-11-24|   11| 24|  20|        6|
|      1|3830808|    4181361|      pv|1511593493|2017-11-25 02:04:53|2017-11-25|   11| 25|   2|        7|
|      1|4365585|    2520377|      pv|1511596146|2017-11-25 02:49:06|2017-11-25|   11| 25|   2|        7|
|      1|4606018|    2735466|      pv|1511616481|2017-11-25 08:28:01|2017-11-25|   11| 25|   8|        7|
|      1| 230380|     411153|      pv|1511644942|2017-11-25 16:22:22|2017-11-25|   11| 25|  16|        7|
|      1|3827899|    2920476|      pv|1511713473|2017-11-26 11:24:33|2017-11-26|   11| 26|  11|        1|
|      1|3745169|    2891509|      pv|1511725471|2017-11-26 14:44:31|2017-11-26|   11| 26|  14|        1|
|      1|1531036|    2920476|      pv|1511733732|2017-11-26 17:02:12|2017-11-26|   11| 26|  17|        1|
+-------+-------+-----------+--------+----------+-------------------+----------+-----+---+----+---------+
only showing top 10 rows
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Behavior&lt;/th&gt;
      &lt;th&gt;Explanation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;pv&lt;/td&gt;
      &lt;td&gt;Page view of an item’s detail page, equivalent to an item click&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;fav&lt;/td&gt;
      &lt;td&gt;Purchase an item&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;cart&lt;/td&gt;
      &lt;td&gt;Add an item to shopping cart&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;buy&lt;/td&gt;
      &lt;td&gt;Favor an item&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;2-revenue&quot;&gt;2. Revenue&lt;/h4&gt;

&lt;p&gt;A transaction is made by users conducting a buy behavior, defined by this analysis. No matter the order is completely fulfilled or not. In fact, a metric called Gross Merchandise Volume (GMV) is used to evaluate the total gross income within a period of time. Unfortunately, the table does not contain the price feature for items, therefore we only calculate the total sale volume in dates and rank them group by the item category and items themselves.&lt;/p&gt;

&lt;h6 id=&quot;daily-sale-volume-in-10-days&quot;&gt;Daily Sale Volume in 10 days&lt;/h6&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df_daily_sales_volume&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
SELECT
    date,
    SUM(CASE WHEN behavior = 'pv' then 1 else 0 end) as pv,
    SUM(CASE WHEN behavior = 'fav' then 1 else 0 end) as fav,
    SUM(CASE WHEN behavior = 'cart' then 1 else 0 end) as cart,
    SUM(CASE WHEN behavior = 'buy' then 1 else 0 end) as buy
FROM taobao
GROUP BY
    date
&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toPandas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_3/Daily_sale_volume.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_3/Daily_sale_volume.png&quot; alt=&quot;drawing&quot; style=&quot;width: 60%;&quot; /&gt;
   &lt;/a&gt;
   &lt;!-- &lt;figcaption&gt;Fig 1: Daily Active behaviors histogram &lt;/figcaption&gt; --&gt;
&lt;/div&gt;

&lt;p&gt;As PART II mentioned, due to the parsing issue, the UNIX time collected from GMT+8 time zone is interpreted to GMT-5 time zone, so part of sales conducted on 11-25 are binned to 11-24. One day shift to right, the sale volume based on a date shows a consistent invariance even encountering the weekends.&lt;/p&gt;

&lt;h6 id=&quot;hourly-sale-volume-in-10-days&quot;&gt;Hourly Sale Volume in 10 days&lt;/h6&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df_hourly_sales_volume&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
SELECT
    hour,
    SUM(CASE WHEN behavior = 'pv' then 0.1 else 0 end) as pv,
    SUM(CASE WHEN behavior = 'fav' then 0.1 else 0 end) as fav,
    SUM(CASE WHEN behavior = 'cart' then 0.1 else 0 end) as cart,
    SUM(CASE WHEN behavior = 'buy' then 0.1 else 0 end) as buy
FROM taobao
GROUP BY
    hour
ORDER BY
    hour
&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toPandas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_3/Hourly_sale_volume.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_3/Hourly_sale_volume.png&quot; alt=&quot;drawing&quot; style=&quot;width: 60%;&quot; /&gt;
   &lt;/a&gt;
   &lt;!-- &lt;figcaption&gt;Fig 1: Daily Active behaviors histogram &lt;/figcaption&gt; --&gt;
&lt;/div&gt;

&lt;p&gt;The UNIX time functions from Pyspark make the hour become the US time based on the local machine, so the hour shows in the figure should convert into the Beijing time as the sale are conducted in China region. e.g. 6 pm to 19:00&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Comparing with hourly behaviors distribution, the difference is in the period of time between 3 am (16:00) to 6 pm (19:00), the sale volume has a little decrease among all hours of the day.&lt;/li&gt;
  &lt;li&gt;The peak has around 14000 sale volumes whereas the bottom has around 1000 sale volumes. The difference is around  14 times which is the same as the behavior distribution.&lt;/li&gt;
&lt;/ol&gt;

&lt;h6 id=&quot;sales-volume-ranking-on-item-category&quot;&gt;Sales Volume Ranking on item category&lt;/h6&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df_sales_volume_ranking_category&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
SELECT
    a.buy_times AS sales_volume,
    COUNT(a.category_id) AS category_num 
FROM
    (SELECT 
        category_id, 
        COUNT(user_id) AS buy_times
    FROM 
        taobao 
    WHERE 
        behavior='buy' 
    GROUP BY 
        category_id ) AS a 
GROUP BY
    a.buy_times 
ORDER BY
    category_num DESC;
&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toPandas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sales_volume 	1 	2 	3 	4 	5 	6 	7 	8 	9 	10 	... 	1158 	3096 	18016 	1147 	458 	1326 	2015 	6354 	2782 	2203
category_num 	767 	448 	313 	268 	198 	195 	163 	133 	105 	97 	... 	1 	1 	1 	1 	1 	1 	1 	1 	1 	1


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_3/ranking_category.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_3/ranking_category.png&quot; alt=&quot;drawing&quot; style=&quot;width: 60%;&quot; /&gt;
   &lt;/a&gt;
   &lt;!-- &lt;figcaption&gt;Fig 1: Daily Active behaviors histogram &lt;/figcaption&gt; --&gt;
&lt;/div&gt;

&lt;p&gt;Based on the sale volume, we ranked the item categories’ count. The figure above shows there are almost 800 categories of items are sold only once among all users. The second place’s category of an item which sold twice counts around 450. The overall trend follows a logarithmic pattern in a descending prone.&lt;/p&gt;

&lt;h6 id=&quot;sales-volume-ranking-on-item-id&quot;&gt;Sales Volume Ranking on item id&lt;/h6&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df_sales_volume_ranking_item&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
SELECT
    a.buy_times AS sales_volume,
    COUNT(a.item_id) AS item_num 
FROM
    (SELECT 
        item_id, 
        COUNT(user_id) AS buy_times
    FROM 
        taobao 
    WHERE 
        behavior='buy' 
    GROUP BY 
        item_id ) AS a 
GROUP BY
    a.buy_times 
ORDER BY
    item_num DESC;
&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toPandas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_3/ranking_item.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_3/ranking_item.png&quot; alt=&quot;drawing&quot; style=&quot;width: 60%;&quot; /&gt;
   &lt;/a&gt;
   &lt;!-- &lt;figcaption&gt;Fig 1: Daily Active behaviors histogram &lt;/figcaption&gt; --&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The second study on item id ranking based on the sale volume indicates a similar trend as to how it was performed with the item category rank. They both follow a logarithmic declining trend, but for the current item ranking trend, it is deeper. Over 350,000 items are sold once which takes a larger portion, whereas the items sold twice are only take 1/4 in the value.&lt;/p&gt;

&lt;h6 id=&quot;item-id-ranking-vs-item-category-ranking-on-sale-volume&quot;&gt;Item id ranking vs Item category ranking (on Sale Volume)&lt;/h6&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;row&quot; style=&quot;padding-left: 30rem;&quot;&gt;
  &lt;div class=&quot;column&quot; style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_3/category_pie.png&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
  &lt;/div&gt;
  &lt;div class=&quot;column&quot;&gt;
    &lt;img src=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_3/item_pie.png&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;It is interesting to conduct the pie charts for both rankings and compare how much the portions take for different granularity of data tag. Much easy to understand, the category tag has fewer unique values than item id since one category can include multiple items, hence the portion for ranking would be different.&lt;/p&gt;

&lt;p&gt;As seen from the figure on the left-hand side, half of the item categories have their belonging items sold 20+ times, as for those less popular item categories, one sold only once still takes 10 percent, these are the super unpopular item category. From the figure on the right-hand side, within the super unpopular item category, the items’ number overwhelmingly populates 58.2 percent among all items. Combined with the items which sold 2-10 times, it is interesting to see that most of the items (96 percent) of items are not popular at all, whereas only 2 percent of items are able to sell at least 20 times, in other words, getting into the transaction order.&lt;/p&gt;

&lt;p&gt;For further mining processing, a possible direction is to cluster the item category based on the distribution of its item sale volume. One guess is there might have an item category that has 1 or 2 items specifically popular with almost no visit for the rest, or some item categories may exist that all items belonging to them are regular.&lt;/p&gt;

&lt;h4 id=&quot;3-recency-frequency-monetary-value-rfm-model&quot;&gt;3. Recency, frequency, monetary value (RFM) Model&lt;/h4&gt;

&lt;p&gt;Recency, frequency, monetary value is a marketing analysis tool used to identify a company’s or an organization’s best customers by using certain measures. The RFM model is based on three quantitative factors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Recency: How recently a customer has made a purchase&lt;/li&gt;
  &lt;li&gt;Frequency: How often a customer makes a purchase&lt;/li&gt;
  &lt;li&gt;Monetary Value: How much money a customer spends on purchases&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;RFM analysis numerically ranks a customer in each of these three categories, generally on a scale of 1 to 5 (the higher the number, the better the result). The “best” customer would receive a top score in every category.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.investopedia.com/terms/r/rfm-recency-frequency-monetary-value.asp&quot;&gt;-[Source]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We take the above approach to category the users based on the rule with the last time buy behavior and frequency of buy behavior. Here are the rules:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;R：score the user's recency based on the time difference from the buy behavior date to 17-12-03
difference &amp;gt; 7 score = 1
difference BETWEEN 5-7 score = 2
difference BETWEEN 3-4 score = 3
difference BETWEEN 0-2 score = 4

F：score the user's frequency based on the date of the buy behavior count
purchase once score = 1
purchase twice score = 2
purchase 3-10 times score = 3
purchase times &amp;gt; 10 score = 4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since the table does not contain monetary info, hence ignore the monetary value.&lt;/p&gt;

&lt;p&gt;Once having the scores of users’ Recency and Frequency, applying another rule to classify users into different group.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Champion:
FrequencyScore BETWEEN 3-4 AND RecencyScore BETWEEN 3-4

Loyal:
FrequencyScore BETWEEN 3-4 AND RecencyScore BETWEEN 1-2

Potential Loyalists:
FrequencyScore BETWEEN 1-2 AND RecencyScore BETWEEN 3-4

Need Attentions
FrequencyScore BETWEEN 1-2 AND RecencyScore BETWEEN 1-2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
SELECT 
    user_id,
    (CASE WHEN Rdiff &amp;gt;7 THEN 1
    WHEN Rdiff BETWEEN 5 AND 7 THEN 2
    WHEN Rdiff BETWEEN 3 AND 4 THEN 3
    WHEN Rdiff BETWEEN 0 AND 2 THEN 4
    ELSE NULL END ) AS RecencyScore
FROM
    (SELECT 
        user_id,
        DATEDIFF('2017-12-03',max(date)) AS Rdiff
    FROM 
        taobao
    WHERE 
        behavior='buy'
    GROUP BY 
        user_id)

&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createOrReplaceTempView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;R1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
SELECT 
    user_id,
    (case WHEN SaleVolume BETWEEN 1 AND 1 THEN 1
    WHEN SaleVolume BETWEEN 2 AND 2 THEN 2
    WHEN SaleVolume BETWEEN 3 AND 10 THEN 3
    WHEN SaleVolume &amp;gt;=11 THEN 4
    ELSE NULL END ) as FrequencyScore
FROM(
    SELECT 
        user_id,
        COUNT(behavior) AS SaleVolume
    FROM 
        taobao
    WHERE 
        behavior='buy'
    GROUP BY 
        user_id)
&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createOrReplaceTempView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;F1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df_RFM&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
SELECT 
    user_id,
    RecencyScore,
    FrequencyScore,
    (CASE WHEN (FrequencyScore BETWEEN 1 AND 2)AND(RecencyScore BETWEEN 1 AND 2 )THEN 1
    WHEN (FrequencyScore BETWEEN 1 AND 2)AND(RecencyScore BETWEEN 3 AND 4 )THEN 2
    WHEN (FrequencyScore BETWEEN 3 AND 4)AND(RecencyScore BETWEEN 1 AND 2 )THEN 3
    WHEN (FrequencyScore BETWEEN 3 AND 4)AND(RecencyScore BETWEEN 3 AND 4 )THEN 4
    ELSE NULL END ) AS CustomerLevel
FROM 
    (SELECT 
        R1.user_id, 
        R1.RecencyScore,
        F1.FrequencyScore
    FROM 
        R1
    INNER JOIN 
        F1
    ON 
        R1.user_id=F1.user_id)
&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toPandas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_RFM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;        user_id  RecencyScore  FrequencyScore  CustomerLevel
0       1000240             4               3              4
1       1000280             4               2              2
2       1000665             4               3              4
3       1000795             4               2              2
4       1000839             4               3              4
...         ...           ...             ...            ...
672399   999498             2               1              1
672400   999507             4               3              4
672401   999510             4               3              4
672402   999616             2               1              1
672403   999656             3               1              2

[672404 rows x 4 columns]


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_3/RFM.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_3/RFM.png&quot; alt=&quot;drawing&quot; style=&quot;width: 30%;&quot; /&gt;
   &lt;/a&gt;
   &lt;!-- &lt;figcaption&gt;Fig 1: Daily Active behaviors histogram &lt;/figcaption&gt; --&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Copyright @ 2021 Zizhun Guo. All Rights Reserved.&lt;/p&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html"></summary></entry><entry><title type="html">Alibaba Taobao User Behaviors Analysis III: AARRR Framework (Activation/Retention) - DAU, Retention Rate, etc</title><link href="http://localhost:4001/jekyll/update/projects/2021/05/25/Taobao_Behavior_Analysis_Model_2.html" rel="alternate" type="text/html" title="Alibaba Taobao User Behaviors Analysis III: AARRR Framework (Activation/Retention) - DAU, Retention Rate, etc" /><published>2021-05-25T09:34:57-04:00</published><updated>2021-05-25T09:34:57-04:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/05/25/Taobao_Behavior_Analysis_Model_2</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/05/25/Taobao_Behavior_Analysis_Model_2.html">&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;1-recap-of-the-user-behavior-table&quot;&gt;1. Recap of the user behavior table&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+-------+-------+-----------+--------+----------+-------------------+----------+-----+---+----+---------+
|user_id|item_id|category_id|behavior|timestamps|           datetime|      date|month|day|hour|dayofweek|
+-------+-------+-----------+--------+----------+-------------------+----------+-----+---+----+---------+
|      1|2268318|    2520377|      pv|1511544070|2017-11-24 12:21:10|2017-11-24|   11| 24|  12|        6|
|      1|2333346|    2520771|      pv|1511561733|2017-11-24 17:15:33|2017-11-24|   11| 24|  17|        6|
|      1|2576651|     149192|      pv|1511572885|2017-11-24 20:21:25|2017-11-24|   11| 24|  20|        6|
|      1|3830808|    4181361|      pv|1511593493|2017-11-25 02:04:53|2017-11-25|   11| 25|   2|        7|
|      1|4365585|    2520377|      pv|1511596146|2017-11-25 02:49:06|2017-11-25|   11| 25|   2|        7|
|      1|4606018|    2735466|      pv|1511616481|2017-11-25 08:28:01|2017-11-25|   11| 25|   8|        7|
|      1| 230380|     411153|      pv|1511644942|2017-11-25 16:22:22|2017-11-25|   11| 25|  16|        7|
|      1|3827899|    2920476|      pv|1511713473|2017-11-26 11:24:33|2017-11-26|   11| 26|  11|        1|
|      1|3745169|    2891509|      pv|1511725471|2017-11-26 14:44:31|2017-11-26|   11| 26|  14|        1|
|      1|1531036|    2920476|      pv|1511733732|2017-11-26 17:02:12|2017-11-26|   11| 26|  17|        1|
+-------+-------+-----------+--------+----------+-------------------+----------+-----+---+----+---------+
only showing top 10 rows
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Behavior&lt;/th&gt;
      &lt;th&gt;Explanation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;pv&lt;/td&gt;
      &lt;td&gt;Page view of an item’s detail page, equivalent to an item click&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;fav&lt;/td&gt;
      &lt;td&gt;Purchase an item&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;cart&lt;/td&gt;
      &lt;td&gt;Add an item to shopping cart&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;buy&lt;/td&gt;
      &lt;td&gt;Favor an item&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;2-activation&quot;&gt;2. Activation&lt;/h4&gt;

&lt;p&gt;The Activation evaluates the Ecommerce’s ability to provide users with the “Aha moment”. It overlaps the concept with the acquisition a little, but the difference is that the activation focuses on the micro-conversion part whereas users are having enjoyable and solid experiences in the individual part of the product process.&lt;/p&gt;

&lt;h6 id=&quot;daily-active-behaviors-distribution&quot;&gt;Daily Active behaviors distribution&lt;/h6&gt;

&lt;p&gt;Details aside, first look at the distribution for the number of daily behaviors between 2017-11-24 to 2017-12-03.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df_date_behavior_count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
SELECT 
    date,
    SUM(CASE WHEN behavior = 'pv' THEN 1 ELSE 0 END) AS pv,
    SUM(CASE WHEN behavior = 'fav' THEN 1 ELSE 0 END) AS fav,
    SUM(CASE WHEN behavior = 'cart' THEN 1 ELSE 0 END) AS cart,
    SUM(CASE WHEN behavior = 'buy' THEN 1 ELSE 0 END) AS buy
FROM 
    taobao
GROUP BY 
    date
ORDER BY date
&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toPandas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_date_behavior_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_2/DAB.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_2/DAB.png&quot; alt=&quot;drawing&quot; style=&quot;width: 60%;&quot; /&gt;
   &lt;/a&gt;
   &lt;figcaption&gt;Fig 1: Daily Active behaviors histogram &lt;/figcaption&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;The number of page view behaviors overwhelmed the other three behaviors favorite, cart, and buy.&lt;/li&gt;
  &lt;li&gt;The day of the week for 2017-11-24 is Friday in Beijing Time (GMT+8), whereas it has 13 hours jet leg from US Eastern Time (GMT-5) in winter. It is weird to find that the behavior count on 11-24 is much smaller than 12-01. An assumption to this phenomenon is when binning the behaviors, the part of behaviors conducted in 2017-11-25 morning in china was grouped into the 2017-11-24 in American Time zone, &lt;strong&gt;hence the current bars should be moved 1 day after and the value for each date should be partially tunned one by one&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;The current histogram cannot quantitively confirm the relation of behavior count between days, but the trend can be guessed out. After modification, the number of behaviors on Saturday and Sunday is higher than on weekdays.&lt;/li&gt;
&lt;/ol&gt;

&lt;h6 id=&quot;daily-active-usersdau-distribution&quot;&gt;Daily Active Users(DAU) distribution&lt;/h6&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df_DAU&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
SELECT 
    date,
    COUNT(DISTINCT user_id) AS DAU
FROM 
    taobao
GROUP BY 
    date
ORDER BY 
    date
&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toPandas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_DAU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_2/DAU.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_2/DAU.png&quot; alt=&quot;drawing&quot; style=&quot;width: 60%;&quot; /&gt;
   &lt;/a&gt;
   &lt;figcaption&gt;Fig 2: Daily Active Users histogram &lt;/figcaption&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;As to count the unique users in these 10 days, the criteria is any user who conducted one of four behavior count as one active user. Therefore, the relation between DAU to daily active behaviors is similar to the relationship between global unique user numbers and global behavior numbers.&lt;/li&gt;
  &lt;li&gt;The trend is similar to DAU histogram, as the time leg and Unix Time function rule still work poorly on a dataset collected from another time zone. The part of unique users is supposed to be grouped on the day after.&lt;/li&gt;
&lt;/ol&gt;

&lt;h6 id=&quot;hourly-active-behaviors-distribution&quot;&gt;Hourly Active Behaviors distribution&lt;/h6&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df_hour_behavior_count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
SELECT 
    hour,
    SUM(CASE WHEN behavior = 'pv' THEN 0.1 ELSE 0 END) AS pv,
    SUM(CASE WHEN behavior = 'fav' THEN 0.1 ELSE 0 END) AS fav,
    SUM(CASE WHEN behavior = 'cart' THEN 0.1 ELSE 0 END) AS cart,
    SUM(CASE WHEN behavior = 'buy' THEN 0.1 ELSE 0 END) AS buy
FROM 
    taobao
WHERE date &amp;lt; '2017-12-04' AND date &amp;gt; '2017-11-23'
GROUP BY 
    hour
ORDER BY 
    hour
&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toPandas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_2/HAB.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_2/HAB.png&quot; alt=&quot;drawing&quot; style=&quot;width: 60%;&quot; /&gt;
   &lt;/a&gt;
   &lt;figcaption&gt;Fig 3: Hourly Active Behaviors histogram &lt;/figcaption&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;The distribution is binned by the hour attribute from the table, as it is calculated by averaging the behavior count across 10 days, it compensates for the difference between days.&lt;/li&gt;
  &lt;li&gt;The hour illustrates the parsed UNIX time in the American time zone, so there is 13 hours time lag for the real hour within a day scenario. e.g. The 7:00 in US eastern time indicates the 20:00 in the Beijing time zone.&lt;/li&gt;
  &lt;li&gt;Based on 2, the peak found between 6:00 to 10:00, when the most popular product using time, is 7 pm to 11 pm in China. It makes sense since this is the time when people get off work and spend time online shopping.&lt;/li&gt;
  &lt;li&gt;The behavior count in peak say 9 pm (8:00) is 800k round own, whereas at 4 am (15:00) in the morning, the count is almost only 50k. There are 16 times between the peak and bottom. In day times, the average behavior count is around 500k.&lt;/li&gt;
  &lt;li&gt;The rate of decline from peak to bottom is great. It is a common bedtime and people go to sleep quickly. However, once wake up, the usage recovers a bit slower hence users have different things to do.&lt;/li&gt;
&lt;/ol&gt;

&lt;h6 id=&quot;hourly-active-users-distribution&quot;&gt;Hourly Active Users distribution&lt;/h6&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df_AverageHAU&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
SELECT 
    hour,
    ROUND(COUNT(DISTINCT user_id)/10, 0) AS Average_HAU
FROM 
    taobao
WHERE date &amp;lt; '2017-12-04' AND date &amp;gt; '2017-11-23'
GROUP BY 
    hour
ORDER BY hour
&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toPandas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_2/HAU.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_2/HAU.png&quot; alt=&quot;drawing&quot; style=&quot;width: 60%;&quot; /&gt;
   &lt;/a&gt;
   &lt;figcaption&gt;Fig 4: Hourly Active Users histogram &lt;/figcaption&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;The trend is similar to hourly behavior count. However, the peak is not as significant as the last one. This indicates that the contribution for unique users on behaviors is not balanced. Given that the trend is similar (same shape), therefore the aspect for causing the balancing issue is that users who are active in the daytime conduct more behaviors at night. It intuitively may make sense that people work in the daytime and get hard to shop online, but at night, they have more time and convenience to use the APP.&lt;/li&gt;
  &lt;li&gt;The max value of peak is around 70k whereas the value for the bottom is around 5k, the 12 times difference is greater than 13 times for the behavior count. This indicates the at least for two periods of time (7 pm to 10 pm and 12 am to 5 am), users’ behaviors are normally equalized which helps understand combined with the first point that the users in the daytime are less efficient (number of behaviors per user) than at night.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;2-retention&quot;&gt;2. Retention&lt;/h4&gt;

&lt;h6 id=&quot;retention-rate&quot;&gt;Retention Rate&lt;/h6&gt;

&lt;p&gt;Retention rate formula:
The # of active users continuing to subscribe divided by the total active users at the start of a period = retention rate.
[-[Source])(https://www.profitwell.com/customer-retention/calculate-retention-rate)]&lt;/p&gt;

&lt;p&gt;The concept to have retention rate metric in a marketing atmosphere is to monitor firm performance in attracting and retaining customers. [-&lt;a href=&quot;https://en.wikipedia.org/wiki/Retention_rate&quot;&gt;Wikipedia&lt;/a&gt;] It is similar to churn rate.&lt;/p&gt;

&lt;p&gt;This part of Taobao user behavior analysis technically only provides practice on calculating retention rate metric, since there are no attributes identifying the new users, therefore the users who are count as the first-day user may of the old user, which should not be considered.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df_retention&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    SELECT
        SUM(CASE WHEN day1 &amp;gt; 0 then 1 else 0 end) AS day1,
        SUM(CASE WHEN day1 &amp;gt; 0 AND day2 &amp;gt; 0 then 1 else 0 end) AS day2retention,
        SUM(CASE WHEN day1 &amp;gt; 0 AND day3 &amp;gt; 0 then 1 else 0 end) AS day3retention,
        SUM(CASE WHEN day1 &amp;gt; 0 AND day4 &amp;gt; 0 then 1 else 0 end) AS day4retention,
        SUM(CASE WHEN day1 &amp;gt; 0 AND day5 &amp;gt; 0 then 1 else 0 end) AS day5retention,
        SUM(CASE WHEN day1 &amp;gt; 0 AND day6 &amp;gt; 0 then 1 else 0 end) AS day6retention,
        SUM(CASE WHEN day1 &amp;gt; 0 AND day7 &amp;gt; 0 then 1 else 0 end) AS day7retention,
        SUM(CASE WHEN day1 &amp;gt; 0 AND day8 &amp;gt; 0 then 1 else 0 end) AS day8retention,
        SUM(CASE WHEN day1 &amp;gt; 0 AND day9 &amp;gt; 0 then 1 else 0 end) AS day9retention,
        SUM(CASE WHEN day1 &amp;gt; 0 AND day10 &amp;gt; 0 then 1 else 0 end) AS day10retention
    FROM
        (SELECT
            user_id,
            SUM(CASE WHEN date = '2017-11-24' then 1 else 0 end) as day1,
            SUM(CASE WHEN date = '2017-11-25' then 1 else 0 end) as day2,
            SUM(CASE WHEN date = '2017-11-26' then 1 else 0 end) as day3,
            SUM(CASE WHEN date = '2017-11-27' then 1 else 0 end) as day4,
            SUM(CASE WHEN date = '2017-11-28' then 1 else 0 end) as day5,
            SUM(CASE WHEN date = '2017-11-29' then 1 else 0 end) as day6,
            SUM(CASE WHEN date = '2017-11-30' then 1 else 0 end) as day7,
            SUM(CASE WHEN date = '2017-12-01' then 1 else 0 end) as day8,
            SUM(CASE WHEN date = '2017-12-02' then 1 else 0 end) as day9,
            SUM(CASE WHEN date = '2017-12-03' then 1 else 0 end) as day10
        FROM taobao
        GROUP BY
            user_id)
    &quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toPandas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_2/retention.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model_2/retention.png&quot; alt=&quot;drawing&quot; style=&quot;width: 60%;&quot; /&gt;
   &lt;/a&gt;
   &lt;figcaption&gt;Fig 5: simulating retention rate &lt;/figcaption&gt;
&lt;/div&gt;

&lt;h6 id=&quot;repurchase-rate&quot;&gt;Repurchase Rate&lt;/h6&gt;

&lt;p&gt;Repurchase rate is the percentage rate of a cohort having placed another order within a certain period of time, typically calculated within 30/60/90/180/360 days from the first order. [-&lt;a href=&quot;https://medium.com/@matsutton/repurchase-rate-the-most-overlooked-ecommerce-kpi-337bccde184b&quot;&gt;Source&lt;/a&gt;]&lt;/p&gt;

&lt;p&gt;Due to the limit of time periods, we calculate the 10-day repurchase rate. The way to calculate it is to find the number of unique users who have purchased twice within 10 days.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;n_repurchase&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
SELECT COUNT(DISTINCT user_id)
FROM
    (SELECT user_id, COUNT(behavior) AS buy_times
    FROM taobao
    WHERE behavior = 'buy'
    GROUP BY 
        user_id)
WHERE buy_times &amp;gt; 1
&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;n_purchase&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
SELECT COUNT(DISTINCT user_id)
FROM
    (SELECT user_id, COUNT(behavior) AS buy_times
    FROM taobao
    behavior = 'buy'
    GROUP BY 
        user_id)
&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_repurchase&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_purchase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The repurchase rate is &lt;strong&gt;66%&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;There is another way to calculate which is by finding the count of unique users number who has conducted another transaction within the 10 days except for the first day.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Copyright @ 2021 Zizhun Guo. All Rights Reserved.&lt;/p&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html"></summary></entry><entry><title type="html">Alibaba Taobao User Behaviors Analysis I: Dataset(1 billion records) and Preprocessing</title><link href="http://localhost:4001/jekyll/update/projects/2021/05/21/Taobao_Behavior_Analysis_Intro.html" rel="alternate" type="text/html" title="Alibaba Taobao User Behaviors Analysis I: Dataset(1 billion records) and Preprocessing" /><published>2021-05-21T07:34:57-04:00</published><updated>2021-05-21T07:34:57-04:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/05/21/Taobao_Behavior_Analysis_Intro</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/05/21/Taobao_Behavior_Analysis_Intro.html">&lt;p&gt;&lt;strong&gt;Alibaba Taobao.com&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Taobao (Chinese: 淘宝网) is a Chinese online shopping platform. It is headquartered in Hangzhou and owned by Alibaba. It is ranked as the eighth most-visited website. Taobao.com was registered on April 21, 2003 by Alibaba Cloud Computing (Beijing) Co., Ltd.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Taobao Marketplace facilitates consumer-to-consumer (C2C) retail by providing a platform for small businesses and individual entrepreneurs to open online stores that mainly cater to consumers in Chinese-speaking regions (Mainland China, Hong Kong, Macau and Taiwan) and abroad,[4] which is made payable by online accounts. Its stores usually offer an express delivery service.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Taobao&quot;&gt;https://en.wikipedia.org/wiki/Taobao&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Intro/AlibabaLogo.jpg&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Intro/AlibabaLogo.jpg&quot; alt=&quot;drawing&quot; style=&quot;width: 40%;&quot; /&gt;
   &lt;/a&gt;
   &lt;br /&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Intro/Taobao_Logo.svg&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Intro/Taobao_Logo.svg&quot; alt=&quot;drawing&quot; style=&quot;width: 40%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Alibaba Group LOGO &lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;dataset-user-behavior-data-from-taobao-for-recommendation&quot;&gt;Dataset: User Behavior Data from Taobao for Recommendation&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The dataset is collected from &lt;a href=&quot;https://tianchi.aliyun.com/dataset/dataDetail?dataId=649&amp;amp;userId=1&quot;&gt;&lt;strong&gt;Tianchi&lt;/strong&gt;&lt;/a&gt; - Data Science Workshop from Aliyun(阿里云)- literally means &lt;a href=&quot;https://us.alibabacloud.com/&quot;&gt;&lt;strong&gt;Alibaba Cloud&lt;/strong&gt;&lt;/a&gt;, the cloud computing service ranked &lt;strong&gt;third-largest&lt;/strong&gt; infrastucture as a service provider, right behind Amazon Web Services, Microsoft Azure.&lt;/p&gt;

&lt;p&gt;User Behavior is a dataset of user behaviors from Taobao, for recommendation problem with implicit feedback. The dataset is offered by Alibaba.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;File&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Feature&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;UserBehavior.csv&lt;/td&gt;
      &lt;td&gt;All user behavior data&lt;/td&gt;
      &lt;td&gt;User ID, item ID, category ID, behavior type, timestamp&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;UserBehavior.csv&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We random select about 1 million users who have behaviors including click, purchase, adding item to shopping cart and item favoring during November 25 to December 03, 2017. The dataset is organized in a very similar form to MovieLens-20M, i.e., each line represents a specific user-item interaction, which consists of user ID, item ID, item’s category ID, behavior type and timestamp, separated by commas. The detailed descriptions of each field are as follows:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Field&lt;/th&gt;
      &lt;th&gt;Explanation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;User ID&lt;/td&gt;
      &lt;td&gt;An integer, the serialized ID that represents a user&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Item ID&lt;/td&gt;
      &lt;td&gt;An integer, the serialized ID that represents an item&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Category ID&lt;/td&gt;
      &lt;td&gt;An integer, the serialized ID that represents the category which the corresponding item belongs to&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Behavior type&lt;/td&gt;
      &lt;td&gt;A string, enum-type from (‘pv’, ‘buy’, ‘cart’, ‘fav’)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Timestamp&lt;/td&gt;
      &lt;td&gt;An integer, the timestamp of the behavior&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Note that the dataset contains 4 different types of behaviors, they are&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Behavior&lt;/th&gt;
      &lt;th&gt;Explanation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;pv&lt;/td&gt;
      &lt;td&gt;Page view of an item’s detail page, equivalent to an item click&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;fav&lt;/td&gt;
      &lt;td&gt;Purchase an item&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;cart&lt;/td&gt;
      &lt;td&gt;Add an item to shopping cart&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;buy&lt;/td&gt;
      &lt;td&gt;Favor an item&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Dimensions of the dataset are&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Dimension&lt;/th&gt;
      &lt;th&gt;Number&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;# of users&lt;/td&gt;
      &lt;td&gt;987,994&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;# of items&lt;/td&gt;
      &lt;td&gt;4,162,024&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;# of categories&lt;/td&gt;
      &lt;td&gt;9,439&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;# of interactions&lt;/td&gt;
      &lt;td&gt;100,150,807&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;1-dateset-preprocessing&quot;&gt;1. Dateset preprocessing&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Load the CSV dataset as Spark DateFrame using Pyspark&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;findspark&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;findspark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/home/zizhun/spark-3.1.1-bin-hadoop2.7'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyspark.sql&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SparkSession&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Load csv file into spark dataframe&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'UserBehavior.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Change field names&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumnRenamed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;_c0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;user_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumnRenamed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;_c1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;item_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumnRenamed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;_c2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;category_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumnRenamed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;_c3&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;behavior&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumnRenamed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;_c4&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;timestamps&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Check out the schema and partial view&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root
 |-- user_id: string (nullable = true)
 |-- item_id: string (nullable = true)
 |-- category_id: string (nullable = true)
 |-- behavior: string (nullable = true)
 |-- timestamps: string (nullable = true)

+-------+-------+-----------+--------+----------+
|user_id|item_id|category_id|behavior|timestamps|
+-------+-------+-----------+--------+----------+
|      1|2268318|    2520377|      pv|1511544070|
|      1|2333346|    2520771|      pv|1511561733|
|      1|2576651|     149192|      pv|1511572885|
|      1|3830808|    4181361|      pv|1511593493|
|      1|4365585|    2520377|      pv|1511596146|
|      1|4606018|    2735466|      pv|1511616481|
|      1| 230380|     411153|      pv|1511644942|
|      1|3827899|    2920476|      pv|1511713473|
|      1|3745169|    2891509|      pv|1511725471|
|      1|1531036|    2920476|      pv|1511733732|
+-------+-------+-----------+--------+----------+
only showing top 10 rows

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Transform timestamps from Unixtime to date&lt;/strong&gt;
The original timestamp is in format of Unixtime, therefore transforming it into 6 new readable field as datetime, date, month, day, hour and dayofweek.&lt;/p&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyspark.sql.functions&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dayofmonth&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hour&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                  &lt;span class=&quot;n&quot;&gt;dayofyear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;month&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dayofmonth&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                  &lt;span class=&quot;n&quot;&gt;year&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weekofyear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                  &lt;span class=&quot;n&quot;&gt;format_number&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;date_format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to_date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dayofweek&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dayofmonth&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to_date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; \
            &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'month'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;month&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; \
            &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'day'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dayofmonth&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; \
            &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'hour'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hour&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; \
            &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;withColumn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dayofweek'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dayofweek&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Results:
+-------+-------+-----------+--------+----------+-------------------+----------+-----+---+----+---------+
|user_id|item_id|category_id|behavior|timestamps|           datetime|      date|month|day|hour|dayofweek|
+-------+-------+-----------+--------+----------+-------------------+----------+-----+---+----+---------+
|      1|2268318|    2520377|      pv|1511544070|2017-11-24 12:21:10|2017-11-24|   11| 24|  12|        6|
|      1|2333346|    2520771|      pv|1511561733|2017-11-24 17:15:33|2017-11-24|   11| 24|  17|        6|
|      1|2576651|     149192|      pv|1511572885|2017-11-24 20:21:25|2017-11-24|   11| 24|  20|        6|
|      1|3830808|    4181361|      pv|1511593493|2017-11-25 02:04:53|2017-11-25|   11| 25|   2|        7|
|      1|4365585|    2520377|      pv|1511596146|2017-11-25 02:49:06|2017-11-25|   11| 25|   2|        7|
|      1|4606018|    2735466|      pv|1511616481|2017-11-25 08:28:01|2017-11-25|   11| 25|   8|        7|
|      1| 230380|     411153|      pv|1511644942|2017-11-25 16:22:22|2017-11-25|   11| 25|  16|        7|
|      1|3827899|    2920476|      pv|1511713473|2017-11-26 11:24:33|2017-11-26|   11| 26|  11|        1|
|      1|3745169|    2891509|      pv|1511725471|2017-11-26 14:44:31|2017-11-26|   11| 26|  14|        1|
|      1|1531036|    2920476|      pv|1511733732|2017-11-26 17:02:12|2017-11-26|   11| 26|  17|        1|
+-------+-------+-----------+--------+----------+-------------------+----------+-----+---+----+---------+
only showing top 10 rows
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Discover dataset on the range of date&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;SELECT Date, n_interactions
FROM
    (SELECT date as Date, COUNT(user_id) as n_interactions
    FROM taobao
    GROUP BY date
    ORDER BY date)
WHERE n_interactions &amp;gt; 10000
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;         Date  n_interactions
0  2017-11-24         3453235
1  2017-11-25        10598765
2  2017-11-26        10496631
3  2017-11-27         9985084
4  2017-11-28         9987905
5  2017-11-29        10350799
6  2017-11-30        10542266
7  2017-12-01        11712571
8  2017-12-02        14057989
9  2017-12-03         8946657
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The distribution shows most of the interactions are conducted between &lt;em&gt;2017-11-24&lt;/em&gt; to 2017-12-03 (10 days).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Create TempView as Taobao from records based on the distribution&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createOrReplaceTempView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;taobao&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;
&lt;p&gt;Copyright @ 2021 Zizhun Guo. All Rights Reserved.&lt;/p&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html">Alibaba Taobao.com</summary></entry><entry><title type="html">Alibaba Taobao User Behaviors Analysis II: AARRR Framework (Acquisition) - PV, UV, CVR, Funnel Plot</title><link href="http://localhost:4001/jekyll/update/projects/2021/05/21/Taobao_Behavior_Analysis_Model.html" rel="alternate" type="text/html" title="Alibaba Taobao User Behaviors Analysis II: AARRR Framework (Acquisition) - PV, UV, CVR, Funnel Plot" /><published>2021-05-21T07:34:57-04:00</published><updated>2021-05-21T07:34:57-04:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/05/21/Taobao_Behavior_Analysis_Model</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/05/21/Taobao_Behavior_Analysis_Model.html">&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;1-recap-of-the-user-behavior-table&quot;&gt;1. Recap of the user behavior table&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+-------+-------+-----------+--------+----------+-------------------+----------+-----+---+----+---------+
|user_id|item_id|category_id|behavior|timestamps|           datetime|      date|month|day|hour|dayofweek|
+-------+-------+-----------+--------+----------+-------------------+----------+-----+---+----+---------+
|      1|2268318|    2520377|      pv|1511544070|2017-11-24 12:21:10|2017-11-24|   11| 24|  12|        6|
|      1|2333346|    2520771|      pv|1511561733|2017-11-24 17:15:33|2017-11-24|   11| 24|  17|        6|
|      1|2576651|     149192|      pv|1511572885|2017-11-24 20:21:25|2017-11-24|   11| 24|  20|        6|
|      1|3830808|    4181361|      pv|1511593493|2017-11-25 02:04:53|2017-11-25|   11| 25|   2|        7|
|      1|4365585|    2520377|      pv|1511596146|2017-11-25 02:49:06|2017-11-25|   11| 25|   2|        7|
|      1|4606018|    2735466|      pv|1511616481|2017-11-25 08:28:01|2017-11-25|   11| 25|   8|        7|
|      1| 230380|     411153|      pv|1511644942|2017-11-25 16:22:22|2017-11-25|   11| 25|  16|        7|
|      1|3827899|    2920476|      pv|1511713473|2017-11-26 11:24:33|2017-11-26|   11| 26|  11|        1|
|      1|3745169|    2891509|      pv|1511725471|2017-11-26 14:44:31|2017-11-26|   11| 26|  14|        1|
|      1|1531036|    2920476|      pv|1511733732|2017-11-26 17:02:12|2017-11-26|   11| 26|  17|        1|
+-------+-------+-----------+--------+----------+-------------------+----------+-----+---+----+---------+
only showing top 10 rows
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Behavior&lt;/th&gt;
      &lt;th&gt;Explanation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;pv&lt;/td&gt;
      &lt;td&gt;Page view of an item’s detail page, equivalent to an item click&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;fav&lt;/td&gt;
      &lt;td&gt;Purchase an item&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;cart&lt;/td&gt;
      &lt;td&gt;Add an item to shopping cart&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;buy&lt;/td&gt;
      &lt;td&gt;Favor an item&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;2-acquisition&quot;&gt;2. Acquisition&lt;/h4&gt;

&lt;p&gt;The point for Acquisition analysis is to develop knowledge about the ability of the product to convert visitors into customers. It helps evaluate the efficiency of the business process. The product may have diverse marketing sources of visitors and different channels to fulfill the conversion.&lt;/p&gt;

&lt;p&gt;In Taobao user behavior dataset, the ‘behavior’ field can be intuitively interpreted owning the values in an ordinal nature, since the business allows provide purchasing behaviors which are able to independently conducted, e.g. users can choose to purchase the item directly or put it into the cart or favorites. Therefore, there are multiple channels that convert the item visit into the final order. Hence, I take multiple funnel analyses to study its acquisitional traits.&lt;/p&gt;

&lt;h6 id=&quot;global-page-view-pv-unique-user-uv-pvuv&quot;&gt;Global Page View (PV), Unique User (UV), PV/UV&lt;/h6&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Pychart can query records using Dataframe SQL functions&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyspark.sql.functions&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;countDistinct&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;countDistinct&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'uv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+------+
|    uv|
+------+
|987991|
+------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The dataset has &lt;strong&gt;987,991&lt;/strong&gt; unique users. Hence, &lt;strong&gt;UV&lt;/strong&gt; = &lt;strong&gt;987,991&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# The same, group by on 'behavior' and find the count for the 'pv'&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'behavior'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;orderBy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'count'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ascending&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+--------+--------+
|behavior|   count|
+--------+--------+
|      pv|89697359|
|    cart| 5530446|
|     fav| 2888258|
|     buy| 2015839|
+--------+--------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The dataset has &lt;strong&gt;89,697,359&lt;/strong&gt; page view behaviors between 2017-11-24 to 2017-12-03. Hence, &lt;strong&gt;PV&lt;/strong&gt; = &lt;strong&gt;89,697,359&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The PV/UV, the average page view per user evaluates the popularity for the items to be seen in a global sense. We could calculate it for each item. However, this metric needs to be used with other global metrics.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;PV/UV&lt;/strong&gt; is &lt;strong&gt;90&lt;/strong&gt;. In these 10 days, the average page views for each unique user is 90.&lt;/p&gt;

&lt;h6 id=&quot;bounce-rate&quot;&gt;Bounce Rate&lt;/h6&gt;

&lt;p&gt;Bounce rate is single-page sessions divided by all sessions, or the percentage of all sessions on the site in which users viewed only a single page and triggered only a single request to the Analytics server. - &lt;a href=&quot;https://support.google.com/analytics/answer/1009409?hl=en&quot;&gt;reference&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In the Taobao user behavior case, the unique users who visit items once during the 10-day session would be only considered. Therefore, this bounce rate evaluates the attractiveness of the website instead of a specific item.&lt;/p&gt;

&lt;p&gt;Create&lt;/p&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Create TempView with the user count for the different behavior.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# PK: user_id&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
SELECT 
    user_id, 
    SUM(case when behavior='pv' then 1 else 0 end) as PageView,
    SUM(case when behavior='fav' then 1 else 0 end) as Favorite,
    SUM(case when behavior='cart' then 1 else 0 end) as Cart,
    SUM(case when behavior='buy' then 1 else 0 end) as Buy
FROM 
    taobao
GROUP BY
    user_id
&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createTempView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;behaviorCount&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
SELECT 
    COUNT(user_id)
FROM
    behaviorCount
WHERE PageView = 1 AND Favorite = 0 AND Cart = 0 AND Buy = 0;
&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+-----------------------+
|count(DISTINCT user_id)|
+-----------------------+
|                     53|
+-----------------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The number of unique users who have only &lt;strong&gt;1 page view&lt;/strong&gt; count is &lt;strong&gt;53&lt;/strong&gt;. The &lt;strong&gt;bounce rate&lt;/strong&gt;, 53/UV is &lt;strong&gt;0.0053%&lt;/strong&gt;. It is very small, it proves the visitors, no matter new or old, would not stop discovering the website at the first sight.&lt;/p&gt;

&lt;h6 id=&quot;conversion-rate-cvr&quot;&gt;Conversion Rate (CVR)&lt;/h6&gt;

&lt;p&gt;The &lt;strong&gt;conversion rate&lt;/strong&gt; is the percentage of visitors to the website that complete a desired goal (a conversion) out of the total number of visitors.&lt;a href=&quot;https://www.wordstream.com/conversion-rate&quot;&gt;-[Source]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The desired goal is &lt;strong&gt;make-purchase&lt;/strong&gt;.  There are three channels to make such conversion (see fig 1 below): 1. page view - favorite - buy; 2. page view - cart - buy; 3. page view - buy. Each channel can conduct a funnel analysis.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model/01.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model/01.png&quot; alt=&quot;drawing&quot; style=&quot;width: 20%;&quot; /&gt;
   &lt;/a&gt;
   &lt;figcaption&gt;Fig 1: Three conversion channels &lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;pv - fav - buy&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;CVR for page view user to favorite user = # of users who have pv and fav/ # of users who have pv&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;n_unique_fav_users&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
SELECT COUNT(DISTINCT user_id)
FROM behaviorCount
WHERE PageView &amp;gt; 0 AND Favorite &amp;gt; 0
&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 387548&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;n_unique_pv_users&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
SELECT COUNT(DISTINCT user_id)
FROM behaviorCount
WHERE PageView &amp;gt; 0
&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 984107&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;CVR_pv2fav&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_unique_fav_users&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_unique_pv_users&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 387548/984107&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The conversion rate for pv to fav is 39.38%.&lt;/p&gt;

&lt;p&gt;CVR for page view user to favorite to buy user = # of users who have pv, fav and buy / # of users who have pv&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;n_unique_fav_buy_users&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
SELECT COUNT(DISTINCT user_id)
FROM behaviorCount
WHERE PageView &amp;gt; 0 AND Favorite &amp;gt; 0 AND Buy &amp;gt; 0
&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 275476&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;CVR_pv2fav2buy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_unique_fav_buy_users&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_unique_pv_users&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 275476 / 984107&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CVR_pv2fav2buy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The conversion rate for pv-fav-buy is 27.99%.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;pv - cart - buy&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;CVR for page view user to cart user = # of users who have pv and cart / # of users who have pv&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;n_unique_cart_users&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
SELECT COUNT(DISTINCT user_id)
FROM behaviorCount
WHERE PageView &amp;gt; 0 AND Cart &amp;gt; 0
&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 735674&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;CVR_pv2cart&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_unique_cart_users&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_unique_pv_users&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 735674 / 984107&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CVR_pv2cart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The conversion rate for pv-cart is 74.56%.&lt;/p&gt;

&lt;p&gt;CVR for page view user to cart to buy user = # of users who have pv, cart and buy / # of users who have pv&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;n_unique_cart_buy_users&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
SELECT COUNT(DISTINCT user_id)
FROM behaviorCount
WHERE PageView &amp;gt; 0 AND Cart &amp;gt; 0 AND Buy &amp;gt; 0
&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 528408&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;CVR_pv2cart2buy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_unique_cart_buy_users&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_unique_pv_users&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 528408 / 984107&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CVR_pv2cart2buy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The conversion rate for pv-cart-buy is 53.69%.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;pv - buy&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;CVR for page view user to buy user = # of users who have pv and buy / # of users who have pv&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;n_unique_pv_buy_users&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
SELECT COUNT(DISTINCT user_id)
FROM behaviorCount
WHERE PageView &amp;gt; 0 AND Favorite = 0 AND Cart = 0 AND Buy &amp;gt; 0
&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;CVR_pv2buy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_unique_pv_buy_users&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_unique_pv_users&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CVR_pv2buy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The conversion rate for pv-buy is 7.01%. (There might have users who have both pv-buy or pv-fav/cart-buy behaviors, such SQL would exclude those users who have both behaviors, therefore the CVR for pv-buy would be higher if based on items)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Funnel plot for 3 channels based on # of users&lt;/strong&gt;&lt;/p&gt;

&lt;!-- #80bdff
#f1b0b7
#ffc107
#54bc4b --&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plotly&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph_objects&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;go&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fig1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;go&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;go&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Funnel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'fav'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'buy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_unique_pv_users&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_unique_fav_users&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_unique_fav_buy_users&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;textposition&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;inside&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;textinfo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;value+percent initial&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;color&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;#80bdff&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;#f1b0b7&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;#54bc4b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]})&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model/funnel_1.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model/funnel_1.png&quot; alt=&quot;drawing&quot; style=&quot;width: 60%;&quot; /&gt;
   &lt;/a&gt;
   &lt;figcaption&gt;Fig 2: Funnel plot: pv-fav-buy &lt;/figcaption&gt;
&lt;/div&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model/funnel_2.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model/funnel_2.png&quot; alt=&quot;drawing&quot; style=&quot;width: 60%;&quot; /&gt;
   &lt;/a&gt;
   &lt;figcaption&gt;Fig 3: Funnel plot: pv-cart-buy &lt;/figcaption&gt;
&lt;/div&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model/funnel_3.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-05-21-Taobao_Behavior_Analysis_Model/funnel_3.png&quot; alt=&quot;drawing&quot; style=&quot;width: 60%;&quot; /&gt;
   &lt;/a&gt;
   &lt;figcaption&gt;Fig 4: Funnel plot: pv-buy &lt;/figcaption&gt;
&lt;/div&gt;
&lt;hr /&gt;

&lt;p&gt;Copyright @ 2021 Zizhun Guo. All Rights Reserved.&lt;/p&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html"></summary></entry><entry><title type="html">YOLO Project Poster Released!!! [with Audio]</title><link href="http://localhost:4001/jekyll/update/projects/2021/04/16/CP-Poster.html" rel="alternate" type="text/html" title="YOLO Project Poster Released!!! [with Audio]" /><published>2021-04-16T08:18:18-04:00</published><updated>2021-04-16T08:18:18-04:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/04/16/CP-Poster</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/04/16/CP-Poster.html">&lt;audio controls=&quot;&quot; autoplay=&quot;&quot; muted=&quot;&quot;&gt;
  &lt;!-- &lt;source src=&quot;http://localhost:4001/assets/2021-04-16-CP-Poster/record.mp3&quot; type=&quot;audio/ogg&quot;&gt; --&gt;
  &lt;source src=&quot;http://localhost:4001/assets/2021-04-16-CP-Poster/recordnew.mp3&quot; type=&quot;audio/mpeg&quot; /&gt;
  Your browser does not support the audio element.
&lt;/audio&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-04-16-CP-Poster/Poster.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-04-16-CP-Poster/Poster.png&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Poster for capstone project&lt;/figcaption&gt;
&lt;/div&gt;

&lt;hr /&gt;
&lt;p&gt;Copyright @ 2021 Zizhun Guo. All Rights Reserved.&lt;/p&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html">Your browser does not support the audio element.</summary></entry><entry><title type="html">YOLO Explore Object Detection PERFORMANCES In Relation To INPUT IMAGE DETERIORATION</title><link href="http://localhost:4001/jekyll/update/projects/2021/04/15/YOLO-Experiment-copy.html" rel="alternate" type="text/html" title="YOLO Explore Object Detection PERFORMANCES In Relation To INPUT IMAGE DETERIORATION " /><published>2021-04-15T18:34:18-04:00</published><updated>2021-04-15T18:34:18-04:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/04/15/YOLO-Experiment%20copy</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/04/15/YOLO-Experiment-copy.html">&lt;p&gt;&lt;strong&gt;Experiment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Baseline Model and Tuning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We consider having the model trained based on the original dataset without processing it with distorted quality and treat it as the baseline model. A fine-tune process is also necessary to conduct in order to get the optimal performance for object detection. This allows us to fit the same dataset processed in distorted methods in further step and make it possible for us to compare how the dataset in the variable of distortion types would impact the performance of the baseline model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning Rate Annealing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;During the training process, the adjustment of the learning rate is best have. The learning rate represents the speed at which information accumulates in the neural network over time. In an ideal situation, we would start with a large learning rate and gradually reduce the speed until the loss value no longer diverges.
The technique we use is learning rate annealing, which starts with a relatively high learning rate and then slowly reduces the learning rate during training. The idea behind this method is to move from the initial parameter to a “good” range of parameter values. We use the &lt;strong&gt;ReduceLROnPlateau&lt;/strong&gt; callback method of the TensorFlow API to implement this step. This callback monitors a quantity and if no improvement is seen for a ‘patience’ number of epochs, the learning rate is reduced.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Early Stopping&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In order to get better generalization performance to prevent the model from overfitting, we use the early stopping method. It is implemented by the TensorFlow EarlyStopping callback function. It stops training when a monitored metric has stopped improving based on the ‘patience’ - number of epochs with no improvement after which training will be stopped. We set it 3 for giving the 3 epochs training threshold times.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NMS Parameters Tuning: Score and IoU&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Once the YOLO model has finished the training process, it is necessary to fine-tune the IoU threshold and Score threshold to improve the overall performance in mAP metric. We choose the strategy by individually incrementing the values of each parameter in the stride of 0.1 and evaluate its model performance. During the individual tuning of each parameter, we select the current parameter value that can make mAP reach the highest value as the baseline setting. Therefore, for the final tuning result, we expect to get IoU and Score setting as in the format of values of pairs.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/01-finetune-score.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/01-finetune-score.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Score Threshold Tuning Results&lt;/figcaption&gt;
&lt;/div&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/02-finetune-score.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/02-finetune-score.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Score Threshold Tuning Line Graph&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;It can be seen from Figure 12 that the overall line graph follows a monotonous downward trend which also applies to both categories of the Traffic sign and Stop sign.&lt;/p&gt;

&lt;p&gt;The Step sign category has the lowest degree of decline in AP value, and the decline rate is slower, so the curve looks smoother. From table 1, the results show Stop sign performs consistently steady until the score threshold reaches the values that close to the end.&lt;/p&gt;

&lt;p&gt;Traffic sign behaves in an opposite way, the AP value is decreasing from the beginning, as the score threshold increases linearly, the AP value decreases more rapidly.&lt;/p&gt;

&lt;p&gt;When the score value is 0.1, the mAP value of the YOLO model is the highest and performs best. Therefore 0.1 is selected for the score threshold value for the baseline YOLO model.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/03-finetune-iou.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/03-finetune-iou.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;IoU Threshold Tuning Results&lt;/figcaption&gt;
&lt;/div&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/04-finetune-iou.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/04-finetune-iou.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;IoU Threshold Tuning&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;It can be seen from Figure 13 all three lines display a two-phased behavior. In the first phase, they both demonstrate a smooth upwards trend in values. The increasing rate is too small to be noticed in the figure, but the performance improves from the results table.&lt;/p&gt;

&lt;p&gt;In the second phase, the overall mAP and AP of all categories drop rapidly. The starting point for having such a situation is when IoU threshold equals around 0.6.&lt;/p&gt;

&lt;p&gt;When the IoU value is 0.3, the mAP value of the YOLO model is the highest and performs best. Therefore, the IoU threshold setting for the baseline YOLO model is 0.3.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fitting Dataset in Distortion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As it is mentioned in part 2, the dataset that employed for the experiment contains 3200 images, in which 10 percent of them are 320 images that are selected as the test set. The images in the test set will first be processed by different distortion methods, and then the test images of different types and levels of distortion will be input into the neural network for prediction, and the results will be used to measure the performance of the object detector using the final statistics of mAP.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/05-distortion-types.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/05-distortion-types.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Distortion Image Comparison Results&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;Noted that all 320 test images serve only for testing the performance of the YOLO model. The model is trained and validated by the other 90 percent of the dataset.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/06-map_results.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/06-map_results.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Distortion Image Comparison Results&lt;/figcaption&gt;
&lt;/div&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/07-map_comparison.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-04-15-YOLO-Experiment/07-map_comparison.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Distortion Image Comparison Bar Graph&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;From the experiment results, it is interesting to observe that the object detector is remarkably affected by the images of different distorted types in various behaviors.&lt;/p&gt;

&lt;p&gt;There are 3 distortion types that affect the detector in a positive way by increasing the mAP values. These three distortion types are Noise100, Grayscale, and Contrast. However, the increase of values is not significant therefore the impact levels are considered to be limited and weakly.&lt;/p&gt;

&lt;p&gt;The other three distortion types of images – Noise200, 2percentPackLost, 5percentPackLost – suffer extreme decrease. The 2percentPackLost and 5percentPackLost are even close to half and one-third of the performance with original images.&lt;/p&gt;

&lt;p&gt;Horizontally comparing detector’s performances with Stop signs(SS) and Traffic signs(TS), one using SS for all distortion types and levels beats what use TS. Surprisingly to observe that the 4 distortion types of images in Noise100, Noise200, Grayscale, and Contrast boosts the performances using SS. In other words, these 4 distortion types improved the detector’s ability to predict. However, it works in the opposite way when employs TS and exhibits a similar decrease in performance of overall mAP.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Copyright @ 2021 Zizhun Guo. All Rights Reserved.&lt;/p&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html">Experiment</summary></entry><entry><title type="html">YOLO Mechanism Part II: Training, Evaluation Metrics</title><link href="http://localhost:4001/jekyll/update/projects/2021/04/11/YOLO-Mechanism-part2-copy.html" rel="alternate" type="text/html" title="YOLO Mechanism Part II: Training, Evaluation Metrics" /><published>2021-04-11T06:13:14-04:00</published><updated>2021-04-11T06:13:14-04:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/04/11/YOLO-Mechanism-part2%20copy</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/04/11/YOLO-Mechanism-part2-copy.html">&lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Use Pretrained Model&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The pre-trained model is a deep learning architecture that has been trained to perform specific tasks on large amounts of data (for example, identifying classification problems in pictures). This kind of training is not easy to perform, and usually requires a lot of resources, beyond the resources available to many people who can use deep learning models, such as not having a large number of such GPUs.
It should be noted that the weight of the pre-training model needs to conform to the model structure used in the current task. In other words, the pre-training model must also be trained with the model structure of the current architecture. Therefore, the only difference is whether the machine equipment can support large-scale dataset training.&lt;/p&gt;

&lt;p&gt;The purpose of this is to increase training efficiency. In fact, many pre-training models already have relatively good prediction efficiency, but it should be noted that the data set during training of the pre-training model should have a target value similar to the current prediction task. In our case, there are two target categories in the Coco dataset that are similar to the two categories of the currently used Open Image dataset, namely Street Sign (COCO) corresponding to Traffic sign (Open Image) and Stop sign (COCO) and Stop sign ( Open Image).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training Work Flow&lt;/strong&gt;&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism-part2/01-training.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism-part2/01-training.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Training workflow&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;We train in two steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Warm-up stage: Load the weights of the pre-trained model. For the first 50 epochs, the first 60 layers of pre-trained model hidden layers are frozen. These 60 layers of hidden layers are the backbone feature extraction network. The purpose of this is to speed up the training speed, because only the weights of the YOLO head part are updated, which protects the network weights from being influenced at the initial stage of training.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unfreeze stage: Unfreeze the front 60 layers of backbone feature extraction network starting from the 51st epoch, and let the entire neural network participate in training. The training time will be greatly increased, but all weights will be updated in which the purpose is to make the backbone network fully adapt to the dataset of the current task.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Evaluation Metrics&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Generally speaking, for neural network models, it is hoped that the network model has a fast recognition speed, a small memory usage, and high detection accuracy. In terms of the current target detection problem, the general commonly used evaluation indicators are: mean average precision (map) and floating point operations per second (FLOPS). For the reason that our task only detects objects in the format of image files instead of video streaming, it is no longer needed to evaluate the detecting speed of the YOLO detector in our case, therefore mAP is the only metric we use to evaluate the performance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Precision and Recall&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Below follows the definition of precison and recall :&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism-part2/02-metric.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism-part2/02-metric.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Precision Recall&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;tp&lt;/strong&gt; represents true positive, &lt;strong&gt;fp&lt;/strong&gt; represents false positive, &lt;strong&gt;fn&lt;/strong&gt; represents false negative. The precision measures of all predictions made the percentage of correct positive predictions. The recall measures of all positive cases in reality the percentage of the correct positives cases are predicted. The trade-off between the two metrics depends on the task goal. In our case, the YOLO object detector is designed for traffic signs and stop signs detection which is related to traffic scenarios that are safety-sensitive, therefore to detects as many positive cases as possible from all positive cases is more important to increase the detection percentage among its own prediction, hence recall should be more primary to consider than precision. However, we only use precision and recall to calculate AP by drawing the PR-Curve.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Average Precision (AP) and PR-Curve&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The definition of &lt;strong&gt;AP&lt;/strong&gt; is defined as the area under the precision-recall curver (PR-curve). For multiple pairs of precision and recalls values, we get them through changing the confidence score threshold. The object that has a lower score than the threshold would be detected as a false positive.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism-part2/03-prcurve.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism-part2/03-prcurve.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;PR-Curve Example for Stop Sign&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;Given a PR-curve example shows above, the precision goes down once the recall goes up and vice versa. As a simple explanation to this pattern is, as more cases being detected positive cases, the precision increases due to at the same time, the overall detection cases increase as well but in smaller percentages. This would cause the recall to become smaller. Hence, the curve looks monotonically decreasing.&lt;/p&gt;

&lt;p&gt;Noted that the later number simply represents the IoU threshold values. For example, AP50 represents the IoU threshold that equals 50%.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mean Average Precision (mAP)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The metric mAP the averaged AP across all categories. In our cases, we have two categories Traffic sign and Stop signs, therefore the mAP in our experiment it simply means the summation of Traffic sign mAP and Stop sign mAP that divided by 2. The mAP is calculated after running the NMS algorithm. The mAP is the final evaluation metric of the detection model. After operations are completed, the final detection result is used to calculate the mAP value.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Copyright @ 2021 Zizhun Guo. All Rights Reserved.&lt;/p&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html">Training</summary></entry><entry><title type="html">YOLO Mechanism Part I: Input, Output, Encoding, Anchor Box, IoU, Decode, Confidence, NMS, Ground Truth, Loss</title><link href="http://localhost:4001/jekyll/update/projects/2021/03/31/YOLO-Mechanism-part1.html" rel="alternate" type="text/html" title="YOLO Mechanism Part I: Input, Output, Encoding, Anchor Box, IoU, Decode, Confidence, NMS, Ground Truth, Loss" /><published>2021-03-31T06:13:14-04:00</published><updated>2021-03-31T06:13:14-04:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/03/31/YOLO-Mechanism-part1</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/03/31/YOLO-Mechanism-part1.html">&lt;!-- ├── Project
   ├── classification
   │   ├── adversarial
   │   ├── contrast020
   │   ├── grayscale
   │   ├── noise100
   │   ├── noise200
   │   └── original
   └── App.py
       ├── contrast020
       ├── ensemble_adversarials
       ├── grayscale
       ├── noise100
       ├── noise200
       └── original --&gt;

&lt;p&gt;&lt;strong&gt;Mechanism&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Object Detection is a regression task that neural networks are required to output not only the predicting score and class for an object but also the bounding boxes. Figure 3 shows the basic mechanism of Yolo networks as the one-staged object detector.&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/01-mechanism.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/01-mechanism.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;YOLO Mechanism&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Label y&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As the figure[mechanism] shows, the output label is not the results output from the YOLO head layer, since the decode component would convert the bounding boxes’ encoding offsets into the real coordinates of the input image, Yolo Head works for filtering the real bound boxes and select ones that are expected to remain, therefore using the intermediate result – encodings as the labels ignores the additional operation and keeps the offsets as the normalized target values, which improved the efficiency to calculate the loss for the training process.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The shape of Input and Output&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The input image is sized in (m, 416, 416, 3), whereas m represents the number of batches set by the practitioners, (416, 416) represents the pixel numbers for the weight and height of the input images, and the 3 represents the channels numbers as RGB channels.&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/02-input-output.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/02-input-output.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Original Input Image vs Images in cells&lt;/figcaption&gt;
&lt;/div&gt;
&lt;p&gt;The output encoding is sized in (m, 26, 26, 21) or (m, 13, 13, 21), whereas m is the same as the input image, due to the object detector is FPN structured, therefore two scaled feature maps are extracted but the network, however, both (26, 26) and (13, 13) represents the same meaning, which is the cells numbers of the original images. Yolo algorithm requires the image to be divided into cells into variable scales so that to enable extracting objects in different distances since the objects at a far distance would be small-sized in the image which takes fewer numbers of pixels than the objects in closer distance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Encodings&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The encodings are fetched by the encoding process (see Encode in figure 3) while assigning the ground true samples during the training session or they are the intermediate results representing the extracted features process (see Feature Extractor in figure 3) which are output by the network backbone. The tensor size for the embeddings contains the same number of ranks of the input image but the actual features are thereby translated and embedded into the special object detection format.&lt;/p&gt;

&lt;p&gt;The last dimension number of the output encoding represents the list of bounding boxes along with the recognized classes. In our case, we have two classes: Traffic Signs and Stop Signs. As the specific flatten format of the encodings can be reorganized in a more meaningful way, from figure 5, the last element of the output encoding is 21, which also can be represented in (3, 7). The first element of 3 indicates that there are three anchor boxes, which are set to vary the scales of detecting results, whereas the number of 7 represents p_c, t_x, t_y, t_w, t_h, c_1, c_2.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;p_c  : the confidence if it exists an object in current cell.&lt;/li&gt;
  &lt;li&gt;t_x  : the x coordinate of the offset to anchor box grid cell.&lt;/li&gt;
  &lt;li&gt;t_y  : the y coordinate of the offset to anchor box grid cell..&lt;/li&gt;
  &lt;li&gt;t_w  : the relative weight ratio for the anchor box.&lt;/li&gt;
  &lt;li&gt;t_h  : the relative height ratio for the anchor box.&lt;/li&gt;
  &lt;li&gt;c_1  : the probability for the object being as a Traffic Sign.&lt;/li&gt;
  &lt;li&gt;c_2  : the probability for the object being as a Stop Sign.&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/03-encodings.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/03-encodings.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Encoding in (13, 13, 3, 7)&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Anchor Box&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The concept of anchor box was originally introduced by Faster RCNN. The anchor box (also known as the bounding box prior in the paper, and the anchor box is used later) is the statistics (using k-means) from all the ground truth boxes in the training set and the most frequently appearing box shapes and sizes in the training set. These statistical prior (or human) experiences can be added to the model in advance so that when the model is learning, the model converges quickly.&lt;/p&gt;

&lt;p&gt;Another understanding about using anchor boxes is that the traditional object detection head used as the regressor can only detect one object. The performance for multiple object detection would be affected and interference due to the variant shape of the bounding boxes. To solve this issue, multiple regressors can be used which are all limited to specific detecting regions. To achieve such a mechanism, for each grid cell, there could set multiple anchor boxes in different shapes specifically in charge for detecting objects around the positions.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/04-anchor-boxes.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/04-anchor-boxes.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;3 Anchor Boxes for single route of feature maps&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;From 4.2 FPN and 5.3 Encodings parts, we have two routes of feature map output for each input image sample. Therefore, we have two features that are in tensors’ shape in (13, 13, 3, 7) and (26, 26, 3, 7). The third number 3 implies there are three anchor boxes that are used for specifying the regressors.&lt;/p&gt;

&lt;p&gt;For the route feature maps in the shape of (26, 26, 3, 7), we select (23,27), (37,58), (81,82) sized bounding boxes as the anchor boxes. For the route feature maps in the shape of (13, 13, 3, 7), we select (81,82), (135,169), (344,319) as the anchor boxes.&lt;/p&gt;

&lt;p&gt;As can be seen by the size values, for images divided into (26, 26) cells, each cell is smaller than the ones processed into (13, 13), hence the anchor boxes for (26, 26) cells are smaller in the number of pixels, whereas the (13, 13) cells have greater sized anchor boxes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Intersection over Union for bounding boxes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The bounding box is represented in a format of 4 numbers. In the different processes of the YOLO neural networks, the tensors of the bounding boxes are variant processed. For example, the bounding box values as the intermediate results that are produced after the encoding process are represented in the format of (t_x, t_y, t_w, t_h). This tensor cannot be directly used to reference the exact bounding box’s coordinates so that to use it as the prediction results. Therefore, such intermediate should be decoded into the real coordinates that have the midpoints, weights, and heights in size of the same scales of the real image. The actual bounding box is set to be in the format of (b_x, b_y, b_w, b_h) whereas b_x represents the bounding box mid point x-coordinate, b_y  represents the bounding box mid point y-coordinate, b_w   represents the weight of the bounding box and b_h  represents the height of the bounding box.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/05-IoU.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/05-IoU.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Bounding Box example&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;Intersection over Union (IoU) is an evaluation metric that can be used in the prediction task of bounding boxes in values of ranges. The metric applies to all shapes of objects. In YOLO neural networks, it serves the same purpose by calculating the ratio between the intersection area over the union area to measure the accuracy of bounding boxes regressor.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Decode&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As the anchor boxes have different sizes of the boxes, YOLO only predicts the bounding boxes in the format of (t_x, t_y, t_w, t_h) that bonds to different anchor boxes where each value represent the offsets and relative values, due to the model instability when in early training iteration[]. However, this form of coordinates does not affect the final location prediction. The decoding process would translate the temporary coordinates into the bounding boxes coordinates relevant to the real image size.&lt;/p&gt;

&lt;p&gt;b_x= σ(t_x )+ c_x&lt;/p&gt;

&lt;p&gt;b_y= σ(t_y )+ c_y&lt;/p&gt;

&lt;p&gt;b_w=p_w  e^(t_w )&lt;/p&gt;

&lt;p&gt;b_h=p_h  e^(t_h )&lt;/p&gt;

&lt;p&gt;b_x and b_y represent the bounding box center coordinates,  b_w and b_h represent the bounding box weight and height. t_x and t_y represent the level of the center point shifts reletively to the c_x and c_y which represent that the cell is offset from the top left corner of the image. The b_w and b_h represent the weight and height of the bounding box prior (also known as the anchor box).&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/06-decode.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/06-decode.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Bounding Box Decode&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;The Decode process transforms the feature maps in the shape of (t_x, t_y, t_w, t_h) into the shape of (b_x, b_y, b_w, b_h), which makes it possible to not only take the advantage of anchor box mechanism but also by constraining the values of bounding boxes so that to make the network more stable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Confidence Scores&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;YOLO defines the Confidence Scores as the product of the confidence and the Intersection of Union (IOU) of prediction and ground truth bounding boxes’ paris.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/07-confidence.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/07-confidence.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
&lt;/div&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;The Pr(〖Class〗_i&lt;/td&gt;
      &lt;td&gt;Object) from the equation in our case is c_1  and c_2. Therefore, the final confidence score is determined how much the overlapping ratio between the prediction and ground true bounding boxes and the object-specific probability.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Non-Max Suppression&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Non-maximum suppression (NMS) is an algorithm for removing non-maximum values. The simple understanding is to select all the partial substitutions by defining the part. In the process of object detection, a large number of candidate bounding boxes will be generated at the same object position. These candidate boxes may overlap with each other. In this situation, the NMS would first generates a detection box with the object detection confidence score, the detection boxes with the highest scores are replaced, and other detection boxes that have obvious overlap with the replaced detection boxes are suppressed.&lt;/p&gt;

&lt;p&gt;The progress of the Non-maximum suppression algorithm : (1) Sort bounding box list by confidence scores; (2) Select the bounding box with the highest confidence score and add it to the final output list, and delete it from the bounding box list; (3) Calculate the area of all bounding boxes; (4) Calculate the IoU of the bounding box with the highest confidence score and other additional boxes; (5) Delete bounding boxes with IoU greater than the threshold; (6) Repeat the above process until the bounding box list is empty.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/08-NMS.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/08-NMS.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;NMSe&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;The image on the left is the result of the candidate boxes of YOLO detection. Each bounding box has a confidence score. If NMS is not used, multiple candidate boxes will appear such as duplicate bounding boxes on the same object and bounding boxes on other objects which should not be seen as the target objects. The image on the right-hand side is the result of using non-maximum suppression, which fits the expectation to have two bounding boxes attached on two traffic signs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ground Truth Boxes Assignment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Before the training session, one necessary step is to define the positive and negative cases for target variables for each image sample. It is essential to assign the ground true bounding box in the appropriate anchor box. Since each object, has only one corresponding bounding box for labeling but exists multiple anchor box regressors. The strategy is to calculate the IoU between ground true bounding box with each anchor box and select the anchor box with the greatest IoU as the target.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Loss&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The loss function of YOLO v4 is mainly divided into three parts: bounding box regression loss, confidence loss and classification loss.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/09-loss.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-31-YOLO-Mechanism/09-loss.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;The S and B represent the grid cells number and the bounding box priors (anchor boxes) number. The value of I_ij^obj parameter decides whether count the loss of the bounding boxes. If I_ij^obj equals 1, it means that the predicting bounding box matches the current anchor box, whereas if it is 0, the loss does not take into account.&lt;/p&gt;

&lt;p&gt;The G_ij plays a similar role as the I_ij^obj, it defines whether the confidence scores of the prediction result should be considered. For C_i^j =1, the predicted bounding box has the greatest IoU with ground truth box, hence C_i^j =0 for the cells of other anchor box types. One particular case is, the current anchor box which does not assign to detect a specific object class produces the IoU that is greater than the IoU threshold (YOLO paper is 0.5), G_ij would be 0 to neglect the loss calculation for it.&lt;/p&gt;

&lt;p&gt;In the original YOLO paper[], the loss function employs sum-squared error for it is easy to calculate. In our project, we use the binary cross-entropy as the cost for each grid cell and bounding box prior.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Copyright @ 2021 Zizhun Guo. All Rights Reserved.&lt;/p&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html"></summary></entry><entry><title type="html">YOLO Background &amp;amp; YOLO v4 tiny Archintecture</title><link href="http://localhost:4001/jekyll/update/projects/2021/03/12/YOLO-Background.html" rel="alternate" type="text/html" title="YOLO Background &amp; YOLO v4 tiny Archintecture" /><published>2021-03-12T03:54:12-05:00</published><updated>2021-03-12T03:54:12-05:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/03/12/YOLO-Background</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/03/12/YOLO-Background.html">&lt;!-- ├── Project
   ├── classification
   │   ├── adversarial
   │   ├── contrast020
   │   ├── grayscale
   │   ├── noise100
   │   ├── noise200
   │   └── original
   └── App.py
       ├── contrast020
       ├── ensemble_adversarials
       ├── grayscale
       ├── noise100
       ├── noise200
       └── original --&gt;

&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Object detection is the main direction of computer vision and digital image processing. It has important practical significance to reduce the consumption of human capital through computer vision in many fields such as robot navigation, intelligent video surveillance, industrial detection, and aerospace. Therefore, object detection has become a research hotspot in theory and application. It is an important branch of image processing and computer vision and is also a core part of intelligent monitoring systems. At the same time, object detection is also a basic algorithm in the field of generic recognition. It plays a vital role in subsequent tasks such as face recognition, gait recognition, crowd counting, and instance segmentation. The detection method mainly introduces two target detection algorithm ideas based on deep learning, a one-stage object detection algorithm, and a two-stage target detection algorithm.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-12-YOLO-Background/01-example.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-12-YOLO-Background/01-example.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Example: Object Detection Output &lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;The task of object detection can be divided into two key subtasks: object classification and object localization. The object classification task is responsible for judging whether there are objects of the interest category in the input image or the selected image area (Proposals) and output the possibility of a series of scored labels indicating that the objects of the interest category appear in the input image or the selected image area (Proposals). The object localization task is responsible for determining the position and range of the object of interest in the input image or the selected image area (Proposals), the bounding box of the output object, or the center of the object, or the closed boundary of the object, etc., usually a square bounding box, namely Bounding Box is used to represent the location information of an object.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Existing Approaches&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The current mainstream object detection algorithms are mainly based on deep learning models, which can be roughly divided into two categories: (1) One-Stage target detection algorithms: this type of detection algorithm does not require the Region Proposal stage, and can directly generate the category probability of the object and the position coordinate value, the more typical algorithms are YOLO, SSD and CornerNet; (2) Two-Stage target detection algorithm: this kind of detection algorithm divides the detection problem into two stages, the first stage generates the candidate region (Region Proposals), which contains the approximate location information of the object, and then classifies and refines the location of the candidate area in the second stage. Typical representatives of this type of algorithm are R-CNN, Fast R-CNN, Faster R-CNN, etc.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-12-YOLO-Background/02-onestaged-vs-twostaged.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-12-YOLO-Background/02-onestaged-vs-twostaged.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;One-Stage Detector vs Two-Stage Detector &lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;The One-Stage object detection algorithm can directly generate the category probability and position coordinate value of the object in one stage. Compared with the two-stage object detection algorithm, the Region Proposal stage is not required, and the overall process is simpler. As shown in the figure above, the input image is output through the backbone and neck of the networks, and the corresponding detection frame can be generated by decoding (post-processing) in dense prediction, whereas the two-staged object detection has one more step sparse prediction to process.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Model Architecture&lt;/strong&gt; 
For our study, we choose to use a One-Stage object detection model to conduct the experiements.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CSPDarknet53-tiny&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Darknet53 was first proposed by YOLOv4. Its main goal is to use as a Feature Extractor to embed images into vectors to perform post-prediction regression calculations after decoding. Therefore, as the backbone feature extractor, its role is to encode the image into smaller sized feature maps which contain the unique information about the input image.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CSP&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The CSP in CSPdarknet represents CSPNet architecture, which stands for Cross Stage Partial Network [1], its main purpose is to enable the architecture to achieve richer gradient combination information while reducing the amount of calculation. This goal can be achieved by dividing the feature map of the base layer into two parts and then merging them through the proposed cross-stage hierarchical structure. See CSPBlock structure in Figure 1, as the module adopting CSP architecture as part of the CSPDarknet53-tiny, it is composed of partial dense layer and partial convolutional layer. It is similar to the proposed CSPDenseNet which keeps the benefits of DenseNet’s feature for reusing characteristics, but meanwhile prevents an additional amount of duplicate gradient information by truncating the gradient flow [1]. In CSPDarknet53-tiny, the CSPBlock module divides the feature map into two parts, and combines the two parts by cross stage residual edge. This makes the gradient flow can propagate in two different network paths to increase the correlation difference of gradient information [2].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;FPN&lt;/strong&gt;
Through the FPN algorithm, the high-layer features of low-resolution and high-semantic information and the low-layer features of high-resolution and low-semantic information are connected from the top to the bottom side, so that to make the semantic information from features at all scales rich. Prediction on feature layers of different scales makes the effect of generating proposals better than the YOLOv2 Darknet19 backbone network and other traditional feature extraction algorithms such as VGG16 that only predict at the top level. As Figure 1 shows, using FPN can effectively reduce detection time [2]，it employs two-scale predictions (shape size in 26x26 and 13x13), conducting convolution layer and up-sampling layer at last feature layer in shapes of 26x26 and 13x13 so that to perform feature fusion.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-12-YOLO-Background/03-CSPDarkent53-tiny.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-12-YOLO-Background/03-CSPDarkent53-tiny.png&quot; alt=&quot;drawing&quot; style=&quot;width: 50%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Example: Architecture of CSPDarknet53-tiny for COCO Dataset&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;Through the above modules, the input image is extracted from the backbone feature network through the backbone feature network, and the two branch layer features are extracted through the CSP structure. After feature fusion is performed through the FPN algorithm, the two encoded image features have been obtained. The next part is to perform decoding process on these feature maps and run YOLO algorithm to make predictions. These processes are implemented by the YOLO Head in the last part of the result.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;YOLO Head&lt;/strong&gt;
The shape of the feature maps output from the above feature detectors is either (26, 26, 255) or (13, 13, 255). Since YOLO, it has a grid system that divides the image into smaller cells, therefore the first two coordinates represent the number of cells in total. The last coordinate represents the encoded information about the images. The details of how encoding works would be illustrated in the next part.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Copyright @ 2021 Zizhun Guo. All Rights Reserved.&lt;/p&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html"></summary></entry><entry><title type="html">YOLOv4 Deployment on AWS EC2 for Traffic/Stop Signs (TensorFlow + Flask + nginx + gunicorn)</title><link href="http://localhost:4001/jekyll/update/projects/2021/03/01/YOLOv4-Deployment-AWSEC2.html" rel="alternate" type="text/html" title="YOLOv4 Deployment on AWS EC2 for Traffic/Stop Signs (TensorFlow + Flask + nginx + gunicorn)" /><published>2021-03-01T15:13:14-05:00</published><updated>2021-03-01T15:13:14-05:00</updated><id>http://localhost:4001/jekyll/update/projects/2021/03/01/YOLOv4-Deployment-AWSEC2</id><content type="html" xml:base="http://localhost:4001/jekyll/update/projects/2021/03/01/YOLOv4-Deployment-AWSEC2.html">&lt;!-- ├── Project
   ├── classification
   │   ├── adversarial
   │   ├── contrast020
   │   ├── grayscale
   │   ├── noise100
   │   ├── noise200
   │   └── original
   └── App.py
       ├── contrast020
       ├── ensemble_adversarials
       ├── grayscale
       ├── noise100
       ├── noise200
       └── original --&gt;

&lt;p&gt;&lt;strong&gt;Preparation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;YOLOv3 python program&lt;/li&gt;
  &lt;li&gt;Flask APP&lt;/li&gt;
  &lt;li&gt;AWS EC2 instance&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Make sure YOLOv3 works fine: input an image and output the corresponding image plotting the predicted category score and bounding boxes.&lt;/li&gt;
  &lt;li&gt;Create a Flask App, design a web page templete that matches the routes.&lt;/li&gt;
  &lt;li&gt;Create an ubuntu EC2 Instance with default setting. Follow the rules by &lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/connection-prereqs.html#connection-prereqs-get-info-about-instance&quot;&gt;General prerequisites for connecting to your instance&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;Open Security Group and open inbound SSH connection on local IP address.&lt;/li&gt;
      &lt;li&gt;Use PuTTy create SSH session for CLI mode remotely control the instance. Use winSCP create SFTP session for file uploading to the instance. Detail in &lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html&quot;&gt;Connect to your Linux instance from Windows using PuTTY&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Configure the instance by upgrade/update/install environment for being able to running the YOLO application:
    &lt;ul&gt;
      &lt;li&gt;Python3.8&lt;/li&gt;
      &lt;li&gt;TensorFlow 2.3.1&lt;/li&gt;
      &lt;li&gt;Pillow&lt;/li&gt;
      &lt;li&gt;opencv-python&lt;/li&gt;
      &lt;li&gt;matplotlib&lt;/li&gt;
      &lt;li&gt;Flask&lt;/li&gt;
      &lt;li&gt;Gunicorn&lt;/li&gt;
      &lt;li&gt;nginx&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Deploy it online (Details in (Deploy flask app with Nginx using Gunicorn)[https://medium.com/faun/deploy-flask-app-with-nginx-using-gunicorn-7fda4f50066a]):
    &lt;ul&gt;
      &lt;li&gt;Upload the project to the instance&lt;/li&gt;
      &lt;li&gt;Create the WSGI Entry Point wsgi.py&lt;/li&gt;
      &lt;li&gt;Serve the project by Gunicorn&lt;/li&gt;
      &lt;li&gt;Configuring Nginx&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt apt-get install python3.8
python3.8 -m pip install --upgrade pip
python3.8 -m pip install tensorflow==2.3.1
python3.8 -m pip install Pillow
python3.8 -m pip install opencv-python
python3.8 -m pip install matplotlib
python3.8 -m pip install Flask
python3.8 -m pip install Gunicorn
python3.8 -m pip install tnginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Application URL&lt;/strong&gt;
&lt;a href=&quot;http://3.16.135.19/home&quot;&gt;http://3.16.135.19/home&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Application screenshot&lt;/strong&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/Yolo_deployment.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/Yolo_deployment.png&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;YOLO project web page&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Input example&lt;/strong&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/test.jpg&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/test.jpg&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Test Image&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/img_output.jpg&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/img_output.jpg&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Predicted Image&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Results Screenshot&lt;/strong&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/output_screenshot.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/output_screenshot.png&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Results Screenshot&lt;/figcaption&gt;
&lt;/div&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
    &lt;a href=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/1_zGC7qRcsw4G9I9u9KjMqaQ.png&quot;&gt;
   &lt;img src=&quot;http://localhost:4001/assets/2021-03-01-YOLOv3-Deployment-AWSEC2/1_zGC7qRcsw4G9I9u9KjMqaQ.png&quot; alt=&quot;drawing&quot; style=&quot;width: 100%;&quot; /&gt;
    &lt;/a&gt;
   &lt;figcaption&gt;Deployment Architecture&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Problems:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Unicorn Server crash frequently when uploading greater size image.&lt;/li&gt;
  &lt;li&gt;The result of predicted image may stay unchanged though using different input image&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Potential Solutions:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Deploy a supervisor program to manage server automatically by restarting the server.&lt;/li&gt;
  &lt;li&gt;The browser keep the cache for the previous results since the name of the output image is the same. Mechanism for storing the output image on server could be changed to solve it.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://faun.pub/deploy-flask-app-with-nginx-using-gunicorn-7fda4f50066a&quot;&gt;Deploy flask app with Nginx using Gunicorn:&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/32254439&quot;&gt;python web 部署：nginx + gunicorn + supervisor + flask 部署笔记:&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtu.be/tW6jtOOGVJI&quot;&gt;Deployment video:&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Zizhun Guo</name></author><category term="Projects" /><summary type="html"></summary></entry></feed>